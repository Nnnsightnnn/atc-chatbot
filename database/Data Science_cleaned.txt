Foundations of Data Science

Avrim Blum, John Hopcroft, and Ravindran Kannan

Thursday 4th January, 2018

Copyright 2015. All rights reserved

1

Contents

1 Introduction

9

2 High-Dimensional Space

2.4.1 Volume of the Unit Ball

2.4.2 Volume Near the Equator

12

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

2.1

. . . . . . . . . . . . . . . . . . . . . . . . . . 12

2.2 The Law of Large Numbers

2.3 The Geometry of High Dimensions

. . . . . . . . . . . . . . . . . . . . . . 15

2.4 Properties of the Unit Ball . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

. . . . . . . . . . . . . . . . . . . . . . . . 17

. . . . . . . . . . . . . . . . . . . . . . . 19

2.5 Generating Points Uniformly at Random from a Ball

. . . . . . . . . . . . 22

2.6 Gaussians in High Dimension . . . . . . . . . . . . . . . . . . . . . . . . . 23

2.7 Random Projection and Johnson-Lindenstrauss Lemma . . . . . . . . . . . 25

2.8 Separating Gaussians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

2.9 Fitting a Spherical Gaussian to Data . . . . . . . . . . . . . . . . . . . . . 29

2.10 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

2.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

3 Best-Fit Subspaces and Singular Value Decomposition (SVD)

40

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

3.1

3.2 Preliminaries

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

3.3 Singular Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

3.4 Singular Value Decomposition (SVD) . . . . . . . . . . . . . . . . . . . . . 45

3.5 Best Rank-k Approximations

. . . . . . . . . . . . . . . . . . . . . . . . . 47

3.6 Left Singular Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

3.7 Power Method for Singular Value Decomposition . . . . . . . . . . . . . . . 51

3.7.1 A Faster Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

3.8 Singular Vectors and Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . 54

3.9 Applications of Singular Value Decomposition . . . . . . . . . . . . . . . . 54

3.9.1 Centering Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54

3.9.2 Principal Component Analysis . . . . . . . . . . . . . . . . . . . . . 56

3.9.3 Clustering a Mixture of Spherical Gaussians . . . . . . . . . . . . . 56

. . . . . . . . . . . . . . . . . 62

3.9.4 Ranking Documents and Web Pages

3.9.5 An Application of SVD to a Discrete Optimization Problem . . . . 63

3.10 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

3.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67

4 Random Walks and Markov Chains

76

4.1 Stationary Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80

4.2 Markov Chain Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . . . 81

4.2.1 Metropolis-Hasting Algorithm . . . . . . . . . . . . . . . . . . . . . 83

4.2.2 Gibbs Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84

4.3 Areas and Volumes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86

2

4.4 Convergence of Random Walks on Undirected Graphs . . . . . . . . . . . . 88

4.4.1 Using Normalized Conductance to Prove Convergence . . . . . . . . 94

4.5 Electrical Networks and Random Walks . . . . . . . . . . . . . . . . . . . . 97

4.6 Random Walks on Undirected Graphs with Unit Edge Weights . . . . . . . 102

4.7 Random Walks in Euclidean Space . . . . . . . . . . . . . . . . . . . . . . 109

4.8 The Web as a Markov Chain . . . . . . . . . . . . . . . . . . . . . . . . . . 112

4.9 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116

4.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118

5 Machine Learning

129

5.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129

5.2 The Perceptron algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 130

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132

5.3 Kernel Functions

5.4 Generalizing to New Data . . . . . . . . . . . . . . . . . . . . . . . . . . . 134

5.5 Overtting and Uniform Convergence . . . . . . . . . . . . . . . . . . . . . 135

Illustrative Examples and Occams Razor . . . . . . . . . . . . . . . . . . . 138

5.6

5.6.1 Learning Disjunctions

. . . . . . . . . . . . . . . . . . . . . . . . . 138

5.6.2 Occams Razor

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139

5.6.3 Application: Learning Decision Trees . . . . . . . . . . . . . . . . . 140

5.7 Regularization: Penalizing Complexity . . . . . . . . . . . . . . . . . . . . 141

5.8 Online Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141

5.8.1 An Example: Learning Disjunctions . . . . . . . . . . . . . . . . . . 142

5.8.2 The Halving Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 143

5.8.3 The Perceptron Algorithm . . . . . . . . . . . . . . . . . . . . . . . 143

. . . . . . . . . . . . 145

5.8.4 Extensions: Inseparable Data and Hinge Loss

5.9 Online to Batch Conversion . . . . . . . . . . . . . . . . . . . . . . . . . . 146

5.10 Support-Vector Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147

5.11 VC-Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148

5.11.1 Denitions and Key Theorems . . . . . . . . . . . . . . . . . . . . . 149

5.11.2 Examples: VC-Dimension and Growth Function . . . . . . . . . . . 151

5.11.3 Proof of Main Theorems . . . . . . . . . . . . . . . . . . . . . . . . 153

5.11.4 VC-Dimension of Combinations of Concepts . . . . . . . . . . . . . 156

5.11.5 Other Measures of Complexity . . . . . . . . . . . . . . . . . . . . . 156

5.12 Strong and Weak Learning - Boosting . . . . . . . . . . . . . . . . . . . . . 157

5.13 Stochastic Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . 160

5.14 Combining (Sleeping) Expert Advice . . . . . . . . . . . . . . . . . . . . . 162

5.15 Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164

5.15.1 Generative Adversarial Networks (GANs) . . . . . . . . . . . . . . . 170

5.16 Further Current Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . 171

5.16.1 Semi-Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . 171

5.16.2 Active Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174

5.16.3 Multi-Task Learning . . . . . . . . . . . . . . . . . . . . . . . . . . 174

5.17 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175

3

5.18 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176

6 Algorithms for Massive Data Problems: Streaming, Sketching, and

181

Sampling

6.1

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181

6.2 Frequency Moments of Data Streams . . . . . . . . . . . . . . . . . . . . . 182

6.2.1 Number of Distinct Elements in a Data Stream . . . . . . . . . . . 183

6.2.2 Number of Occurrences of a Given Element.

. . . . . . . . . . . . . 186

. . . . . . . . . . . . . . . . . . . . . . . . . . . 187

6.2.3 Frequent Elements

6.2.4 The Second Moment . . . . . . . . . . . . . . . . . . . . . . . . . . 189

6.3 Matrix Algorithms using Sampling . . . . . . . . . . . . . . . . . . . . . . 192

6.3.1 Matrix Multiplication using Sampling . . . . . . . . . . . . . . . . . 193

Implementing Length Squared Sampling in Two Passes . . . . . . . 197

6.3.2

Sketch of a Large Matrix . . . . . . . . . . . . . . . . . . . . . . . . 197

6.3.3

6.4 Sketches of Documents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201

6.5 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203

6.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204

7 Clustering

7.1

Structural Properties of the k-Means Objective

208

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208

7.1.1 Preliminaries

7.1.2 Two General Assumptions on the Form of Clusters

. . . . . . . . . 209

Spectral Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . 211

7.1.3

7.2 k-Means Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211

7.2.1 A Maximum-Likelihood Motivation . . . . . . . . . . . . . . . . . . 211

7.2.2

. . . . . . . . . . . 212

7.2.3 Lloyds Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213

7.2.4 Wards Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215

k-Means Clustering on the Line . . . . . . . . . . . . . . . . . . . . 215

7.2.5

7.3 k-Center Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215

7.4 Finding Low-Error Clusterings . . . . . . . . . . . . . . . . . . . . . . . . . 216

7.5 Spectral Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216

7.5.1 Why Project? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216

7.5.2 The Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218

7.5.3 Means Separated by (1) Standard Deviations . . . . . . . . . . . . 219

7.5.4 Laplacians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221

7.5.5 Local spectral clustering . . . . . . . . . . . . . . . . . . . . . . . . 221

7.6 Approximation Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224

7.6.1 The Conceptual Idea . . . . . . . . . . . . . . . . . . . . . . . . . . 224

7.6.2 Making this Formal . . . . . . . . . . . . . . . . . . . . . . . . . . . 224

. . . . . . . . . . . . . . . . . . . . . . . . 225

7.6.3 Algorithm and Analysis

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227

Single Linkage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227

7.7 High-Density Clusters

7.7.1

4

7.7.2 Robust Linkage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228

7.8 Kernel Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228

7.9 Recursive Clustering based on Sparse Cuts . . . . . . . . . . . . . . . . . . 229

7.10 Dense Submatrices and Communities . . . . . . . . . . . . . . . . . . . . . 230

7.11 Community Finding and Graph Partitioning . . . . . . . . . . . . . . . . . 233

7.12 Spectral clustering applied to social networks . . . . . . . . . . . . . . . . . 236

7.13 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239

7.14 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240

8 Random Graphs

8.1 The G(n, p) Model

245

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245

8.1.1 Degree Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . 246

. . . . . . . . . . . . . . . . . . 250

8.1.2 Existence of Triangles in G(n, d/n)

8.2 Phase Transitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252

8.3 Giant Component . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261

8.3.1 Existence of a giant component . . . . . . . . . . . . . . . . . . . . 261

8.3.2 No other large components . . . . . . . . . . . . . . . . . . . . . . . 263

8.3.3 The case of p < 1/n . . . . . . . . . . . . . . . . . . . . . . . . . . . 264

8.4 Cycles and Full Connectivity . . . . . . . . . . . . . . . . . . . . . . . . . . 265

. . . . . . . . . . . . . . . . . . . . . . . . . . 265

8.4.1 Emergence of Cycles

8.4.2 Full Connectivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266

8.4.3 Threshold for O(ln n) Diameter . . . . . . . . . . . . . . . . . . . . 268

8.5 Phase Transitions for Increasing Properties . . . . . . . . . . . . . . . . . . 270

8.6 Branching Processes

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272

8.7 CNF-SAT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277

8.7.1

SAT-solvers in practice . . . . . . . . . . . . . . . . . . . . . . . . . 278

8.7.2 Phase Transitions for CNF-SAT . . . . . . . . . . . . . . . . . . . . 279

8.8 Nonuniform Models of Random Graphs . . . . . . . . . . . . . . . . . . . . 284

8.8.1 Giant Component in Graphs with Given Degree Distribution . . . . 285

8.9 Growth Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286

8.9.1 Growth Model Without Preferential Attachment . . . . . . . . . . . 287

. . . . . . . . . . . . 293

8.9.2 Growth Model With Preferential Attachment

8.10 Small World Graphs

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294

8.11 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299

8.12 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301

9 Topic Models, Nonnegative Matrix Factorization, Hidden Markov Mod-

310

els, and Graphical Models

9.1 Topic Models

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310

9.2 An Idealized Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313

9.3 Nonnegative Matrix Factorization - NMF . . . . . . . . . . . . . . . . . . . 315

9.4 NMF with Anchor Terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317

9.5 Hard and Soft Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318

5

9.6 The Latent Dirichlet Allocation Model for Topic Modeling . . . . . . . . . 320

. . . . . . . . . . . . . . . . . . . . . . . 322

9.7 The Dominant Admixture Model

9.8 Formal Assumptions

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324

9.9 Finding the Term-Topic Matrix . . . . . . . . . . . . . . . . . . . . . . . . 327

9.10 Hidden Markov Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332

9.11 Graphical Models and Belief Propagation . . . . . . . . . . . . . . . . . . . 337

9.12 Bayesian or Belief Networks . . . . . . . . . . . . . . . . . . . . . . . . . . 338

9.13 Markov Random Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339

9.14 Factor Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340

9.15 Tree Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341

9.16 Message Passing in General Graphs . . . . . . . . . . . . . . . . . . . . . . 342

9.17 Graphs with a Single Cycle

. . . . . . . . . . . . . . . . . . . . . . . . . . 344

9.18 Belief Update in Networks with a Single Loop . . . . . . . . . . . . . . . . 346

9.19 Maximum Weight Matching . . . . . . . . . . . . . . . . . . . . . . . . . . 347

9.20 Warning Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351

9.21 Correlation Between Variables . . . . . . . . . . . . . . . . . . . . . . . . . 351

9.22 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355

9.23 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357

10 Other Topics

10.2 Compressed Sensing and Sparse Vectors

360

10.1 Ranking and Social Choice . . . . . . . . . . . . . . . . . . . . . . . . . . . 360

10.1.1 Randomization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363

10.1.2 Examples

. . . . . . . . . . . . . . . . . . . 364

10.2.1 Unique Reconstruction of a Sparse Vector

. . . . . . . . . . . . . . 365

10.2.2 Eciently Finding the Unique Sparse Solution . . . . . . . . . . . . 366

10.3 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368

10.3.1 Biological

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368

10.3.2 Low Rank Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . 369

10.4 An Uncertainty Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370

. . . . . . . . . . . . . . . 370

10.4.1 Sparse Vector in Some Coordinate Basis

10.4.2 A Representation Cannot be Sparse in Both Time and Frequency

Domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371

10.5 Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373

10.6 Linear Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375

10.6.1 The Ellipsoid Algorithm . . . . . . . . . . . . . . . . . . . . . . . . 375

10.7 Integer Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377

10.8 Semi-Denite Programming . . . . . . . . . . . . . . . . . . . . . . . . . . 378

10.9 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380

10.10Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381

6

11 Wavelets

385

11.1 Dilation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385

11.2 The Haar Wavelet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386

11.3 Wavelet Systems

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390

11.4 Solving the Dilation Equation . . . . . . . . . . . . . . . . . . . . . . . . . 390

11.5 Conditions on the Dilation Equation . . . . . . . . . . . . . . . . . . . . . 392

11.6 Derivation of the Wavelets from the Scaling Function . . . . . . . . . . . . 394

11.7 Sucient Conditions for the Wavelets to be Orthogonal . . . . . . . . . . . 398

. . . . . . . . . . . . . . . . . 401

11.8 Expressing a Function in Terms of Wavelets

11.9 Designing a Wavelet System . . . . . . . . . . . . . . . . . . . . . . . . . . 402

11.10Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402

11.11 Bibliographic Notes

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403

11.12 Exercises

12 Appendix

406

12.1 Denitions and Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406

12.2 Asymptotic Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406

12.3 Useful Relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408

12.4 Useful Inequalities

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413

12.5 Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420

12.5.1 Sample Space, Events, and Independence . . . . . . . . . . . . . . . 420

12.5.2 Linearity of Expectation . . . . . . . . . . . . . . . . . . . . . . . . 421

12.5.3 Union Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422

12.5.4 Indicator Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . 422

12.5.5 Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422

12.5.6 Variance of the Sum of Independent Random Variables . . . . . . . 423

12.5.7 Median . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423

12.5.8 The Central Limit Theorem . . . . . . . . . . . . . . . . . . . . . . 423

12.5.9 Probability Distributions . . . . . . . . . . . . . . . . . . . . . . . . 424

12.5.10 Bayes Rule and Estimators . . . . . . . . . . . . . . . . . . . . . . . 428

12.6 Bounds on Tail Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . 430

12.6.1 Cherno Bounds

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 430

12.6.2 More General Tail Bounds . . . . . . . . . . . . . . . . . . . . . . . 433

12.7 Applications of the Tail Bound . . . . . . . . . . . . . . . . . . . . . . . . 436

. . . . . . . . . . . . . . . . . . . . . . . . . 437

12.8 Eigenvalues and Eigenvectors

12.8.1 Symmetric Matrices

. . . . . . . . . . . . . . . . . . . . . . . . . . 439

12.8.2 Relationship between SVD and Eigen Decomposition . . . . . . . . 441

12.8.3 Extremal Properties of Eigenvalues . . . . . . . . . . . . . . . . . . 441

12.8.4 Eigenvalues of the Sum of Two Symmetric Matrices . . . . . . . . . 443

12.8.5 Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445

12.8.6 Important Norms and Their Properties . . . . . . . . . . . . . . . . 446

12.8.7 Additional Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . 448

12.8.8 Distance between subspaces . . . . . . . . . . . . . . . . . . . . . . 450

7

12.8.9 Positive semidenite matrix . . . . . . . . . . . . . . . . . . . . . . 451

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451

12.9 Generating Functions

12.9.1 Generating Functions for Sequences Dened by Recurrence Rela-

tionships . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452

12.9.2 The Exponential Generating Function and the Moment Generating

Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454

12.10Miscellaneous . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456

12.10.1 Lagrange multipliers

. . . . . . . . . . . . . . . . . . . . . . . . . . 456

12.10.2 Finite Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457

12.10.3 Application of Mean Value Theorem . . . . . . . . . . . . . . . . . 457

12.10.4 Sperners Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459

12.10.5 Prufer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459

12.11Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460

Index

466

8

1

Introduction

Computer science as an academic discipline began in the 1960s. Emphasis was on

programming languages, compilers, operating systems, and the mathematical theory that

supported these areas. Courses in theoretical computer science covered nite automata,

regular expressions, context-free languages, and computability. In the 1970s, the study

of algorithms was added as an important component of theory. The emphasis was on

making computers useful. Today, a fundamental change is taking place and the focus is

more on a wealth of applications. There are many reasons for this change. The merging

of computing and communications has played an important role. The enhanced ability

to observe, collect, and store data in the natural sciences, in commerce, and in other

elds calls for a change in our understanding of data and how to handle it in the modern

setting. The emergence of the web and social networks as central aspects of daily life

presents both opportunities and challenges for theory.

While traditional areas of computer science remain highly important, increasingly re-

searchers of the future will be involved with using computers to understand and extract

usable information from massive data arising in applications, not just how to make com-

puters useful on specic well-dened problems. With this in mind we have written this

book to cover the theory we expect to be useful in the next 40 years, just as an under-

standing of automata theory, algorithms, and related topics gave students an advantage

in the last 40 years. One of the major changes is an increase in emphasis on probability,

statistics, and numerical methods.

Early drafts of the book have been used for both undergraduate and graduate courses.

Background material needed for an undergraduate course has been put in the appendix.

For this reason, the appendix has homework problems.

Modern data in diverse elds such as information processing, search, and machine

learning is often advantageously represented as vectors with a large number of compo-

nents. The vector representation is not just a book-keeping device to store many elds

of a record. Indeed, the two salient aspects of vectors: geometric (length, dot products,

orthogonality etc.) and linear algebraic (independence, rank, singular values etc.) turn

out to be relevant and useful. Chapters 2 and 3 lay the foundations of geometry and

linear algebra respectively. More specically, our intuition from two or three dimensional

space can be surprisingly o the mark when it comes to high dimensions. Chapter 2

works out the fundamentals needed to understand the dierences. The emphasis of the

chapter, as well as the book in general, is to get across the intellectual ideas and the

mathematical foundations rather than focus on particular applications, some of which are

briey described. Chapter 3 focuses on singular value decomposition (SVD) a central tool

to deal with matrix data. We give a from-rst-principles description of the mathematics

and algorithms for SVD. Applications of singular value decomposition include principal

component analysis, a widely used technique which we touch upon, as well as modern

9

applications to statistical mixtures of probability densities, discrete optimization, etc.,

which are described in more detail.

Exploring large structures like the web or the space of congurations of a large system

with deterministic methods can be prohibitively expensive. Random walks (also called

Markov Chains) turn out often to be more ecient as well as illuminative. The station-

ary distributions of such walks are important for applications ranging from web search to

the simulation of physical systems. The underlying mathematical theory of such random

walks, as well as connections to electrical networks, forms the core of Chapter 4 on Markov

chains.

One of the surprises of computer science over the last two decades is that some domain-

independent methods have been immensely successful in tackling problems from diverse

areas. Machine learning is a striking example. Chapter 5 describes the foundations

of machine learning, both algorithms for optimizing over given training examples, as

well as the theory for understanding when such optimization can be expected to lead to

good performance on new, unseen data. This includes important measures such as the

Vapnik-Chervonenkis dimension, important algorithms such as the Perceptron Algorithm,

stochastic gradient descent, boosting, and deep learning, and important notions such as

regularization and overtting.

The eld of algorithms has traditionally assumed that the input data to a problem is

presented in random access memory, which the algorithm can repeatedly access. This is

not feasible for problems involving enormous amounts of data. The streaming model and

other models have been formulated to reect this. In this setting, sampling plays a crucial

role and, indeed, we have to sample on the y. In Chapter 6 we study how to draw good

samples eciently and how to estimate statistical and linear algebra quantities, with such

samples.

While Chapter 5 focuses on supervised learning, where one learns from labeled training

data, the problem of unsupervised learning, or learning from unlabeled data, is equally

important. A central topic in unsupervised learning is clustering, discussed in Chapter

7. Clustering refers to the problem of partitioning data into groups of similar objects.

After describing some of the basic methods for clustering, such as the k-means algorithm,

Chapter 7 focuses on modern developments in understanding these, as well as newer al-

gorithms and general frameworks for analyzing dierent kinds of clustering problems.

Central to our understanding of large structures, like the web and social networks, is

building models to capture essential properties of these structures. The simplest model

is that of a random graph formulated by Erdos and Renyi, which we study in detail in

Chapter 8, proving that certain global phenomena, like a giant connected component,

arise in such structures with only local choices. We also describe other models of random

graphs.

10

Chapter 9 focuses on linear-algebraic problems of making sense from data, in par-

ticular topic modeling and non-negative matrix factorization. In addition to discussing

well-known models, we also describe some current research on models and algorithms with

provable guarantees on learning error and time. This is followed by graphical models and

belief propagation.

Chapter 10 discusses ranking and social choice as well as problems of sparse represen-

tations such as compressed sensing. Additionally, Chapter 10 includes a brief discussion

of linear programming and semidenite programming. Wavelets, which are an impor-

tant method for representing signals across a wide range of applications, are discussed in

Chapter 11 along with some of their fundamental mathematical properties. The appendix

includes a range of background material.

A word about notation in the book. To help the student, we have adopted certain

notations, and with a few exceptions, adhered to them. We use lower case letters for

scalar variables and functions, bold face lower case for vectors, and upper case letters

for matrices. Lower case near the beginning of the alphabet tend to be constants, in the

middle of the alphabet, such as i, j, and k, are indices in summations, n and m for integer

sizes, and x, y and z for variables. If A is a matrix its elements are aij and its rows are ai.

If ai is a vector its coordinates are aij. Where the literature traditionally uses a symbol

for a quantity, we also used that symbol, even if it meant abandoning our convention. If

we have a set of points in some vector space, and work with a subspace, we use n for the

number of points, d for the dimension of the space, and k for the dimension of the subspace.

The term almost surely means with probability tending to one. We use ln n for the

natural logarithm and log n for the base two logarithm. If we want base ten, we will use

log10 . To simplify notation and to make it easier to read we use E2(1  x) for (cid:0)E(1  x)(cid:1)2

and E(1  x)2 for E (cid:0)(1  x)2(cid:1) . When we say randomly select some number of points

from a given probability distribution, independence is always assumed unless otherwise

stated.

11

2 High-Dimensional Space

2.1 Introduction

High dimensional data has become very important. However, high dimensional space

is very dierent from the two and three dimensional spaces we are familiar with. Generate

n points at random in d-dimensions where each coordinate is a zero mean, unit variance

Gaussian. For suciently large d, with high probability the distances between all pairs

of points will be essentially the same. Also the volume of the unit ball in d-dimensions,

the set of all points x such that |x|  1, goes to zero as the dimension goes to innity.

The volume of a high dimensional unit ball is concentrated near its surface and is also

concentrated at its equator. These properties have important consequences which we will

consider.

2.2 The Law of Large Numbers

If one generates random points in d-dimensional space using a Gaussian to generate

coordinates, the distance between all pairs of points will be essentially the same when d

is large. The reason is that the square of the distance between two points y and z,

|y  z|2 =

d

(cid:88)

(yi  zi)2,

i=1

can be viewed as the sum of d independent samples of a random variable x that is dis-

tributed as the squared dierence of two Gaussians. In particular, we are summing inde-

pendent samples xi = (yi  zi)2 of a random variable x of bounded variance. In such a

case, a general bound known as the Law of Large Numbers states that with high proba-

bility, the average of the samples will be close to the expectation of the random variable.

This in turn implies that with high probability, the sum is close to the sums expectation.

Specically, the Law of Large Numbers states that

(cid:12)

(cid:12)

 E(x)

(cid:12)

(cid:12)

x1 + x2 +    + xn

n

(cid:18)(cid:12)

(cid:12)

(cid:12)

(cid:12)

Prob

(cid:19)

 (cid:15)



V ar(x)

n(cid:15)2

.

(2.1)

The larger the variance of the random variable, the greater the probability that the error

will exceed (cid:15). Thus the variance of x is in the numerator. The number of samples n is in

the denominator since the more values that are averaged, the smaller the probability that

the dierence will exceed (cid:15). Similarly the larger (cid:15) is, the smaller the probability that the

dierence will exceed (cid:15) and hence (cid:15) is in the denominator. Notice that squaring (cid:15) makes

the fraction a dimensionless quantity.

We use two inequalities to prove the Law of Large Numbers. The rst is Markovs

inequality that states that the probability that a nonnegative random variable exceeds a

is bounded by the expected value of the variable divided by a.

12

Theorem 2.1 (Markovs inequality) Let x be a nonnegative random variable. Then

for a > 0,

Prob(x  a) 

E(x)

a

.

Proof: For a continuous nonnegative random variable x with probability density p,

E (x) =





(cid:90)

0



(cid:90)

a

xp(x)dx =

a

(cid:90)

0

xp(x)dx +



(cid:90)

a

xp(x)dx

xp(x)dx  a



(cid:90)

a

p(x)dx = aProb(x  a).

Thus, Prob(x  a)  E(x)

a .

The same proof works for discrete random variables with sums instead of integrals.

Corollary 2.2 Prob (cid:0)x  bE(x)(cid:1)  1

b

Markovs inequality bounds the tail of a distribution using only information about the

mean. A tighter bound can be obtained by also using the variance of the random variable.

Theorem 2.3 (Chebyshevs inequality) Let x be a random variable. Then for c > 0,

(cid:16)

Prob

|x  E(x)|  c

(cid:17)



V ar(x)

c2

.

Proof: Prob(cid:0)|x  E(x)|  c(cid:1) = Prob(cid:0)|x  E(x)|2  c2(cid:1). Let y = |x  E(x)|2. Note that

y is a nonnegative random variable and E(y) = V ar(x), so Markovs inequality can be

applied giving:

Prob(|x  E(x)|  c) = Prob (cid:0)|x  E(x)|2  c2(cid:1) 

E(|x  E(x)|2)

c2

=

V ar(x)

c2

.

The Law of Large Numbers follows from Chebyshevs inequality together with facts

about independent random variables. Recall that:

E(x + y) = E(x) + E(y),

V ar(x  c) = V ar(x),

V ar(cx) = c2V ar(x).

13

Also, if x and y are independent, then E(xy) = E(x)E(y). These facts imply that if x

and y are independent then V ar(x + y) = V ar(x) + V ar(y), which is seen as follows:

V ar(x + y) = E(x + y)2  E2(x + y)

= E(x2 + 2xy + y2)  (cid:0)E2(x) + 2E(x)E(y) + E2(y)(cid:1)

= E(x2)  E2(x) + E(y2)  E2(y) = V ar(x) + V ar(y),

where we used independence to replace E(2xy) with 2E(x)E(y).

Theorem 2.4 (Law of Large Numbers) Let x1, x2, . . . , xn be n independent samples

of a random variable x. Then

Prob

(cid:18)(cid:12)

(cid:12)

(cid:12)

(cid:12)

x1 + x2 +    + xn

n

(cid:12)

(cid:12)

 E(x)

(cid:12)

(cid:12)

(cid:19)

 (cid:15)



V ar(x)

n(cid:15)2

Proof: By Chebychevs inequality

Prob

(cid:18)(cid:12)

(cid:12)

(cid:12)

(cid:12)

x1 + x2 +    + xn

n

 E(x)

(cid:19)

 (cid:15)



(cid:12)

(cid:12)

(cid:12)

(cid:12)

V ar (cid:0) x1+x2++xn

(cid:15)2

n

(cid:1)

=

=

=

1

n2(cid:15)2 V ar(x1 + x2 +    + xn)

1

n2(cid:15)2

V ar(x)

n(cid:15)2

.

(cid:0)V ar(x1) + V ar(x2) +    + V ar(xn)(cid:1)

The Law of Large Numbers is quite general, applying to any random variable x of

nite variance. Later we will look at tighter concentration bounds for spherical Gaussians

and sums of 0-1 valued random variables.

One observation worth making about the Law of Large Numbers is that the size of the

universe does not enter into the bound. For instance, if you want to know what fraction

of the population of a country prefers tea to coee, then the number n of people you need

to sample in order to have at most a  chance that your estimate is o by more than (cid:15)

depends only on (cid:15) and  and not on the population of the country.

As an application of the Law of Large Numbers, let z be a d-dimensional random point

whose coordinates are each selected from a zero mean, 1

2 variance Gaussian. We set the

variance to 1

2 so the Gaussian probability density equals one at the origin and is bounded

below throughout the unit ball by a constant.1 By the Law of Large Numbers, the square

of the distance of z to the origin will be (d) with high probability. In particular, there is

1If we instead used variance 1, then the density at the origin would be a decreasing function of d,

namely ( 1

2 )d/2, making this argument more complicated.

14

vanishingly small probability that such a random point z would lie in the unit ball. This

implies that the integral of the probability density over the unit ball must be vanishingly

small. On the other hand, the probability density in the unit ball is bounded below by a

constant. We thus conclude that the unit ball must have vanishingly small volume.

Similarly if we draw two points y and z from a d-dimensional Gaussian with unit

variance in each direction, then |y|2  d and |z|2  d. Since for all i,

E(yi  zi)2 = E(y2

i ) + E(z2

i )  2E(yizi) = V ar(yi) + V ar(zi) + 2E(yi)E(zi) = 2,

d

(cid:80)

i=1

|yz|2 =

(yi zi)2  2d. Thus by the Pythagorean theorem, the random d-dimensional

y and z must be approximately orthogonal. This implies that if we scale these random

points to be unit length and call y the North Pole, much of the surface area of the unit ball

must lie near the equator. We will formalize these and related arguments in subsequent

sections.

We now state a general theorem on probability tail bounds for a sum of indepen-

dent random variables. Tail bounds for sums of Bernoulli, squared Gaussian and Power

Law distributed random variables can all be derived from this. The table in Figure 2.1

summarizes some of the results.

Theorem 2.5 (Master Tail Bounds Theorem) Let x = x1 + x2 +    + xn, where

x1, x2, . . . , xn are mutually independent random variables with zero mean and variance at

i )|  2s! for s = 3, 4, . . . , (cid:98)(a2/4n2)(cid:99).

most 2. Let 0  a 

Then,

2n2. Assume that |E(xs



Prob (|x|  a)  3ea2/(12n2).

The proof of Theorem 2.5 is elementary. A slightly more general version, Theorem 12.5,

is given in the appendix. For a brief intuition of the proof, consider applying Markovs

inequality to the random variable xr where r is a large even number. Since r is even, xr

is nonnegative, and thus Prob(|x|  a) = Prob(xr  ar)  E(xr)/ar. If E(xr) is not

too large, we will get a good bound. To compute E(xr), write E(x) as E(x1 + . . . + xn)r

and expand the polynomial into a sum of terms. Use the fact that by independence

E(xri

j ) to get a collection of simpler expectations that can be bounded

using our assumption that |E(xs

i )|  2s!. For the full proof, see the appendix.

j ) = E(xri

i )E(xrj

i xrj

2.3 The Geometry of High Dimensions

An important property of high-dimensional objects is that most of their volume is

near the surface. Consider any object A in Rd. Now shrink A by a small amount (cid:15) to

produce a new object (1  (cid:15))A = {(1  (cid:15))x|x  A}. Then the following equality holds:

volume(cid:0)(1  (cid:15))A(cid:1) = (1  (cid:15))dvolume(A).

15

Markov

Chebychev

Cherno

Condition

Tail bound

x  0

Any x

Prob(x  a)  E(x)

a

Prob(cid:0)|x  E(x)|  a(cid:1)  Var(x)

a2

x = x1 + x2 +    + xn

xi  [0, 1] i.i.d. Bernoulli;

Prob(|x  E(x)|  E(x))

 3ec2E(x)

Higher Moments

r positive even integer

Prob(|x|  a)  E(xr)/ar

Gaussian

Annulus

x = (cid:112)x2

xi  N (0, 1);  

1 + x2

2 +    + x2

n



n indep.

Prob(|x 



n|  )  3ec2

Power Law

for xi; order k  4

x = x1 + x2 + . . . + xn

xi i.i.d ;   1/k2

Prob(cid:0)|x  E(x)|  E(x)(cid:1)

 (4/2kn)(k3)/2

Figure 2.1: Table of Tail Bounds. The Higher Moments bound is obtained by apply-

ing Markov to xr. The Cherno, Gaussian Annulus, and Power Law bounds follow from

Theorem 2.5 which is proved in the appendix.

To see that this is true, partition A into innitesimal cubes. Then, (1  )A is the union

of a set of cubes obtained by shrinking the cubes in A by a factor of 1  . When we

shrink each of the 2d sides of a d-dimensional cube by a factor f , its volume shrinks by a

factor of f d. Using the fact that 1  x  ex, for any object A in Rd we have:

volume(cid:0)(1  (cid:15))A(cid:1)

volume(A)

= (1  (cid:15))d  e(cid:15)d.

Fixing (cid:15) and letting d  , the above quantity rapidly approaches zero. This means

that nearly all of the volume of A must be in the portion of A that does not belong to

the region (1  (cid:15))A.

Let S denote the unit ball in d dimensions, that is, the set of points within distance

one of the origin. An immediate implication of the above observation is that at least a

1  e(cid:15)d fraction of the volume of the unit ball is concentrated in S \ (1  (cid:15))S, namely

in a small annulus of width (cid:15) at the boundary. In particular, most of the volume of the

d-dimensional unit ball is contained in an annulus of width O(1/d) near the boundary. If

the ball is of radius r, then the annulus width is O (cid:0) r

(cid:1) .

d

16

1

1  1

d

Annulus of

width 1

d

Figure 2.2: Most of the volume of the d-dimensional ball of radius r is contained in an

annulus of width O(r/d) near the boundary.

2.4 Properties of the Unit Ball

We now focus more specically on properties of the unit ball in d-dimensional space.

We just saw that most of its volume is concentrated in a small annulus of width O(1/d)

near the boundary. Next we will show that in the limit as d goes to innity, the volume of

the ball goes to zero. This result can be proven in several ways. Here we use integration.

2.4.1 Volume of the Unit Ball

To calculate the volume V (d) of the unit ball in Rd, one can integrate in either Cartesian

or polar coordinates. In Cartesian coordinates the volume is given by



x2=

(cid:90)

1x2

1

x1=1

(cid:90)



xd=

 

V (d) =

1x2

d1

1x2

(cid:90)

dxd    dx2dx1.

x1=1

x2=



1x2

1



xd=

1x2

1x2

d1

Since the limits of the integrals are complicated, it is easier to integrate using polar

coordinates. In polar coordinates, V (d) is given by

V (d) =

(cid:90)

1

(cid:90)

Sd

r=0

rd1drd.

Since the variables  and r do not interact,

V (d) =

1

(cid:90)

(cid:90)

d

Sd

r=0

rd1dr =

1

d

(cid:90)

Sd

d =

A(d)

d

where A(d) is the surface area of the d-dimensional unit ball. For instance, for d = 3 the

surface area is 4 and the volume is 4

3. The question remains, how to determine the

17

surface area A (d) = (cid:82)

Sd

d for general d.

Consider a dierent integral



(cid:90)



(cid:90)



(cid:90)

 

I (d) =







e(x2

1+x2

2+x2

d)dxd    dx2dx1.

Including the exponential allows integration to innity rather than stopping at the surface

of the sphere. Thus, I(d) can be computed by integrating in both Cartesian and polar

coordinates. Integrating in polar coordinates will relate I(d) to the surface area A(d).

Equating the two results for I(d) allows one to solve for A(d).

First, calculate I(d) by integration in Cartesian coordinates.

I (d) =



d

ex2dx





(cid:90)







= (cid:0)

(cid:1)d = 

d

2 .

Here, we have used the fact that (cid:82) 

. For a proof of this, see Section 12.3

of the appendix. Next, calculate I(d) by integrating in polar coordinates. The volume of

the dierential element is rd1ddr. Thus,

 ex2 dx =



I (d) =

d



(cid:90)

0

(cid:90)

Sd

er2rd1dr.

d is the integral over the entire solid angle and gives the surface area,

The integral (cid:82)

Sd

A(d), of a unit sphere. Thus, I (d) = A (d)

integral gives



(cid:90)

0

er2rd1dr =

ett

d1

2



(cid:90)

0

(cid:16) 1

2t 1

2 dt

(cid:17)

er2rd1dr. Evaluating the remaining



(cid:82)

0

=

1

2



(cid:90)

0

ett

d

2  1dt =

(cid:19)

1

2



(cid:18) d

2

(cid:1) where the Gamma function  (x) is a generalization of the

and hence, I(d) = A(d) 1

factorial function for noninteger values of x.  (x) = (x  1)  (x  1),  (1) =  (2) = 1,

and  (cid:0) 1

. For integer x,  (x) = (x  1)!.

2 (cid:0) d

(cid:1) =



2

2

Combining I (d) = 

d

2 with I (d) = A (d) 1

2 (cid:0) d

2

(cid:1) yields

A (d) =

establishing the following lemma.

d

2



2 (cid:0) d

1

2

(cid:1)

18

Lemma 2.6 The surface area A(d) and the volume V (d) of a unit-radius ball in d di-

mensions are given by

A (d) =

2

2 d

( d

2 )

and

V (d) =

2

2 d

d ( d

2 )

.

= 4

To check the formula for the volume of a unit ball, note that V (2) =  and V (3) =

3

2



2

3, which are the correct volumes for the unit balls in two and three dimen-

2)

( 3

3

sions. To check the formula for the surface area of a unit ball, note that A(2) = 2 and

3

A(3) = 2

2





dimensions. Note that  d

This implies that lim

d

= 4, which are the correct surface areas for the unit ball in two and three

(cid:1) grows as the factorial of d

2 .

2 is an exponential in d

V (d) = 0, as claimed.

2 and  (cid:0) d

1

2

2

2.4.2 Volume Near the Equator

An interesting fact about the unit ball in high dimensions is that most of its volume

is concentrated near its equator. In particular, for any unit-length vector v dening

north, most of the volume of the unit ball lies in the thin slab of points whose dot-

d). To show this fact, it suces by symmetry to x

product with v has magnitude O(1/

v to be the rst coordinate vector. That is, we will show that most of the volume of the

d). Using this fact, we will show that two random points in the

unit ball has |x1| = O(1/

unit ball are with high probability nearly orthogonal, and also give an alternative proof

from the one in Section 2.4.1 that the volume of the unit ball goes to zero as d  .





Theorem 2.7 For c  1 and d  3, at least a 1  2

d-dimensional unit ball has |x1|  c

.

d1

c ec2/2 fraction of the volume of the

Proof: By symmetry we just need to prove that at most a 2

the ball with x1  0 has x1  c

d1

and let H denote the upper hemisphere. We will then show that the ratio of the volume

of A to the volume of H goes to zero by calculating an upper bound on volume(A) and

a lower bound on volume(H) and proving that

. Let A denote the portion of the ball with x1  c

c ec2/2 fraction of the half of

d1

volume(A)

volume(H)



upper bound volume(A)

lower bound volume(H)

=

2

c

e c2

2 .

To calculate the volume of A, integrate an incremental volume that is a disk of width

1. The surface area of

dx1 and whose face is a ball of dimension d  1 and radius (cid:112)1  x2

the disk is (1  x2

2 V (d  1) and the volume above the slice is

1) d1

volume(A) =

(cid:90) 1

c

d1

(1  x2

1)

d1

2 V (d  1)dx1

19

volume(A) 

(cid:90) 

x1

Now

c

d1

(cid:90) 

c

d1

x1

A







H

c

d1

Figure 2.3: Most of the volume of the upper hemisphere of the d-dimensional ball is

below the plane x1 = c

.

d1

To get an upper bound on the above integral, use 1  x  ex and integrate to innity.

To integrate, insert x1

, which is greater than one in the range of integration, into the

integral. Then

d1

c





d  1

c

e d1

2 x2

1V (d  1)dx1 = V (d  1)



d  1

c

(cid:90) 

c

d1

x1e d1

2 x2

1dx1

x1e d1

2 x2

1dx1 = 

1

d  1

e d1

2 x2

1

(cid:12)

(cid:12)

(cid:12)



c

(d1)

=

1

d  1

e c2

2

e c2

2 .



c

Thus, an upper bound on volume(A) is V (d1)

d1

The volume of the hemisphere below the plane x1 = 1

volume of the upper hemisphere and this volume is at least that of a cylinder of height

d1. The volume of the cylinder is V (d  1)(1  1

and radius

d1

fact that (1x)a  1ax for a  1, the volume of the cylinder is at least V (d1)

d1

d1) d1

1  1

1

(cid:113)

d1



2

2

1

d1

. Using the

for d  3.

is a lower bound on the entire

Thus,

ratio 

upper bound above plane

lower bound total hemisphere

=

e c2

2

V (d1)



d1

c

V (d1)

d1

2



=

e c2

2

2

c

One might ask why we computed a lower bound on the total hemisphere since it is one

half of the volume of the unit ball which we already know. The reason is that the volume

of the upper hemisphere is 1

2V (d) and we need a formula with V (d  1) in it to cancel the

V (d  1) in the numerator.

20

Near orthogonality. One immediate implication of the above analysis is that if we

draw two points at random from the unit ball, with high probability their vectors will be

nearly orthogonal to each other. Specically, from our previous analysis in Section 2.3,

with high probability both will be close to the surface and will have length 1  O(1/d).

From our analysis above, if we dene the vector in the direction of the rst point as

d) in

north, with high probability the second will have a projection of only O(1/

d). This implies that with high

this direction, and thus their dot-product will be O(1/

d). In particular, we

probability, the angle between the two vectors will be /2  O(1/

have the following theorem that states that if we draw n points at random in the unit

ball, with high probability all points will be close to unit length and each pair of points

will be almost orthogonal.







Theorem 2.8 Consider drawing n points x1, x2, . . . , xn at random from the unit ball.

With probability 1  O(1/n)

1. |xi|  1  2 ln n

d

for all i, and

2. |xi  xj| 



6 ln n

d1

for all i (cid:54)= j.

Proof: For the rst part, for any xed i by the analysis of Section 2.3, the probability

that |xi| < 1  (cid:15) is less than e(cid:15)d. Thus

Prob(cid:0)|xi| < 1 

2 ln n

d

(cid:1)  e( 2 ln n

d

)d = 1/n2.

By the union bound, the probability there exists an i such that |xi| < 1  2 ln n

1/n.

d

is at most

2

For the second part, Theorem 2.7 states that the probability |xi| > c

2 . There are (cid:0)n

c e c2

probability that the projection of xj onto the north direction is more than

most O(e 6 ln n

at most O (cid:0)(cid:0)n

is at most

(cid:1) pairs i and j and for each such pair if we dene xi as north, the

is at

2 ) = O(n3). Thus, the dot-product condition is violated with probability

(cid:1)n3(cid:1) = O(1/n) as well.



6 ln n

d1

d1

2

2

Alternative proof that volume goes to zero. Another immediate implication of

Theorem 2.7 is that as d  , the volume of the ball approaches zero. Specically, con-

sider a small box centered at the origin of side length 2c

. Using Theorem 2.7, we show

ln d, this box contains over half of the volume of the ball. On the other

that for c = 2

hand, the volume of this box clearly goes to zero as d goes to innity, since its volume is

O(( ln d

d1)d/2). Thus the volume of the ball goes to zero as well.

d1



By Theorem 2.7 with c = 2

ln d, the fraction of the volume of the ball with |x1|  c

d1



is at most:

e c2

2 =

2

c



1

ln d

e2 ln d =

1



ln d

d2

<

1

d2 .

21

1



2

2

1

1

1

2

1

2

1

1

2



d

2

 Unit radius sphere

 Nearly all the volume

 Vertex of hypercube

Figure 2.4: Illustration of the relationship between the sphere and the cube in 2, 4, and

d-dimensions.

Since this is true for each of the d dimensions, by a union bound at most a O( 1

fraction of the volume of the ball lies outside the cube, completing the proof.

d)  1

2

Discussion. One might wonder how it can be that nearly all the points in the unit ball

are very close to the surface and yet at the same time nearly all points are in a box of

(cid:1). The answer is to remember that points on the surface of the ball

side-length O (cid:0) ln d

(cid:17)

satisfy x2

d = 1, so for each coordinate i, a typical value will be O

.

In fact, it is often helpful to think of picking a random point on the sphere as very similar

to picking a random point of the form

2 + . . . + x2

1 + x2

(cid:16) 1

d1

(cid:17)

(cid:16)

.

d

 1

d

,  1

d

,  1

d

, . . .  1

d

2.5 Generating Points Uniformly at Random from a Ball

Consider generating points uniformly at random on the surface of the unit ball. For

the 2-dimensional version of generating points on the circumference of a unit-radius cir-

cle, independently generate each coordinate uniformly at random from the interval [1, 1].

This produces points distributed over a square that is large enough to completely contain

the unit circle. Project each point onto the unit circle. The distribution is not uniform

since more points fall on a line from the origin to a vertex of the square than fall on a line

from the origin to the midpoint of an edge of the square due to the dierence in length.

To solve this problem, discard all points outside the unit circle and project the remaining

points onto the circle.

In higher dimensions, this method does not work since the fraction of points that fall

inside the ball drops to zero and all of the points would be thrown away. The solution is to

generate a point each of whose coordinates is an independent Gaussian variable. Generate

exp(x2/2) on the

x1, x2, . . . , xd, using a zero mean, unit variance Gaussian, namely,

1

2

22

real line.2 Thus, the probability density of x is

p (x) =

1+x2

x2

2++x2

d

2

e

1

d

2

(2)

and is spherically symmetric. Normalizing the vector x = (x1, x2, . . . , xd) to a unit vector,

namely x

|x| , gives a distribution that is uniform over the surface of the sphere. Note that

once the vector is normalized, its coordinates are no longer statistically independent.

To generate a point y uniformly over the ball (surface and interior), scale the point

x

|x| generated on the surface by a scalar   [0, 1]. What should the distribution of  be

as a function of r? It is certainly not uniform, even in 2 dimensions. Indeed, the density

of  at r is proportional to r for d = 2. For d = 3, it is proportional to r2. By similar

reasoning, the density of  at distance r is proportional to rd1 in d dimensions. Solving

(cid:82) r=1

r=0 crd1dr = 1 (the integral of density must equal 1) one should set c = d. Another

way to see this formally is that the volume of the radius r ball in d dimensions is rdV (d).

The density at radius r is exactly d

dr (rdVd) = drd1Vd. So, pick (r) with density equal to

drd1 for r over [0, 1].

We have succeeded in generating a point

y = 

x

|x|

uniformly at random from the unit ball by using the convenient spherical Gaussian dis-

tribution. In the next sections, we will analyze the spherical Gaussian in more detail.

2.6 Gaussians in High Dimension

A 1-dimensional Gaussian has its mass close to the origin. However, as the dimension

is increased something dierent happens. The d-dimensional spherical Gaussian with zero

mean and variance 2 in each coordinate has density function

p(x) =

1

(2)d/2 d

exp

(cid:16)

 |x|2

22

(cid:17)

.

The value of the density is maximum at the origin, but there is very little volume there.

When 2 = 1, integrating the probability density over a unit ball centered at the origin

yields almost zero mass since the volume of such a ball is negligible. In fact, one needs

2One might naturally ask: how do you generate a random number from a 1-dimensional Gaussian?

To generate a number from any distribution given its cumulative distribution function P, rst select a

uniform random number u  [0, 1] and then choose x = P 1(u). For any a < b, the probability that x is

between a and b is equal to the probability that u is between P (a) and P (b) which equals P (b)  P (a)

as desired. For the 2-dimensional Gaussian, one can generate a point in polar coordinates by choosing

angle  uniform in [0, 2] and radius r = (cid:112)2 ln(u) where u is uniform random in [0, 1]. This is called

the Box-Muller transform.

23



d before there is a signicant volume and

to increase the radius of the ball to nearly

d, the

hence signicant probability mass.

integral barely increases even though the volume increases since the probability density

is dropping o at a much higher rate. The following theorem formally states that nearly

all the probability is concentrated in a thin annulus of width O(1) at radius

If one increases the radius much beyond





d.

Theorem 2.9 (Gaussian Annulus Theorem) For a d-dimensional spherical Gaussian

d, all but at most 3ec2 of the prob-

with unit variance in each direction, for any  

d + , where c is a xed positive

ability mass lies within the annulus

constant.

d    |x| 







For a high-level intuition, note that E(|x|2) =

1) = d, so the mean

squared distance of a point from the center is d. The Gaussian Annulus Theorem says

that the points are tightly concentrated. We call the square root of the mean squared

distance, namely

d, the radius of the Gaussian.

i ) = dE(x2

E(x2



d

(cid:80)

i=1

To prove the Gaussian Annulus Theorem we make use of a tail inequality for sums of

independent random variables of bounded moments (Theorem 12.5).

Proof (Gaussian Annulus Theorem): Let x = (x1, x2, . . . , xd) be a point selected

d    |y| 

from a unit variance Gaussian centered at the origin, and let r = |x|.



d|  , then multiplying both sides by

d. So, it suces to bound the probability that

d +  is equivalent to |r 

d gives |r2  d|  (r +

d|  . If |r 



d)  











r +

|r2  d|  



d.

Rewrite r2  d = (x2

of variables: yi = x2

Notice that E(yi) = E(x2

moments of yi.

1 + . . . + x2

d)  d = (x2

1  1) + . . . + (x2

i  1. We want to bound the probability that |y1 + . . . + yd|  

d  1) and perform a change

d.

i )  1 = 0. To apply Theorem 12.5, we need to bound the sth



For |xi|  1, |yi|s  1 and for |xi|  1, |yi|s  |xi|2s. Thus

|E(ys

i )| = E(|yi|s)  E(1 + x2s

(cid:114) 2



= 1 +

(cid:90) 

x2sex2/2dx

0

i ) = 1 + E(x2s

i )

Using the substitution 2z = x2,

|E(ys

i )| = 1 +

1





(cid:90) 

0

 2ss!.

2szs(1/2)ezdz

The last inequality is from the Gamma integral.

24

Since E(yi) = 0, V ar(yi) = E(y2

i )  222 = 8. Unfortunately, we do not have |E(ys

i )| 

8s! as required in Theorem 12.5. To x this problem, perform one more change of variables,

using wi = yi/2. Then, V ar(wi)  2 and |E(ws

i )|  2s!, and our goal is now to bound the

probability that |w1 + . . . + wd|  

2 . Applying Theorem 12.5 where 2 = 2 and n = d,

this occurs with probability less than or equal to 3e 2

96 .



d

In the next sections we will see several uses of the Gaussian Annulus Theorem.

2.7 Random Projection and Johnson-Lindenstrauss Lemma

One of the most frequently used subroutines in tasks involving high dimensional data

is nearest neighbor search. In nearest neighbor search we are given a database of n points

in Rd where n and d are usually large. The database can be preprocessed and stored in

an ecient data structure. Thereafter, we are presented query points in Rd and are

asked to nd the nearest or approximately nearest database point to the query point.

Since the number of queries is often large, the time to answer each query should be very

small, ideally a small function of log n and log d, whereas preprocessing time could be

larger, namely a polynomial function of n and d. For this and other problems, dimension

reduction, where one projects the database points to a k-dimensional space with k (cid:28) d

(usually dependent on log d) can be very useful so long as the relative distances between

points are approximately preserved. We will see using the Gaussian Annulus Theorem

that such a projection indeed exists and is simple.

The projection f : Rd  Rk that we will examine (many related projections are

known to work as well) is the following. Pick k Gaussian vectors u1, u2, . . . , uk in Rd

with unit-variance coordinates. For any vector v, dene the projection f (v) by:

f (v) = (u1  v, u2  v, . . . , uk  v).



The projection f (v) is the vector of dot products of v with the ui. We will show that

k|v|. For any two vectors v1 and v2, f (v1  v2) =

with high probability, |f (v)| 

f (v1)  f (v2). Thus, to estimate the distance |v1  v2| between two vectors v1 and v2 in

Rd, it suces to compute |f (v1)  f (v2)| = |f (v1  v2)| in the k-dimensional space since

the factor of

k is known and one can divide by it. The reason distances increase when

we project to a lower dimensional space is that the vectors ui are not unit length. Also

notice that the vectors ui are not orthogonal. If we had required them to be orthogonal,

we would have lost statistical independence.



Theorem 2.10 (The Random Projection Theorem) Let v be a xed vector in Rd

and let f be dened as above. There exists constant c > 0 such that for   (0, 1),

Prob

(cid:16)(cid:12)

(cid:12)

(cid:12)|f (v)| 



(cid:12)

(cid:12)

(cid:12)  

k|v|



k|v|

(cid:17)

 3eck2,

where the probability is taken over the random draws of vectors ui used to construct f .

25

Proof: By scaling both sides of the inner inequality by |v|, we may assume that |v| = 1.

The sum of independent normally distributed real variables is also normally distributed

where the mean and variance are the sums of the individual means and variances. Since

ui  v = (cid:80)d

j=1 uijvj, the random variable ui  v has Gaussian density with zero mean and

unit variance, in particular,

V ar(ui  v) = V ar

(cid:33)

vijvj

=

(cid:32) d

(cid:88)

j=1

d

(cid:88)

j=1

v2

j V ar(uij) =

d

(cid:88)

j=1

v2

j = 1

Since u1 v, u2 v, . . . , uk v are independent Gaussian random variables, f (v) is a random

vector from a k-dimensional spherical Gaussian with unit variance in each coordinate, and

so the theorem follows from the Gaussian Annulus Theorem (Theorem 2.9) with d replaced

by k.

The random projection theorem establishes that the probability of the length of the

projection of a single vector diering signicantly from its expected value is exponentially

small in k, the dimension of the target subspace. By a union bound, the probability that

any of O(n2) pairwise dierences |vi  vj| among n vectors v1, . . . , vn diers signicantly

from their expected values is small, provided k  3

c2 ln n. Thus, this random projection

preserves all relative pairwise distances between points in a set of n points with high

probability. This is the content of the Johnson-Lindenstrauss Lemma.

Theorem 2.11 (Johnson-Lindenstrauss Lemma) For any 0 <  < 1 and any integer

n, let k  3

c2 ln n with c as in Theorem 2.9. For any set of n points in Rd, the random

projection f : Rd  Rk dened above has the property that for all pairs of points vi and

vj, with probability at least 1  3/2n,



(1  )

k |vi  vj|  |f (vi)  f (vj)|  (1 + )



k |vi  vj| .

Proof: Applying the Random Projection Theorem (Theorem 2.10), for any xed vi and

vj, the probability that |f (vi  vj)| is outside the range

(cid:104)

(1  )



k|vi  vj|, (1 + )



(cid:105)

k|vi  vj|

is at most 3eck2  3/n3 for k  3 ln n

union bound, the probability that any pair has a large distortion is less than 3

2n .

c2 . Since there are (cid:0)n

(cid:1) < n2/2 pairs of points, by the

2

Remark: It is important to note that the conclusion of Theorem 2.11 asserts for all vi

and vj, not just for most of them. The weaker assertion for most vi and vj is typically less

useful, since our algorithm for a problem such as nearest-neighbor search might return

one of the bad pairs of points. A remarkable aspect of the theorem is that the number

of dimensions in the projection is only dependent logarithmically on n. Since k is often

much less than d, this is called a dimension reduction technique.

In applications, the

dominant term is typically the 1/2 term.

26

For the nearest neighbor problem, if the database has n1 points and n2 queries are

expected during the lifetime of the algorithm, take n = n1 + n2 and project the database

to a random k-dimensional space, for k as in Theorem 2.11. On receiving a query, project

the query to the same subspace and compute nearby database points. The Johnson

Lindenstrauss Lemma says that with high probability this will yield the right answer

whatever the query. Note that the exponentially small in k probability was useful here in

making k only dependent on ln n, rather than n.

2.8 Separating Gaussians

Mixtures of Gaussians are often used to model heterogeneous data coming from multiple

sources. For example, suppose we are recording the heights of individuals age 20-30 in a

city. We know that on average, men tend to be taller than women, so a natural model

would be a Gaussian mixture model p(x) = w1p1(x) + w2p2(x), where p1(x) is a Gaussian

density representing the typical heights of women, p2(x) is a Gaussian density represent-

ing the typical heights of men, and w1 and w2 are the mixture weights representing the

proportion of women and men in the city. The parameter estimation problem for a mixture

model is the problem: given access to samples from the overall density p (e.g., heights of

people in the city, but without being told whether the person with that height is male

or female), reconstruct the parameters for the distribution (e.g., good approximations to

the means and variances of p1 and p2, as well as the mixture weights).

There are taller women and shorter men, so even if one solved the parameter estima-

tion problem for heights perfectly, given a data point, one couldnt necessarily tell which

population it came from. That is, given a height, one couldnt necessarily tell if it came

from a man or a woman. In this section, we will look at a problem that is in some ways

easier and some ways harder than this problem of heights. It will be harder in that we

will be interested in a mixture of two Gaussians in high-dimensions as opposed to the

d = 1 case of heights. But it will be easier in that we will assume the means are quite

well-separated compared to the variances. Specically, our focus will be on a mixture of

two spherical unit-variance Gaussians whose means are separated by a distance (d1/4).

We will show that at this level of separation, we can with high probability uniquely de-

termine which Gaussian each data point came from. The algorithm to do so will actually

be quite simple. Calculate the distance between all pairs of points. Points whose distance

apart is smaller are from the same Gaussian, whereas points whose distance is larger are

from dierent Gaussians. Later, we will see that with more sophisticated algorithms, even

a separation of (1) suces.

First, consider just one spherical unit-variance Gaussian centered at the origin. From

d.

Theorem 2.9, most of its probability mass lies on an annulus of width O(1) at radius

Also e|x|2/2 = (cid:81)

i /2 and almost all of the mass is within the slab { x | c  x1  c },

for c  O(1). Pick a point x from this Gaussian. After picking x, rotate the coordinate

system to make the rst axis align with x. Independently pick a second point y from

i ex2



27



d



2d



d

(a)



2d

y

z

q



d

x

p





2 + 2d



(b)

Figure 2.5: (a) indicates that two randomly chosen points in high dimension are surely

almost nearly orthogonal. (b) indicates the distance between a pair of random points

from two dierent unit balls approximating the annuli of two Gaussians.

this Gaussian. The fact that almost all of the probability mass of the Gaussian is within

the slab {x |  c  x1  c, c  O(1)} at the equator implies that ys component along

xs direction is O(1) with high probability. Thus, y is nearly perpendicular to x. So,

|x  y|  (cid:112)|x|2 + |y|2. See Figure 2.5(a). More precisely, since the coordinate system

has been rotated so that x is at the North Pole, x = (

d  O(1), 0, . . . , 0). Since y is

almost on the equator, further rotate the coordinate system so that the component of

y that is perpendicular to the axis of the North Pole is in the second coordinate. Then

y = (O(1),





d  O(1), 0, . . . , 0). Thus,



(x  y)2 = d  O(

d) + d  O(



d) = 2d  O(



d)

and |x  y| =



2d  O(1) with high probability.

Consider two spherical unit variance Gaussians with centers p and q separated by a

distance . The distance between a randomly chosen point x from the rst Gaussian

2 + 2d, since x  p, p  q,

and a randomly chosen point y from the second is close to

and q  y are nearly mutually perpendicular. Pick x and rotate the coordinate system

so that x is at the North Pole. Let z be the North Pole of the ball approximating the

second Gaussian. Now pick y. Most of the mass of the second Gaussian is within O(1)

of the equator perpendicular to z  q. Also, most of the mass of each Gaussian is within

distance O(1) of the respective equators perpendicular to the line q  p. See Figure 2.5

(b). Thus,



|x  y|2  2 + |z  q|2 + |q  y|2

= 2 + 2d  O(

d)).



To ensure that the distance between two points picked from the same Gaussian are

closer to each other than two points picked from dierent Gaussians requires that the

upper limit of the distance between a pair of points from the same Gaussian is at most

28





2d + O(1) 

2d + 2  O(1) or 2d + O(

the lower limit of distance between points from dierent Gaussians. This requires that



d)  2d + 2, which holds when   (d1/4).

Thus, mixtures of spherical Gaussians can be separated in this way, provided their centers

are separated by (d1/4).

If we have n points and want to correctly separate all of

them with high probability, we need our individual high-probability statements to hold

with probability 1  1/poly(n),3 which means our O(1) terms from Theorem 2.9 become

O(

log n). So we need to include an extra O(

log n) term in the separation distance.





Algorithm for separating points from two Gaussians: Calculate all

pairwise distances between points. The cluster of smallest pairwise distances

must come from a single Gaussian. Remove these points. The remaining

points come from the second Gaussian.

In the next

One can actually separate Gaussians where the centers are much closer.

chapter we will use singular value decomposition to separate points from a mixture of two

Gaussians when their centers are separated by a distance O(1).

2.9 Fitting a Spherical Gaussian to Data

Given a set of sample points, x1, x2, . . . , xn, in a d-dimensional space, we wish to nd

the spherical Gaussian that best ts the points. Let f be the unknown Gaussian with

mean  and variance 2 in each direction. The probability density for picking these points

when sampling according to f is given by

(cid:32)

c exp



(x1  )2 + (x2  )2 +    + (xn  )2

22

(cid:33)

where the normalizing constant c is the reciprocal of

(cid:20)(cid:90)

(cid:21)n

. In integrating from

22 dx

e |x|2

(cid:20)(cid:90)

e |x|2

22 dx

(cid:21)n

= 1

(2)

n

2

and is

 to , one can shift the origin to  and thus c is

independent of .

The Maximum Likelihood Estimator (MLE) of f, given the samples x1, x2, . . . , xn, is

the f that maximizes the above probability density.

Lemma 2.12 Let {x1, x2, . . . , xn} be a set of n d-dimensional points. Then (x1  )2 +

(x2  )2+  +(xn  )2 is minimized when  is the centroid of the points x1, x2, . . . , xn,

namely  = 1

n (x1 + x2 +    + xn).

Proof: Setting the gradient of (x1  )2 + (x2  )2 +    + (xn  )2 with respect to 

to zero yields

2 (x1  )  2 (x2  )      2 (xn  ) = 0.

Solving for  gives  = 1

n (x1 + x2 +    + xn).

3poly(n) means bounded by a polynomial in n.

29

To determine the maximum likelihood estimate of 2 for f , set  to the true centroid.

Next, show that  is set to the standard deviation of the sample. Substitute  = 1

22 and

a = (x1  )2 + (x2  )2 +    + (xn  )2 into the formula for the probability of picking

the points x1, x2, . . . , xn. This gives

ea

ex2dx

(cid:21)n .

(cid:20)

(cid:82)

x

Now, a is xed and  is to be determined. Taking logs, the expression to maximize is

a  n ln



ex2dx

 .



(cid:90)



x

To nd the maximum, dierentiate with respect to , set the derivative to zero, and solve

for . The derivative is

(cid:82)

|x|2ex2dx

a + n

x

Setting y = |



x| in the derivative, yields

a +

n



(cid:82)

x

(cid:82)

ex2dx

.

y2ey2dy

.

ey2dy

y

(cid:82)

y

Since the ratio of the two integrals is the expected distance squared of a d-dimensional

spherical Gaussian of standard deviation 1

2 , we

2

2 . Substituting 2 for 1

get a + nd

2 gives a + nd2. Setting a + nd2 = 0 shows that

the maximum occurs when  =

. Note that this quantity is the square root of the

average coordinate distance squared of the samples to their mean, which is the standard

deviation of the sample. Thus, we get the following lemma.

to its center, and this is known to be d

a

nd





Lemma 2.13 The maximum likelihood spherical Gaussian for a set of samples is the

Gaussian with center equal to the sample mean and standard deviation equal to the stan-

dard deviation of the sample from the true mean.

Let x1, x2, . . . , xn be a sample of points generated by a Gaussian probability distri-

bution. Then  = 1

n (x1 + x2 +    + xn) is an unbiased estimator of the expected value

of the distribution. However, if in estimating the variance from the sample set, we use

the estimate of the expected value rather than the true expected value, we will not get

an unbiased estimate of the variance, since the sample mean is not independent of the

sample set. One should use  = 1

n1(x1 + x2 +    + xn) when estimating the variance.

See Section 12.5.10 of the appendix.

30

2.10 Bibliographic Notes

The word vector model was introduced by Salton [SWY75]. There is vast literature on

the Gaussian distribution, its properties, drawing samples according to it, etc. The reader

can choose the level and depth according to his/her background. The Master Tail Bounds

theorem and the derivation of Cherno and other inequalities from it are from [Kan09].

The original proof of the Random Projection Theorem by Johnson and Lindenstrauss was

complicated. Several authors used Gaussians to simplify the proof. The proof here is due

to Dasgupta and Gupta [DG99]. See [Vem04] for details and applications of the theorem.

[MU05] and [MR95b] are text books covering much of the material touched upon here.

31

2.11 Exercises

Exercise 2.1

1. Let x and y be independent random variables with uniform distribution in [0, 1].

What is the expected value E(x), E(x2), E(x  y), E(xy), and E(x  y)2?

2. Let x and y be independent random variables with uniform distribution in [ 1

What is the expected value E(x), E(x2), E(x  y), E(xy), and E(x  y)2?

2, 1

2].

3. What is the expected squared distance between two points generated at random inside

a unit d-dimensional cube?

Exercise 2.2 Randomly generate 30 points inside the cube [ 1

2]100 and plot distance

between points and the angle between the vectors from the origin to the points for all pairs

of points.

2, 1

Exercise 2.3 Show that for any a  1 there exist distributions for which Markovs in-

equality is tight by showing the following:

1. For each a = 2, 3, and 4 give a probability distribution p(x) for a nonnegative random

variable x where Prob (cid:0)x  a(cid:1) = E(x)

a .

2. For arbitrary a  1 give a probability distribution for a nonnegative random variable

x where Prob (cid:0)x  a(cid:1) = E(x)

a .

Exercise 2.4 Show that for any c  1 there exist distributions for which Chebyshevs

inequality is tight, in other words, P rob(|x  E(x)|  c) = V ar(x)/c2.

Exercise 2.5 Let x be a random variable with probability density 1

zero elsewhere.

4 for 0  x  4 and

1. Use Markovs inequality to bound the probability that x  3.

2. Make use of Prob(|x|  a) = Prob(x2  a2) to get a tighter bound.

3. What is the bound using Prob(|x|  a) = Prob(xr  ar)?

Exercise 2.6 Consider the probability distribution p(x = 0) = 1  1

a and p(x = a) = 1

a.

Plot the probability that x is greater than or equal to a as a function of a for the bound

given by Markovs inequality and by Markovs inequality applied to x2 and x4.

Exercise 2.7 Consider the probability density function p(x) = 0 for x < 1 and p(x) = c 1

x4

for x  1.

1. What should c be to make p a legal probability density function?

2. Generate 100 random samples from this distribution. How close is the average of

the samples to the expected value of x?

32

Exercise 2.8 Let G be a d-dimensional spherical Gaussian with variance 1

rection, centered at the origin. Derive the expected squared distance to the origin.

2 in each di-

Exercise 2.9 Consider drawing a random point x on the surface of the unit sphere in Rd.

What is the variance of x1 (the rst coordinate of x)? See if you can give an argument

without doing any integrals.

Exercise 2.10 How large must  be for 99% of the volume of a 1000-dimensional unit-

radius ball to lie in the shell of -thickness at the surface of the ball?

Exercise 2.11 Prove that 1 + x  ex for all real x. For what values of x is the approxi-

mation 1 + x  ex within 0.01?

Exercise 2.12 For what value of d does the volume, V (d), of a d-dimensional unit ball

take on its maximum? Hint: Consider the ratio V (d)

V (d1).

Exercise 2.13 A 3-dimensional cube has vertices, edges, and faces. In a d-dimensional

cube, these components are called faces. A vertex is a 0-dimensional face, an edge a

1-dimensional face, etc.

1. For 0  k  d, how many k-dimensional faces does a d-dimensional cube have?

2. What is the total number of faces of all dimensions? The d-dimensional face is the

cube itself which you can include in your count.

3. What is the surface area of a unit cube in d-dimensions (a unit cube has side-length

one in each dimension)?

4. What is the surface area of the cube if the length of each side was 2?

5. Prove that the volume of a unit cube is close to its surface.

Exercise 2.14 Consider the portion of the surface area of a unit radius, 3-dimensional

ball with center at the origin that lies within a circular cone whose vertex is at the origin.

What is the formula for the incremental unit of area when using polar coordinates to

integrate the portion of the surface area of the ball that is lying inside the circular cone?

What is the formula for the integral? What is the value of the integral if the angle of the

cone is 36? The angle of the cone is measured from the axis of the cone to a ray on the

surface of the cone.

Exercise 2.15 Consider a unit radius, circular cylinder in 3-dimensions of height one.

The top of the cylinder could be an horizontal plane or half of a circular ball. Consider

these two possibilities for a unit radius, circular cylinder in 4-dimensions. In 4-dimensions

the horizontal plane is 3-dimensional and the half circular ball is 4-dimensional. In each

of the two cases, what is the surface area of the top face of the cylinder? You can use

V (d) for the volume of a unit radius, d-dimension ball and A(d) for the surface area of

a unit radius, d-dimensional ball. An innite length, unit radius, circular cylinder in 4-

dimensions would be the set {(x1, x2, x3, x4)|x2

4  1} where the coordinate x1 is

the axis.

2 + x2

3 + x2

33

Exercise 2.16 Given a d-dimensional circular cylinder of radius r and height h

1. What is the surface area in terms of V (d) and A(d)?

2. What is the volume?

Exercise 2.17 How does the volume of a ball of radius two behave as the dimension of

the space increases? What if the radius was larger than two but a constant independent

of d? What function of d would the radius need to be for a ball of radius r to have

approximately constant volume as the dimension increases? Hint: you may want to use

Stirlings approximation, n!  (cid:0) n

(cid:1)n , for factorial.

e

Exercise 2.18 If lim

d

V (d) = 0, the volume of a d-dimensional ball for suciently large

d must be less than V (3). How can this be if the d-dimensional ball contains the three

dimensional ball?

Exercise 2.19

1. Write a recurrence relation for V (d) in terms of V (d  1) by integrating over x1.

Hint: At x1 = t, the (d  1)-dimensional volume of the slice is the volume of a

1  t2. Express this in terms of V (d  1) and

(d  1)-dimensional sphere of radius

write down the integral. You need not evaluate the integral.



2. Verify the formula for d = 2 and d = 3 by integrating and comparing with V (2) = 

and V (3) = 4

3

Exercise 2.20 Consider a unit ball A centered at the origin and a unit ball B whose

center is at distance s from the origin. Suppose that a random point x is drawn from

the mixture distribution: with probability 1/2, draw at random from A; with probability

d  1 is sucient so that

1/2, draw at random from B. Show that a separation s (cid:29) 1/



d  1, then

Prob(x  A  B) = o(1); i.e., for any (cid:15) > 0 there exists c such that if s  c/

Prob(x  A  B) < (cid:15). In other words, this extent of separation means that nearly all of

the mixture distribution is identiable.



Exercise 2.21 Consider the upper hemisphere of a unit-radius ball in d-dimensions.

What is the height of the maximum volume cylinder that can be placed entirely inside

the hemisphere? As you increase the height of the cylinder, you need to reduce the cylin-

ders radius so that it will lie entirely within the hemisphere.

Exercise 2.22 What is the volume of the maximum size d-dimensional hypercube that

can be placed entirely inside a unit radius d-dimensional ball?

Exercise 2.23 Calculate the ratio of area above the plane x1 = (cid:15) to the area of the upper

hemisphere of a unit radius ball in d-dimensions for (cid:15) = 0.001, 0.01, 0.02, 0.03, 0.04, 0.05

and for d = 100 and d = 1, 000.

34

Exercise 2.24 Almost all of the volume of a ball in high dimensions lies in a narrow

slice of the ball at the equator. However, the narrow slice is determined by the point on

the surface of the ball that is designated the North Pole. Explain how this can be true

if several dierent locations are selected for the location of the North Pole giving rise to

dierent equators.

Exercise 2.25 Explain how the volume of a ball in high dimensions can simultaneously

be in a narrow slice at the equator and also be concentrated in a narrow annulus at the

surface of the ball.

Exercise 2.26 Generate 500 points uniformly at random on the surface of a unit-radius

ball in 50 dimensions. Then randomly generate ve additional points. For each of the ve

new points, calculate a narrow band of width 2

at the equator, assuming the point was

50

the North Pole. How many of the 500 points are in each band corresponding to one of the

ve equators? How many of the points are in all ve bands? How wide do the bands need

to be for all points to be in all ve bands?

Exercise 2.27 Place 100 points at random on a d-dimensional unit-radius ball. Assume

d is large. Pick a random vector and let it dene two parallel hyperplanes on opposite

sides of the origin that are equal distance from the origin. How close can the hyperplanes

be moved and still have at least a .99 probability that all of the 100 points land between

them?

Exercise 2.28 Let x and y be d-dimensional zero mean, unit variance Gaussian vectors.

Prove that x and y are almost orthogonal by considering their dot product.

Exercise 2.29 Prove that with high probability, the angle between two random vectors in

a high-dimensional space is at least 45. Hint: use Theorem 2.8.

d onto a line

Exercise 2.30 Project the volume of a d-dimensional ball of radius

through the center. For large d, give an intuitive argument that the projected volume

should behave like a Gaussian.



Exercise 2.31

1. Write a computer program that generates n points uniformly distributed over the

surface of a unit-radius d-dimensional ball.

2. Generate 200 points on the surface of a sphere in 50 dimensions.

3. Create several random lines through the origin and project the points onto each line.

Plot the distribution of points on each line.

4. What does your result from (3) say about the surface area of the sphere in relation

to the lines, i.e., where is the surface area concentrated relative to each line?

35

Exercise 2.32 If one generates points in d-dimensions with each coordinate a unit vari-

d.

ance Gaussian, the points will approximately lie on the surface of a sphere of radius



1. What is the distribution when the points are projected onto a random line through

the origin?

2. If one uses a Gaussian with variance four, where in d-space will the points lie?

Exercise 2.33 Randomly generate a 100 points on the surface of a sphere in 3-dimensions

and in 100-dimensions. Create a histogram of all distances between the pairs of points in

both cases.

Exercise 2.34 We have claimed that a randomly generated point on a ball lies near the

equator of the ball, independent of the point picked to be the North Pole. Is the same claim

true for a randomly generated point on a cube? To test this claim, randomly generate ten

1 valued vectors in 128 dimensions. Think of these ten vectors as ten choices for the

North Pole. Then generate some additional 1 valued vectors. To how many of the

original vectors is each of the new vectors close to being perpendicular; that is, how many

of the equators is each new vector close to?

Exercise 2.35 Dene the equator of a d-dimensional unit cube to be the hyperplane

(cid:26)

(cid:27)

x

(cid:12)

(cid:12)

(cid:12)

d

(cid:80)

i=1

xi = d

2

.

1. Are the vertices of a unit cube concentrated close to the equator?

2. Is the volume of a unit cube concentrated close to the equator?

3. Is the surface area of a unit cube concentrated close to the equator?

Exercise 2.36 Consider a nonorthogonal basis e1, e2, . . . , ed. The ei are a set of linearly

independent unit vectors that span the space.

1. Prove that the representation of any vector in this basis is unique.



2

2 , 1)e where z is expressed in the basis e1 =

2. Calculate the squared length of z = (

(1, 0) and e2 = (



2

2 )



2

2 ,

i aiei and z = (cid:80)

3. If y = (cid:80)

i biei, with 0 < ai < bi, is it necessarily true that the

length of z is greater than the length of y? Why or why not?

4. Consider the basis e1 = (1, 0) and e2 = (



2

2 ,



2

2 ).

(a) What is the representation of the vector (0,1) in the basis (e1, e2).

(b) What is the representation of the vector (

(c) What is the representation of the vector (1, 2)?

36



2

2 ,



2

2 )?

e2

e2

e2

e1

e1

e1

Exercise 2.37 Generate 20 points uniformly at random on a 900-dimensional sphere of

radius 30. Calculate the distance between each pair of points. Then, select a method of

projection and project the data onto subspaces of dimension k=100, 50, 10, 5, 4, 3, 2, 1

k times the original distances and the new pair-wise

and calculate the dierence between

distances. For each value of k what is the maximum dierence as a percent of





k.

Exercise 2.38 In d-dimensions there are exactly d-unit vectors that are pairwise orthog-

onal. However, if you wanted a set of vectors that were almost orthogonal you might

squeeze in a few more. For example, in 2-dimensions if almost orthogonal meant at least

45 degrees apart, you could t in three almost orthogonal vectors. Suppose you wanted to

nd 1000 almost orthogonal vectors in 100 dimensions. Here are two ways you could do

it:

1. Begin with 1,000 orthonormal 1,000-dimensional vectors, and then project them to

a random 100-dimensional space.

2. Generate 1000 100-dimensional random Gaussian vectors.

Implement both ideas and compare them to see which does a better job.

Exercise 2.39 Suppose there is an object moving at constant velocity along a straight

line. You receive the gps coordinates corrupted by Gaussian noise every minute. How do

you estimate the current position?

Exercise 2.40

1. What is the maximum size rectangle that can be tted under a unit variance Gaus-

sian?

2. What unit area rectangle best approximates a unit variance Gaussian if one measure

goodness of t by the symmetric dierence of the Gaussian and the rectangle.

37

Exercise 2.41 Let x1, x2, . . . , xn be independent samples of a random variable x with

mean  and variance 2. Let ms = 1

xi be the sample mean. Suppose one estimates

n

n

(cid:80)

i=1

the variance using the sample mean rather than the true mean, that is,

2

s =

1

n

n

(cid:88)

i=1

(xi  ms)2

Prove that E(2

s ) = n1

n 2 and thus one should have divided by n  1 rather than n.

Hint: First calculate the variance of the sample mean and show that var(ms) = 1

Then calculate E(2

nvar(x).

i=1(xims)2] by replacing xims with (xim)(msm).

s ) = E[ 1

n

(cid:80)n

Exercise 2.42 Generate ten values by a Gaussian probability distribution with zero mean

and variance one. What is the center determined by averaging the points? What is the

variance? In estimating the variance, use both the real center and the estimated center.

When using the estimated center to estimate the variance, use both n = 10 and n = 9.

How do the three estimates compare?

Exercise 2.43 Suppose you want to estimate the unknown center of a Gaussian in d-

space which has variance one in each direction. Show that O(log d/2) random samples

from the Gaussian are sucient to get an estimate ms of the true center , so that with

probability at least 99%,

(cid:107)  ms(cid:107)  .

How many samples are sucient to ensure that with probability at least 99%

(cid:107)  ms(cid:107)2  ?

Exercise 2.44 Use the probability distribution

1



2

3

e 1

2

(x5)2

9

to generate ten points.

(a) From the ten points estimate . How close is the estimate of  to the true mean of

5?

(b) Using the true mean of 5, estimate 2 by the formula 2 = 1

10

is the estimate of 2 to the true variance of 9?

10

(cid:80)

i=1

(xi  5)2. How close

(c) Using your estimate m of the mean, estimate 2 by the formula 2 = 1

10

How close is the estimate of 2 to the true variance of 9?

(d) Using your estimate m of the mean, estimate 2 by the formula 2 = 1

9

How close is the estimate of 2 to the true variance of 9?

10

(cid:80)

i=1

(xi  m)2.

10

(cid:80)

i=1

(xi  m)2.

38

Exercise 2.45 Create a list of the ve most important things that you learned about high

dimensions.

Exercise 2.46 Write a short essay whose purpose is to excite a college freshman to learn

about high dimensions.

39

3 Best-Fit Subspaces and Singular Value Decompo-

sition (SVD)

3.1 Introduction

In this chapter, we examine the Singular Value Decomposition (SVD) of a matrix.

Consider each row of an n  d matrix A as a point in d-dimensional space. The singular

value decomposition nds the best-tting k-dimensional subspace for k = 1, 2, 3, . . . , for

the set of n data points. Here, best means minimizing the sum of the squares of the

perpendicular distances of the points to the subspace, or equivalently, maximizing the

sum of squares of the lengths of the projections of the points onto this subspace.4 We

begin with a special case where the subspace is 1-dimensional, namely a line through the

origin. We then show that the best-tting k-dimensional subspace can be found by k

applications of the best tting line algorithm, where on the ith iteration we nd the best

t line perpendicular to the previous i  1 lines. When k reaches the rank of the matrix,

from these operations we get an exact decomposition of the matrix called the singular

value decomposition.

In matrix notation, the singular value decomposition of a matrix A with real entries

(we assume all our matrices have real entries) is the factorization of A into the product

of three matrices, A = U DV T , where the columns of U and V are orthonormal5 and the

matrix D is diagonal with positive real entries. The columns of V are the unit length vec-

tors dening the best tting lines described above (the ith column being the unit-length

vector in the direction of the ith line). The coordinates of a row of U will be the fractions

of the corresponding row of A along the direction of each of the lines.

The SVD is useful in many tasks. Often a data matrix A is close to a low rank ma-

trix and it is useful to nd a good low rank approximation to A. For any k, the singular

value decomposition of A gives the best rank-k approximation to A in a well-dened sense.

If ui and vi are columns of U and V respectively, then the matrix equation A = U DV T

can be rewritten as

(cid:88)

A =

diiuivi

T .

i

T is an n  d matrix with the

Since ui is a n  1 matrix and vi is a d  1 matrix, uivi

same dimensions as A. The ith term in the above sum can be viewed as giving the compo-

nents of the rows of A along direction vi. When the terms are summed, they reconstruct A.

4This equivalence is due to the Pythagorean Theorem. For each point, its squared length (its distance

to the origin squared) is exactly equal to the squared length of its projection onto the subspace plus the

squared distance of the point to its projection; therefore, maximizing the sum of the former is equivalent

to minimizing the sum of the latter. For further discussion see Section 3.2.

5A set of vectors is orthonormal if each is of length one and they are pairwise orthogonal.

40

This decomposition of A can be viewed as analogous to writing a vector x in some

orthonormal basis v1, v2, . . . , vd. The coordinates of x = (x  v1, x  v2 . . . , x  vd) are the

projections of x onto the vis. For SVD, this basis has the property that for any k, the

rst k vectors of this basis produce the least possible total sum of squares error for that

value of k.

In addition to the singular value decomposition, there is an eigenvalue decomposition.

Let A be a square matrix. A vector v such that Av = v is called an eigenvector and

 the eigenvalue. When A is symmetric, the eigenvectors are orthogonal and A can be

expressed as A = V DV T where the eigenvectors are the columns of V and D is a diagonal

matrix with the corresponding eigenvalues on its diagonal. For a symmetric matrix A

the singular values and eigenvalues are identical. If the singular values are distinct, then

As singular vectors and eigenvectors are identical. If a singular value has multiplicity d

greater than one, the corresponding singular vectors span a subspace of dimension d and

any orthogonal basis of the subspace can be used as the eigenvectors or singular vectors.6

The singular value decomposition is dened for all matrices, whereas the more fa-

miliar eigenvector decomposition requires that the matrix A be square and certain other

conditions on the matrix to ensure orthogonality of the eigenvectors.

In contrast, the

columns of V in the singular value decomposition, called the right-singular vectors of A,

always form an orthogonal set with no assumptions on A. The columns of U are called

the left-singular vectors and they also form an orthogonal set (see Section 3.6). A simple

consequence of the orthonormality is that for a square and invertible matrix A, the inverse

of A is V D1U T .

Eigenvalues and eignevectors satisfy Av = v. We will show that singular values and

vectors satisfy a somewhat analogous relationship. Since Avi is a n  1 matrix (vector),

the matrix A cannot act on it from the left. But AT , which is a d  n matrix, can act on

this vector. Indeed, we will show that

Avi = diiui

and AT ui = diivi.

In words, A acting on vi produces a scalar multiple of ui and AT acting on ui produces

iivi. The ith singular vector of A is

the same scalar multiple of vi. Note that AT Avi = d2

the ith eigenvector of the square symmetric matrix AT A.

3.2 Preliminaries

Consider projecting a point ai = (ai1, ai2, . . . , aid) onto a line through the origin. Then

i1 + a2

a2

i2 +    + a2

id = (length of projection)2 + (distance of point to line)2 .

6When d = 1 there are actually two possible singular vectors, one the negative of the other. The

subspace spanned is unique.

41

ai

disti

v

Minimizing (cid:80)

alent to maximizing (cid:80)

i is equiv-

proj2

i

dist2

i

i

proji

Figure 3.1: The projection of the point ai onto the line through the origin in the direction

of v.

This holds by the Pythagorean Theorem (see Figure 3.1). Thus

(distance of point to line)2 = a2

i1 + a2

i2 +    + a2

id  (length of projection)2 .

n

(cid:80)

i=1

(a2

Since

i1 + a2

i2 +    + a2

id) is a constant independent of the line, minimizing the sum

of the squares of the distances to the line is equivalent to maximizing the sum of the

squares of the lengths of the projections onto the line. Similarly for best-t subspaces,

maximizing the sum of the squared lengths of the projections onto the subspace minimizes

the sum of squared distances to the subspace.

Thus we have two interpretations of the best-t subspace. The rst is that it minimizes

the sum of squared distances of the data points to it. This rst interpretation and its use

are akin to the notion of least-squares t from calculus.7 The second interpretation of

best-t-subspace is that it maximizes the sum of projections squared of the data points

on it. This says that the subspace contains the maximum content of data among all

subspaces of the same dimension. The choice of the objective function as the sum of

squared distances seems a bit arbitrary and in a way it is. But the square has many nice

mathematical properties. The rst of these, as we have just seen, is that minimizing the

sum of squared distances is equivalent to maximizing the sum of squared projections.

3.3 Singular Vectors

We now dene the singular vectors of an n  d matrix A. Consider the rows of A as

n points in a d-dimensional space. Consider the best t line through the origin. Let v

be a unit vector along this line. The length of the projection of ai, the ith row of A, onto

v is |ai  v|. From this we see that the sum of the squared lengths of the projections is

7But there is a dierence: here we take the perpendicular distance to the line or subspace, whereas,

in the calculus notion, given n pairs, (x1, y1), (x2, y2), . . . , (xn, yn), we nd a line l = {(x, y)|y = mx + b}

minimizing the vertical squared distances of the points to it, namely, (cid:80)n

i=1(yi  mxi  b)2.

42

|Av|2. The best t line is the one maximizing |Av|2 and hence minimizing the sum of the

squared distances of the points to the line.

With this in mind, dene the rst singular vector v1 of A as

v1 = arg max

|v|=1

|Av|.

Technically, there may be a tie for the vector attaining the maximum and so we should

not use the article the; in fact, v1 is always as good as v1. In this case, we arbitrarily

pick one of the vectors achieving the maximum and refer to it as the rst singular vector

avoiding the more cumbersome one of the vectors achieving the maximum. We adopt

this terminology for all uses of arg max .

The value 1 (A) = |Av1| is called the rst singular value of A. Note that 2

1 =

(ai  v1)2 is the sum of the squared lengths of the projections of the points onto the line

n

(cid:80)

i=1

determined by v1.

If the data points were all either on a line or close to a line, intuitively, v1 should

It is possible that data points are not close to one

give us the direction of that line.

line, but lie close to a 2-dimensional subspace or more generally a low dimensional space.

Suppose we have an algorithm for nding v1 (we will describe one such algorithm later).

How do we use this to nd the best-t 2-dimensional plane or more generally the best t

k-dimensional space?

The greedy approach begins by nding v1 and then nds the best 2-dimensional

subspace containing v1. The sum of squared distances helps. For every 2-dimensional

subspace containing v1, the sum of squared lengths of the projections onto the subspace

equals the sum of squared projections onto v1 plus the sum of squared projections along

a vector perpendicular to v1 in the subspace. Thus, instead of looking for the best 2-

dimensional subspace containing v1, look for a unit vector v2 perpendicular to v1 that

maximizes |Av|2 among all such unit vectors. Using the same greedy strategy to nd the

best three and higher dimensional subspaces, denes v3, v4, . . . in a similar manner. This

is captured in the following denitions. There is no apriori guarantee that the greedy

algorithm gives the best t. But, in fact, the greedy algorithm does work and yields the

best-t subspaces of every dimension as we will show.

The second singular vector , v2, is dened by the best t line perpendicular to v1.

The value 2 (A) = |Av2| is called the second singular value of A. The third singular

v2 = arg max

|Av|

vv1

|v|=1

43

vector v3 and the third singular value are dened similarly by

v3 = arg max

vv1,v2

|v|=1

|Av|

and

3(A) = |Av3|,

and so on. The process stops when we have found singular vectors v1, v2, . . . , vr, singular

values 1, 2, . . . , r, and

max

vv1,v2,...,vr

|v|=1

|Av| = 0.

The greedy algorithm found the v1 that maximized |Av| and then the best t 2-

dimensional subspace containing v1. Is this necessarily the best-t 2-dimensional sub-

space overall? The following theorem establishes that the greedy algorithm nds the best

subspaces of every dimension.

Theorem 3.1 (The Greedy Algorithm Works) Let A be an nd matrix with singu-

lar vectors v1, v2, . . . , vr. For 1  k  r, let Vk be the subspace spanned by v1, v2, . . . , vk.

For each k, Vk is the best-t k-dimensional subspace for A.

Proof: The statement is obviously true for k = 1. For k = 2, let W be a best-t 2-

dimensional subspace for A. For any orthonormal basis (w1, w2) of W , |Aw1|2 + |Aw2|2

is the sum of squared lengths of the projections of the rows of A onto W . Choose an

orthonormal basis (w1, w2) of W so that w2 is perpendicular to v1. If v1 is perpendicular

to W , any unit vector in W will do as w2. If not, choose w2 to be the unit vector in W

perpendicular to the projection of v1 onto W. This makes w2 perpendicular to v1.8 Since

v1 maximizes |Av|2, it follows that |Aw1|2  |Av1|2. Since v2 maximizes |Av|2 over all

v perpendicular to v1, |Aw2|2  |Av2|2. Thus

Hence, V2 is at least as good as W and so is a best-t 2-dimensional subspace.

|Aw1|2 + |Aw2|2  |Av1|2 + |Av2|2.

For general k, proceed by induction. By the induction hypothesis, Vk1 is a best-t

k-1 dimensional subspace. Suppose W is a best-t k-dimensional subspace. Choose an

orthonormal basis w1, w2, . . . , wk of W so that wk is perpendicular to v1, v2, . . . , vk1.

Then

|Aw1|2 + |Aw2|2 +    + |Awk1|2  |Av1|2 + |Av2|2 +    + |Avk1|2

since Vk1 is an optimal k  1 dimensional subspace. Since wk is perpendicular to

v1, v2, . . . , vk1, by the denition of vk, |Awk|2  |Avk|2. Thus

|Aw1|2 + |Aw2|2 +    + |Awk1|2 + |Awk|2  |Av1|2 + |Av2|2 +    + |Avk1|2 + |Avk|2,

proving that Vk is at least as good as W and hence is optimal.

8This can be seen by noting that v1 is the sum of two vectors that each are individually perpendicular

to w2, namely the projection of v1 to W and the portion of v1 orthogonal to W .

44

Note that the n-dimensional vector Avi is a list of lengths (with signs) of the projec-

tions of the rows of A onto vi. Think of |Avi| = i(A) as the component of the matrix

A along vi. For this interpretation to make sense, it should be true that adding up the

squares of the components of A along each of the vi gives the square of the whole content

of A. This is indeed the case and is the matrix analogy of decomposing a vector into its

components along orthogonal directions.

Consider one row, say aj, of A. Since v1, v2, . . . , vr span the space of all rows of A,

(aj  vi)2 =

aj  v = 0 for all v perpendicular to v1, v2, . . . , vr. Thus, for each row aj,

r

(cid:80)

i=1

|aj|2. Summing over all rows j,

n

(cid:88)

j=1

|aj|2 =

n

(cid:88)

r

(cid:88)

j=1

i=1

(aj  vi)2 =

r

(cid:88)

n

(cid:88)

i=1

j=1

(aj  vi)2 =

r

(cid:88)

i=1

|Avi|2 =

r

(cid:88)

i=1

2

i (A).

But

n

(cid:80)

j=1

|aj|2 =

n

(cid:80)

j=1

d

(cid:80)

k=1

a2

jk, the sum of squares of all the entries of A. Thus, the sum of

squares of the singular values of A is indeed the square of the whole content of A, i.e.,

the sum of squares of all the entries. There is an important norm associated with this

quantity, the Frobenius norm of A, denoted ||A||F dened as

||A||F =

(cid:115)(cid:88)

j,k

a2

jk.

Lemma 3.2 For any matrix A, the sum of squares of the singular values equals the square

of the Frobenius norm. That is, (cid:80) 2

i (A) = ||A||2

F .

Proof: By the preceding discussion.

The vectors v1, v2, . . . , vr are called the right-singular vectors. The vectors Avi form

a fundamental set of vectors and we normalize them to length one by

ui =

1

i(A)

Avi.

Later we will show that ui similarly maximizes |uT A| over all u perpendicular to u1, . . . , ui1.

These ui are called the left-singular vectors. Clearly, the right-singular vectors are orthog-

onal by denition. We will show later that the left-singular vectors are also orthogonal.

3.4 Singular Value Decomposition (SVD)

Let A be an n  d matrix with singular vectors v1, v2, . . . , vr and corresponding

Avi where iui is

singular values 1, 2, . . . , r. The left-singular vectors of A are ui = 1

i

45

a vector whose coordinates correspond to the projections of the rows of A onto vi. Each

iuivT

is a rank one matrix whose rows are the vi components of the rows of A, i.e., the

i

projections of the rows of A in the vi direction. We will prove that A can be decomposed

into a sum of rank one matrices as

A =

r

(cid:88)

i=1

iuivT

i .

Geometrically, each point is decomposed in A into its components along each of the r

orthogonal directions given by the vi. We will also prove this algebraically. We begin

with a simple lemma that two matrices A and B are identical if Av = Bv for all v.

Lemma 3.3 Matrices A and B are identical if and only if for all vectors v, Av = Bv.

Proof: Clearly, if A = B then Av = Bv for all v. For the converse, suppose that

Av = Bv for all v. Let ei be the vector that is all zeros except for the ith component

which has value one. Now Aei is the ith column of A and thus A = B if for each i,

Aei = Bei.

Theorem 3.4 Let A be an n  d matrix with right-singular vectors v1, v2, . . . , vr, left-

singular vectors u1, u2, . . . , ur, and corresponding singular values 1, 2, . . . , r. Then

A =

r

(cid:88)

i=1

iuivT

i .

Proof: We rst show that multiplying both A and

r

(cid:80)

i=1

iuivT

i by vj results in equality.

r

(cid:88)

i=1

iuivT

i vj = juj = Avj

Since any vector v can be expressed as a linear combination of the singular vectors

iuivT

i v for all v and by Lemma 3.3,

plus a vector perpendicular to the vi, Av =

A =

r

(cid:80)

i=1

iuivT

i .

r

(cid:80)

i=1

i iuivT

i

The decomposition A = (cid:80)

is called the singular value decomposition, SVD,

of A. We can rewrite this equation in matrix notation as A = U DV T where ui is the ith

column of U , vT

is the ith row of V T , and D is a diagonal matrix with i as the ith entry

i

on its diagonal. For any matrix A, the sequence of singular values is unique and if the

singular values are all distinct, then the sequence of singular vectors is unique up to signs.

However, when some set of singular values are equal, the corresponding singular vectors

span some subspace. Any set of orthonormal vectors spanning this subspace can be used

as the singular vectors.

46

D

r  r

V T

r  d

A

n  d

U

n  r

=

Figure 3.2: The SVD decomposition of an n  d matrix.

3.5 Best Rank-k Approximations

Let A be an n  d matrix and think of the rows of A as n points in d-dimensional

space. Let

A =

r

(cid:88)

i=1

iuivT

i

be the SVD of A. For k  {1, 2, . . . , r}, let

Ak =

k

(cid:88)

i=1

iuivT

i

be the sum truncated after k terms. It is clear that Ak has rank k. We show that Ak

is the best rank k approximation to A, where error is measured in the Frobenius norm.

Geometrically, this says that v1, . . . , vk dene the k-dimensional space minimizing the

sum of squared distances of the points to the space. To see why, we need the following

lemma.

Lemma 3.5 The rows of Ak are the projections of the rows of A onto the subspace Vk

spanned by the rst k singular vectors of A.

Proof: Let a be an arbitrary row vector. Since the vi are orthonormal, the projection

of the vector a onto Vk is given by (cid:80)k

T . Thus, the matrix whose rows are

i=1 (a  vi)vi

the projections of the rows of A onto Vk is given by (cid:80)k

i . This last expression

simplies to

i=1 AvivT

iuivi

T = Ak.

k

(cid:88)

i=1

Avivi

T =

k

(cid:88)

i=1

47

Theorem 3.6 For any matrix B of rank at most k

(cid:107)A  Ak(cid:107)F  (cid:107)A  B(cid:107)F

Proof: Let B minimize (cid:107)A  B(cid:107)2

F among all rank k or less matrices. Let V be the space

spanned by the rows of B. The dimension of V is at most k. Since B minimizes (cid:107)A  B(cid:107)2

F ,

it must be that each row of B is the projection of the corresponding row of A onto V :

Otherwise replace the row of B with the projection of the corresponding row of A onto

V . This still keeps the row space of B contained in V and hence the rank of B is still at

most k. But it reduces (cid:107)A  B(cid:107)2

F , contradicting the minimality of ||A  B||F .

Since each row of B is the projection of the corresponding row of A, it follows that

(cid:107)A  B(cid:107)2

F is the sum of squared distances of rows of A to V . Since Ak minimizes the

sum of squared distance of rows of A to any k-dimensional subspace, from Theorem 3.1,

it follows that (cid:107)A  Ak(cid:107)F  (cid:107)A  B(cid:107)F .

i=1 iuivi

In addition to the Frobenius norm, there is another matrix norm of interest. Consider

an n  d matrix A and a large number of vectors where for each vector x we wish to

compute Ax. It takes time O(nd) to compute each product Ax but if we approximate

A by Ak = (cid:80)k

T and approximate Ax by Akx it requires only k dot products

of d-dimensional vectors, followed by a sum of k n-dimensional vectors, and takes time

O(kd + kn), which is a win provided k (cid:28) min(d, n). How is the error measured? Since x

is unknown, the approximation needs to be good for every x. So we take the maximum

over all x of |(Ak  A)x|. Since this would be innite if |x| could grow without bound,

we restrict the maximum to |x|  1. Formally, we dene a new norm of a matrix A by

||A||2 = max

|x|1

|Ax|.

This is called the 2-norm or the spectral norm. Note that it equals 1(A).

As an application consider a large database of documents that form rows of an n  d

matrix A. There are d terms and each document is a d-dimensional vector with one

component for each term, which is the number of occurrences of the term in the document.

We are allowed to preprocess A. After the preprocessing, we receive queries. Each

query x is an d-dimensional vector which species how important each term is to the

query. The desired answer is an n-dimensional vector which gives the similarity (dot

product) of the query to each document in the database, namely Ax, the matrix-vector

product. Query time is to be much less than preprocessing time, since the idea is that we

need to answer many queries for the same database. There are many other applications

where one performs many matrix vector products with the same matrix. This technique

is applicable to these situations as well.

3.6 Left Singular Vectors

The left singular vectors are also pairwise orthogonal. Intuitively if ui and uj, i < j, were

not orthogonal, one would suspect that the right singular vector vj had a component of vi

48

which would contradict that vi and vj were orthogonal. Let i be the smallest integer such

that ui is not orthogonal to all other uj. Then to prove that ui and uj are orthogonal,

we add a small component of vj to vi, normalize the result to be a unit vector

v(cid:48)

i =

vi + (cid:15)vj

|vi + (cid:15)vj|

and show that |Av(cid:48)

i| > |Avi|, a contradiction.

Theorem 3.7 The left singular vectors are pairwise orthogonal.

Proof: Let i be the smallest integer such that ui is not orthogonal to some other uj.

Without loss of generality assume that uT

T uj < 0 then just replace ui

with ui. Clearly j > i since i was selected to be the smallest such index. For  > 0, let

i uj =  > 0. If ui

v(cid:48)

i =

vi + (cid:15)vj

|vi + (cid:15)vj|

.

Notice that v(cid:48)

i is a unit-length vector.

Av(cid:48)

i =

iui + juj

1 + 2



has length at least as large as its component along ui which is

uT

i

(cid:18) iui + juj



1 + 2

(cid:19)

> (i + j)

(cid:16)

1  2

2

(cid:17)

> i  2

2 i + j  3

2 j > i,

for suciently small (cid:15), a contradiction since vi + vj is orthogonal to v1, v2, . . . , vi1 since

j > i and i is dened to be the maximum of |Av| over such vectors.

Next we prove that Ak is the best rank k, 2-norm approximation to A. We rst show

that the square of the 2-norm of A  Ak is the square of the (k + 1)st singular value of A.

This is essentially by denition of Ak; that is, Ak represents the projections of the rows in

A onto the space spanned by the top k singular vectors, and so A  Ak is the remaining

portion of those rows, whose top singular value will be k+1.

Lemma 3.8 (cid:107)A  Ak(cid:107)2

2 = 2

k+1.

Proof: Let A =

and A  Ak =

r

(cid:80)

i=1

r

(cid:80)

i=k+1

iuivi

T be the singular value decomposition of A. Then Ak =

k

(cid:80)

i=1

iuivi

T

iuivi

T . Let v be the top singular vector of A  Ak. Express v as a

linear combination of v1, v2, . . . , vr. That is, write v =

cjvj. Then

r

(cid:80)

j=1

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

r

(cid:88)

i=k+1

ciiuivi

T vi

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

|(A  Ak)v| =

=

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

r

(cid:88)

i=k+1

r

(cid:88)

i=k+1

iuivi

T

r

(cid:88)

j=1

cjvj

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

=

ciiui

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:118)

(cid:117)

(cid:117)

(cid:116)

=

r

(cid:88)

i=k+1

i 2

c2

i ,

49

since the ui are orthonormal. The v maximizing this last quantity, subject to the con-

c2

straint that |v|2 =

i = 1, occurs when ck+1 = 1 and the rest of the ci are zero. Thus,

r

(cid:80)

i=1

(cid:107)A  Ak(cid:107)2

2 = 2

k+1 proving the lemma.

Finally, we prove that Ak is the best rank k, 2-norm approximation to A:

Theorem 3.9 Let A be an n  d matrix. For any matrix B of rank at most k

(cid:107)A  Ak(cid:107)2  (cid:107)A  B(cid:107)2 .

Proof: If A is of rank k or less, the theorem is obviously true since (cid:107)A  Ak(cid:107)2 = 0.

Assume that A is of rank greater than k. By Lemma 3.8, (cid:107)A  Ak(cid:107)2

k+1. The null

space of B, the set of vectors v such that Bv = 0, has dimension at least d  k. Let

v1, v2, . . . , vk+1 be the rst k + 1 singular vectors of A. By a dimension argument, it

follows that there exists a z (cid:54)= 0 in

2 = 2

Null (B)  Span {v1, v2, . . . , vk+1} .

Scale z to be of length one.

Since Bz = 0,

(cid:107)A  B(cid:107)2

2  |(A  B) z|2 .

(cid:107)A  B(cid:107)2

2  |Az|2 .

Since z is in the Span {v1, v2, . . . , vk+1}

|Az|2 =

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

n

(cid:88)

i=1

iuivi

T z

2

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

=

n

(cid:88)

i=1

2

i

(cid:0)vi

T z(cid:1)2

=

k+1

(cid:88)

i=1

2

i

(cid:0)vi

T z(cid:1)2

 2

k+1

k+1

(cid:88)

i=1

(cid:0)vi

T z(cid:1)2

= 2

k+1.

It follows that (cid:107)A  B(cid:107)2

2  2

k+1 proving the theorem.

For a square symmetric matrix A and eigenvector v, Av = v. We now prove the

analog for singular values and vectors we discussed in the introduction.

Lemma 3.10 (Analog of eigenvalues and eigenvectors)

Avi = iui and AT ui = ivi.

Proof: The rst equation follows from the denition of left singular vectors. For the

second, note that from the SVD, we get AT ui = (cid:80)

T ui, where since the uj are

orthonormal, all terms in the summation are zero except for j = i.

j jvjuj

50

3.7 Power Method for Singular Value Decomposition

Computing the singular value decomposition is an important branch of numerical

analysis in which there have been many sophisticated developments over a long period of

time. The reader is referred to numerical analysis texts for more details. Here we present

an in-principle method to establish that the approximate SVD of a matrix A can be

computed in polynomial time. The method we present, called the power method, is simple

and is in fact the conceptual starting point for many algorithms. Let A be a matrix whose

SVD is (cid:80)

T . We wish to work with a matrix that is square and symmetric. Let

B = AT A. By direct multiplication, using the orthogonality of the uis that was proved

in Theorem 3.7,

i iuivi

B = AT A =

(cid:32)

(cid:88)

i

(cid:33) (cid:32)

(cid:88)

(cid:33)

jujvT

j

iviuT

i

(cid:88)

=

i,j

ijvi(uT

i  uj)vT

j =

j

(cid:88)

i

i vivT

2

i .

The matrix B is square and symmetric, and has the same left and right-singular vectors.

In particular, Bvj = ((cid:80)

j vj, so vj is an eigenvector of B with eigenvalue

2

j . If A is itself square and symmetric, it will have the same right and left-singular vec-

tors, namely A = (cid:80)

T and computing B is unnecessary.

i )vj = 2

i vivT

i 2

ivivi

i

Now consider computing B2.

B2 =

(cid:32)

(cid:88)

i

2

i vivT

i

(cid:33) (cid:32)

(cid:88)

j

2

j vjvT

j

(cid:33)

(cid:88)

=

ij

2

i 2

j vi(vi

T vj)vj

T

When i (cid:54)= j, the dot product vi

T vj is zero by orthogonality.9 Thus, B2 =

computing the kth power of B, all the cross product terms are zero and

r

(cid:80)

i=1

4

i vivi

T . In

Bk =

r

(cid:88)

i=1

2k

i vivi

T .

If 1 > 2, then the rst term in the summation dominates, so Bk  2k

T . This

means a close estimate to v1 can be computed by simply taking the rst column of Bk

and normalizing it to a unit vector.

1 v1v1

3.7.1 A Faster Method

A problem with the above method is that A may be a very large, sparse matrix, say a

108  108 matrix with 109 nonzero entries. Sparse matrices are often represented by just

9The outer product vivj

T is a matrix and is not zero even for i (cid:54)= j.

51

a list of nonzero entries, say a list of triples of the form (i, j, aij). Though A is sparse, B

need not be and in the worse case may have all 1016 entries nonzero10 and it is then impos-

sible to even write down B, let alone compute the product B2. Even if A is moderate in

size, computing matrix products is costly in time. Thus, a more ecient method is needed.

Instead of computing Bk, select a random vector x and compute the product Bkx.

The vector x can be expressed in terms of the singular vectors of B augmented to a full

orthonormal basis as x =

d

(cid:80)

i=1

civi. Then

Bkx  (2k

1 v1v1

T )

(cid:16) d

(cid:88)

(cid:17)

civi

i=1

= 2k

1 c1v1.

Normalizing the resulting vector yields v1, the rst singular vector of A. The way Bkx

is computed is by a series of matrix vector products, instead of matrix products. Bkx =

AT A . . . AT Ax, which can be computed right-to-left. This consists of 2k vector times

sparse matrix multiplications.

To compute k singular vectors, one selects a random vector r and nds an orthonormal

basis for the space spanned by r, Ar, . . . , Ak1r. Then compute A times each of the basis

vectors, and nd an orthonormal basis for the space spanned by the resulting vectors.

Intuitively, one has applied A to a subspace rather than a single vector. One repeat-

edly applies A to the subspace, calculating an orthonormal basis after each application

to prevent the subspace collapsing to the one dimensional subspace spanned by the rst

singular vector. The process quickly converges to the rst k singular vectors.

An issue occurs if there is no signicant gap between the rst and second singular

values of a matrix. Take for example the case when there is a tie for the rst singular

vector and 1 = 2. Then, the above argument fails. We will overcome this hurdle.

Theorem 3.11 below states that even with ties, the power method converges to some

vector in the span of those singular vectors corresponding to the nearly highest singular

values. The theorem assumes it is given a vector x which has a component of magnitude

at least  along the rst right singular vector v1 of A. We will see in Lemma 3.12 that a

random vector satises this condition with fairly high probability.

Theorem 3.11 Let A be an nd matrix and x a unit length vector in Rd with |xT v1|  ,

where  > 0. Let V be the space spanned by the right singular vectors of A corresponding

to singular values greater than (1  ) 1. Let w be the unit vector after k = ln(1/)

iterations of the power method, namely,

2

w =

(cid:0)AT A(cid:1)k x

(cid:12)

(cid:12)

(cid:12)(AT A)k x

(cid:12)

(cid:12)

(cid:12)

.

10E.g., suppose each entry in the rst row of A is nonzero and the rest of A is zero.

52

Then w has a component of at most  perpendicular to V .

Proof: Let

A =

r

(cid:88)

i=1

iuivT

i

be the SVD of A.

If the rank of A is less than d, then for convenience complete

{v1, v2, . . . vr} into an orthonormal basis {v1, v2, . . . vd} of d-space. Write x in the basis

of the vis as

d

(cid:88)

x =

civi.

Since (AT A)k =

|c1|  .

d

(cid:80)

i=1

i vivT

2k

i ,

it follows that (AT A)kx =

d

(cid:80)

i=1

2k

i civi. By hypothesis,

i=1

Suppose that 1, 2, . . . , m are the singular values of A that are greater than or equal

to (1  ) 1 and that m+1, . . . , d are the singular values that are less than (1  ) 1.

Now

|(AT A)kx|2 =

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

d

(cid:88)

i=1

2k

i civi

2

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

=

d

(cid:88)

i=1

i c2

4k

i  4k

1 c2

1  4k

1 2.

The component of |(AT A)kx|2 perpendicular to the space V is

d

(cid:88)

i=m+1

i c2

4k

i  (1  )4k 4k

1

d

(cid:88)

i=m+1

i  (1  )4k 4k

c2

1

since (cid:80)d

i=1 c2

length at most (1)4k4k

1

4k

1 2

i = |x| = 1. Thus, the component of w perpendicular to V has squared

and so its length is at most

(1  )2k2k

1

2k

1

=

(1  )2k





e2k



= 

since k = ln(1/(cid:15)

2(cid:15)

.

Lemma 3.12 Let y  Rn be a random vector with the unit variance spherical Gaussian

as its probability density. Normalize y to be a unit length vector by setting x = y/|y|. Let

v be any unit length vector. Then

(cid:18)

|xT v| 

Prob

(cid:19)

1



20

d



1

10

+ 3ed/96.

53

(cid:16)

(cid:17)

Proof: Proving for the unit length vector x that Prob

10 + 3ed/96 is

d

d)  3ed/96 and

equivalent to proving for the unnormalized vector y that Prob(|y|  2

Prob(|yT v|  1

d) is at most 3ed/96 follows from Theorem

10 is at most 1/10 follows

(2.9) with

from the fact that yT v is a random, zero mean, unit variance Gaussian with density is at

most 1/

2  1/2 in the interval [1/10, 1/10], so the integral of the Gaussian over the

interval is at most 1/10.

d substituted for . The probability that |yT v|  1

10)  1/10. That Prob(|y|  2

|xT v|  1



20

 1









3.8 Singular Vectors and Eigenvectors

For a square matrix B, if Bx = x, then x is an eigenvector of B and  is the corre-

sponding eigenvalue. We saw in Section 3.7, if B = AT A, then the right singular vectors

vj of A are eigenvectors of B with eigenvalues 2

j . The same argument shows that the left

singular vectors uj of A are eigenvectors of AAT with eigenvalues 2

j .

i 2

i vivi

T and for any x, xT vivi

The matrix B = AT A has the property that for any vector x, xT Bx  0. This is

because B = (cid:80)

T x = (xT vi)2  0. A matrix B with

the property that xT Bx  0 for all x is called positive semi-denite. Every matrix of

the form AT A is positive semi-denite. In the other direction, any positive semi-denite

matrix B can be decomposed into a product AT A, and so its eigenvalue decomposition

can be obtained from the singular value decomposition of A. The interested reader should

consult a linear algebra book.

3.9 Applications of Singular Value Decomposition

3.9.1 Centering Data

Singular value decomposition is used in many applications and for some of these ap-

plications it is essential to rst center the data by subtracting the centroid of the data

from each data point.11 If you are interested in the statistics of the data and how it varies

in relationship to its mean, then you would center the data. On the other hand, if you

are interested in nding the best low rank approximation to a matrix, then you do not

center the data. The issue is whether you are nding the best tting subspace or the best

tting ane space. In the latter case you rst center the data and then nd the best

tting subspace. See Figure 3.3.

We rst show that the line minimizing the sum of squared distances to a set of points,

if not restricted to go through the origin, must pass through the centroid of the points.

This implies that if the centroid is subtracted from each data point, such a line will pass

through the origin. The best t line can be generalized to k dimensional planes. The

operation of subtracting the centroid from all data points is useful in other contexts as

well. We give it the name centering data.

11The centroid of a set of points is the coordinate-wise average of the points.

54

Figure 3.3: If one wants statistical information relative to the mean of the data, one

needs to center the data. If one wants the best low rank approximation, one would not

center the data.

Lemma 3.13 The best-t line (minimizing the sum of perpendicular distances squared)

of a set of data points must pass through the centroid of the points.

Proof: Subtract the centroid from each data point so that the centroid is 0. After

centering the data let (cid:96) be the best-t line and assume for contradiction that (cid:96) does

not pass through the origin. The line (cid:96) can be written as {a + v|  R}, where a is

the closest point to 0 on (cid:96) and v is a unit length vector in the direction of (cid:96), which is

perpendicular to a. For a data point ai, let dist(ai, (cid:96)) denote its perpendicular distance to

(cid:96). By the Pythagorean theorem, we have |ai  a|2 = dist(ai, (cid:96))2 + (v  ai)2, or equivalently,

dist(ai, (cid:96))2 = |ai  a|2  (v  ai)2. Summing over all data points:

n

(cid:88)

i=1

dist(ai, (cid:96))2 =

n

(cid:88)

i=1

(cid:0)|ai  a|2  (v  ai)2(cid:1) =

n

(cid:88)

i=1

(cid:0)|ai|2 + |a|2  2ai  a  (v  ai)2(cid:1)

=

n

(cid:88)

i=1

|ai|2 + n|a|2  2a 

(cid:32)

(cid:88)

(cid:33)

ai



i

n

(cid:88)

i=1

(v  ai)2 =

(cid:88)

i

|ai|2 + n|a|2 

(cid:88)

(v  ai)2,

i

where we used the fact that since the centroid is 0, (cid:80)

i ai = 0. The above expression is

minimized when a = 0, so the line (cid:96)(cid:48) = {v :   R} through the origin is a better t

than (cid:96), contradicting (cid:96) being the best-t line.

A statement analogous to Lemma 3.13 holds for higher dimensional objects. Dene

an ane space as a subspace translated by a vector. So an ane space is a set of the

form

k

(cid:88)

{v0 +

civi|c1, c2, . . . , ck  R}.

i=1

Here, v0 is the translation and v1, v2, . . . , vk form an orthonormal basis for the subspace.

Lemma 3.14 The k dimensional ane space which minimizes the sum of squared per-

pendicular distances to the data points must pass through the centroid of the points.

55

Proof: We only give a brief idea of the proof, which is similar to the previous lemma.

Instead of (v  ai)2, we will now have (cid:80)k

j=1(vj  ai)2, where the vj, j = 1, 2, . . . , k are an

orthonormal basis of the subspace through the origin parallel to the ane space.

3.9.2 Principal Component Analysis

The traditional use of SVD is in Principal Component Analysis (PCA). PCA is il-

lustrated by a movie recommendation setting where there are n customers and d movies.

Let matrix A with elements aij represent the amount that customer i likes movie j. One

hypothesizes that there are only k underlying basic factors that determine how much a

given customer will like a given movie, where k is much smaller than n or d. For example,

these could be the amount of comedy, drama, and action, the novelty of the story, etc.

Each movie can be described as a k-dimensional vector indicating how much of these ba-

sic factors the movie has, and each customer can be described as a k-dimensional vector

indicating how important each of these basic factors is to that customer. The dot-product

of these two vectors is hypothesized to determine how much that customer will like that

movie. In particular, this means that the n  d matrix A can be expressed as the product

of an n  k matrix U describing the customers and a k  d matrix V describing the

movies. Finding the best rank k approximation Ak by SVD gives such a U and V . One

twist is that A may not be exactly equal to U V , in which case A  U V is treated as

noise. Another issue is that SVD gives a factorization with negative entries. Nonnegative

matrix factorization (NMF) is more appropriate in some contexts where we want to keep

entries nonnegative. NMF is discussed in Chapter 9

In the above setting, A was available fully and we wished to nd U and V to identify

the basic factors. However, in a case such as movie recommendations, each customer may

have seen only a small fraction of the movies, so it may be more natural to assume that we

are given just a few elements of A and wish to estimate A. If A was an arbitrary matrix

of size n  d, this would require (nd) pieces of information and cannot be done with a

few entries. But again hypothesize that A was a small rank matrix with added noise. If

now we also assume that the given entries are randomly drawn according to some known

distribution, then there is a possibility that SVD can be used to estimate the whole of A.

This area is called collaborative ltering and one of its uses is to recommend movies or to

target an ad to a customer based on one or two purchases. We do not describe it here.

3.9.3 Clustering a Mixture of Spherical Gaussians

Clustering is the task of partitioning a set of points into k subsets or clusters where

each cluster consists of nearby points. Dierent denitions of the quality of a clustering

lead to dierent solutions. Clustering is an important area which we will study in detail

in Chapter 7. Here we will see how to solve a particular clustering problem using singular

value decomposition.

56





























factors





























=





























U

































A

customers

movies

V





Figure 3.4: Customer-movie data

Mathematical formulations of clustering tend to have the property that nding the

highest quality solution to a given set of data is NP-hard. One way around this is to

assume stochastic models of input data and devise algorithms to cluster data generated by

such models. Mixture models are a very important class of stochastic models. A mixture

is a probability density or distribution that is the weighted sum of simple component

probability densities. It is of the form

f = w1p1 + w2p2 +    + wkpk,

where p1, p2, . . . , pk are the basic probability densities and w1, w2, . . . , wk are positive real

numbers called mixture weights that add up to one. Clearly, f is a probability density

and integrates to one.

The model tting problem is to t a mixture of k basic densities to n independent,

identically distributed samples, each sample drawn according to the same mixture dis-

tribution f . The class of basic densities is known, but various parameters such as their

means and the component weights of the mixture are not. Here, we deal with the case

where the basic densities are all spherical Gaussians. There are two equivalent ways of

thinking of the hidden sample generation process when only the samples are given:

1. Pick each sample according to the density f on Rd.

2. Pick a random i from {1, 2, . . . , k} where probability of picking i is wi. Then, pick

a sample according to the density pi.

One approach to the model-tting problem is to break it into two subproblems:

1. First, cluster the set of samples into k clusters C1, C2, . . . , Ck, where Ci is the set of

samples generated according to pi (see (2) above) by the hidden generation process.

2. Then t a single Gaussian distribution to each cluster of sample points.

57

The second problem is relatively easier and indeed we saw the solution in Chapter

2, where we showed that taking the empirical mean (the mean of the sample) and the

empirical standard deviation gives us the best-t Gaussian. The rst problem is harder

and this is what we discuss here.

If the component Gaussians in the mixture have their centers very close together, then

the clustering problem is unresolvable. In the limiting case where a pair of component

densities are the same, there is no way to distinguish between them. What condition on

the inter-center separation will guarantee unambiguous clustering? First, by looking at

1-dimensional examples, it is clear that this separation should be measured in units of the

standard deviation, since the density is a function of the number of standard deviation

from the mean. In one dimension, if two Gaussians have inter-center separation at least

six times the maximum of their standard deviations, then they hardly overlap. This is

summarized in the question: How many standard deviations apart are the means? In one

dimension, if the answer is at least six, we can easily tell the Gaussians apart. What is

the analog of this in higher dimensions?

We discussed in Chapter 2 distances between two sample points from the same Gaus-

sian as well the distance between two sample points from two dierent Gaussians. Recall

from that discussion that if

If x and y are two independent samples from the same spherical Gaussian with

standard deviation12  then

|x  y|2  2(cid:0)

d  O(1)(cid:1)22.

If x and y are samples from dierent spherical Gaussians each of standard deviation

 and means separated by distance , then

|x  y|2  2(cid:0)

d  O(1)(cid:1)22 + 2.

To ensure that points from the same Gaussian are closer to each other than points from

dierent Gaussians, we need

2(cid:0)

d  O(1)(cid:1)22 + 2 > 2(cid:0)

d + O(1)(cid:1)22.

Expanding the squares, the high order term 2d cancels and we need that

 > cd1/4,

for some constant c. While this was not a completely rigorous argument, it can be used to

show that a distance based clustering approach (see Chapter 2 for an example) requires an

12Since a spherical Gaussian has the same standard deviation in every direction, we call it the standard

deviation of the Gaussian.

58

inter-mean separation of at least cd1/4 standard deviations to succeed, thus unfortunately

not keeping with mnemonic of a constant number of standard deviations separation of

the means. Here, indeed, we will show that (1) standard deviations suce provided the

number k of Gaussians is O(1).

The central idea is the following. Suppose we can nd the subspace spanned by the

k centers and project the sample points to this subspace. The projection of a spherical

Gaussian with standard deviation  remains a spherical Gaussian with standard deviation

 (Lemma 3.15). In the projection, the inter-center separation remains the same. So in

the projection, the Gaussians are distinct provided the inter-center separation in the whole

space is at least ck1/4  which is less than cd1/4  for k (cid:28) d. Interestingly, we will see that

the subspace spanned by the k-centers is essentially the best-t k-dimensional subspace

that can be found by singular value decomposition.

Lemma 3.15 Suppose p is a d-dimensional spherical Gaussian with center  and stan-

dard deviation . The density of p projected onto a k-dimensional subspace V is a spherical

Gaussian with the same standard deviation.

Proof: Rotate the coordinate system so V is spanned by the rst k coordinate vectors.

The Gaussian remains spherical with standard deviation  although the coordinates of

its center have changed. For a point x = (x1, x2, . . . , xd), we will use the notation x(cid:48) =

(x1, x2, . . . xk) and x(cid:48)(cid:48) = (xk+1, xk+2, . . . , xn). The density of the projected Gaussian at

the point (x1, x2, . . . , xk) is

|x(cid:48)(cid:48)|2

22

(cid:90)

|x(cid:48)(cid:48)(cid:48)(cid:48)|2

22

e

ce

dx(cid:48)(cid:48) = c(cid:48)e

|x(cid:48)(cid:48)|2

22

.

x(cid:48)(cid:48)

This implies the lemma.

We now show that the top k singular vectors produced by the SVD span the space of

the k centers. First, we extend the notion of best t to probability distributions. Then

we show that for a single spherical Gaussian whose center is not the origin, the best t

1-dimensional subspace is the line though the center of the Gaussian and the origin. Next,

we show that the best t k-dimensional subspace for a single Gaussian whose center is not

the origin is any k-dimensional subspace containing the line through the Gaussians center

and the origin. Finally, for k spherical Gaussians, the best t k-dimensional subspace is

the subspace containing their centers. Thus, the SVD nds the subspace that contains

the centers.

Recall that for a set of points, the best-t line is the line passing through the origin

that maximizes the sum of squared lengths of the projections of the points onto the line.

We extend this denition to probability densities instead of a set of points.

59

1. The best t 1-dimension subspace

to a spherical Gaussian is the line

through its center and the origin.

2. Any k-dimensional subspace contain-

ing the line is a best t k-dimensional

subspace for the Gaussian.

3. The best t k-dimensional subspace

for k spherical Gaussians is the sub-

space containing their centers.

Figure 3.5: Best t subspace to a spherical Gaussian.

Denition 3.1 If p is a probability density in d space, the best t line for p is the line in

the v1 direction where

v1 = arg max

|v|=1

E

xp

(cid:2)(vT x)2(cid:3) .

For a spherical Gaussian centered at the origin, it is easy to see that any line passing

through the origin is a best t line. Our next lemma shows that the best t line for a

spherical Gaussian centered at  (cid:54)= 0 is the line passing through  and the origin.

Lemma 3.16 Let the probability density p be a spherical Gaussian with center  (cid:54)= 0.

The unique best t 1-dimensional subspace is the line passing through  and the origin.

If  = 0, then any line through the origin is a best-t line.

Proof: For a randomly chosen x (according to p) and a xed unit length vector v,

E

xp

(cid:2)(vT x)2(cid:3) = E

xp

= E

xp

= E

xp

= E

xp

(cid:104)(cid:0)vT (x  ) + vT (cid:1)2(cid:105)

(cid:104)(cid:0)vT (x  )(cid:1)2

(cid:104)(cid:0)vT (x  )(cid:1)2(cid:105)

(cid:104)(cid:0)vT (x  )(cid:1)2(cid:105)

+ (cid:0)vT (cid:1)2

+ 2 (cid:0)vT (cid:1) (cid:0)vT (x  )(cid:1) + (cid:0)vT (cid:1)2(cid:105)

+ 2 (cid:0)vT (cid:1) E (cid:2)vT (x  )(cid:3) + (cid:0)vT (cid:1)2

= 2 + (cid:0)vT (cid:1)2

60

where the fourth line follows from the fact that E[vT (x  )] = 0, and the fth line

follows from the fact that E[(vT (x  ))2] is the variance in the direction v. The best t

line v maximizes Exp[(vT x)2] and therefore maximizes (cid:0)vT (cid:1)2. This is maximized when

v is aligned with the center . To see uniqueness, just note that if  (cid:54)= 0, then vT  is

strictly less when v is not aligned with the center.

We now extend Denition 3.1 to k-dimensional subspaces.

Denition 3.2 If p is a probability density in d-space then the best-t k-dimensional

subspace Vk is

Vk = argmax

V

dim(V )=k

E

xp

(cid:0)|proj(x, V )|2(cid:1) ,

where proj(x, V ) is the orthogonal projection of x onto V .

Lemma 3.17 For a spherical Gaussian with center , a k-dimensional subspace is a best

t subspace if and only if it contains .

Proof: If  = 0, then by symmetry any k-dimensional subspace is a best-t subspace. If

 (cid:54)= 0, then, the best-t line must pass through  by Lemma 3.16. Now, as in the greedy

algorithm for nding subsequent singular vectors, we would project perpendicular to the

rst singular vector. But after the projection, the mean of the Gaussian becomes 0 and

any vectors will do as subsequent best-t directions.

This leads to the following theorem.

Theorem 3.18 If p is a mixture of k spherical Gaussians, then the best t k-dimensional

subspace contains the centers. In particular, if the means of the Gaussians are linearly

independent, the space spanned by them is the unique best-t k dimensional subspace.

Proof: Let p be the mixture w1p1+w2p2+  +wkpk. Let V be any subspace of dimension

k or less. Then,

(cid:0)|proj(x, V )|2(cid:1) =

E

xp

k

(cid:88)

i=1

wi E

xpi

(cid:0)|proj(x, V )|2(cid:1)

If V contains the centers of the densities pi, by Lemma 3.17, each term in the summation

is individually maximized, which implies the entire summation is maximized, proving the

theorem.

For an innite set of points drawn according to the mixture, the k-dimensional SVD

subspace gives exactly the space of the centers. In reality, we have only a large number

of samples drawn according to the mixture. However, it is intuitively clear that as the

number of samples increases, the set of sample points will approximate the probability

density and so the SVD subspace of the sample will be close to the space spanned by

the centers. The details of how close it gets as a function of the number of samples are

technical and we do not carry this out here.

61

3.9.4 Ranking Documents and Web Pages

An important task for a document collection is to rank the documents according to

their intrinsic relevance to the collection. A good candidate denition of intrinsic rele-

vance is a documents projection onto the best-t direction for that collection, namely the

top left-singular vector of the term-document matrix. An intuitive reason for this is that

this direction has the maximum sum of squared projections of the collection and so can be

thought of as a synthetic term-document vector best representing the document collection.

Ranking in order of the projection of each documents term vector along the best t

direction has a nice interpretation in terms of the power method. For this, we consider

a dierent example, that of the web with hypertext links. The World Wide Web can

be represented by a directed graph whose nodes correspond to web pages and directed

edges to hypertext links between pages. Some web pages, called authorities, are the most

prominent sources for information on a given topic. Other pages called hubs, are ones

that identify the authorities on a topic. Authority pages are pointed to by many hub

pages and hub pages point to many authorities. One is led to what seems like a circular

denition: a hub is a page that points to many authorities and an authority is a page

that is pointed to by many hubs.

One would like to assign hub weights and authority weights to each node of the web.

If there are n nodes, the hub weights form an n-dimensional vector u and the authority

weights form an n-dimensional vector v. Suppose A is the adjacency matrix representing

the directed graph. Here aij is 1 if there is a hypertext link from page i to page j and 0

otherwise. Given hub vector u, the authority vector v could be computed by the formula

vj 

d

(cid:88)

i=1

uiaij

since the right hand side is the sum of the hub weights of all the nodes that point to node

j. In matrix terms,

v = AT u/|AT u|.

Similarly, given an authority vector v, the hub vector u could be computed by

u = Av/|Av|. Of course, at the start, we have neither vector. But the above discus-

sion suggests a power iteration. Start with any v. Set u = Av, then set v = AT u, then

renormalize and repeat the process. We know from the power method that this converges

to the left and right-singular vectors. So after suciently many iterations, we may use the

left vector u as the hub weights vector and project each column of A onto this direction

and rank columns (authorities) in order of this projection. But the projections just form

the vector AT u which equals a multiple of v. So we can just rank by order of the vj.

This is the basis of an algorithm called the HITS algorithm, which was one of the early

proposals for ranking web pages.

62

A dierent ranking called pagerank is widely used. It is based on a random walk on

the graph described above. We will study random walks in detail in Chapter 4.

3.9.5 An Application of SVD to a Discrete Optimization Problem

In clustering a mixture of Gaussians, SVD was used as a dimension reduction tech-

nique. It found a k-dimensional subspace (the space of centers) of a d-dimensional space

and made the Gaussian clustering problem easier by projecting the data to the subspace.

Here, instead of tting a model to data, we consider an optimization problem where ap-

plying dimension reduction makes the problem easier. The use of SVD to solve discrete

optimization problems is a relatively new subject with many applications. We start with

an important NP-hard problem, the maximum cut problem for a directed graph G(V, E).

The maximum cut problem is to partition the nodes of an n-node directed graph into

two subsets S and S so that the number of edges from S to S is maximized. Let A be

the adjacency matrix of the graph. With each vertex i, associate an indicator variable xi.

The variable xi will be set to 1 for i  S and 0 for i  S. The vector x = (x1, x2, . . . , xn)

is unknown and we are trying to nd it or equivalently the cut, so as to maximize the

number of edges across the cut. The number of edges across the cut is precisely

(cid:88)

i,j

xi(1  xj)aij.

Thus, the maximum cut problem can be posed as the optimization problem

Maximize (cid:80)

xi(1  xj)aij

subject to xi  {0, 1}.

In matrix notation,

i,j

(cid:88)

i,j

xi(1  xj)aij = xT A(1  x),

where 1 denotes the vector of all 1s . So, the problem can be restated as

Maximize xT A(1  x)

subject to xi  {0, 1}.

(3.1)

This problem is NP-hard. However we will see that for dense graphs, that is, graphs

with (n2) edges and therefore whose optimal solution has size (n2),13 we can use the

SVD to nd a near optimal solution in polynomial time. To do so we will begin by

computing the SVD of A and replacing A by Ak = (cid:80)k

T in (3.1) to get

i=1 iuivi

Maximize xT Ak(1  x)

subject to xi  {0, 1}.

(3.2)

Note that the matrix Ak is no longer a 0-1 adjacency matrix.

We will show that:

13Any graph of m edges has a cut of size at least m/2. This can be seen by noting that the expected

size of the cut for a random x  {0, 1}n is exactly m/2.

63

1. For each 0-1 vector x, xT Ak(1  x) and xT A(1  x) dier by at most



n2

k+1

. Thus,

the maxima in (3.1) and (3.2) dier by at most this amount.

2. A near optimal x for (3.2) can be found in time nO(k) by exploiting the low rank

of Ak, which is polynomial time for constant k. By Item 1 this is near optimal for

(3.1) where near optimal means with additive error of at most



.

n2

k+1



First, we prove Item 1. Since x and 1  x are 0-1 n-vectors, each has length at most

n||A  Ak||2. Now since

xT (A  Ak)(1  x) is the dot product of the vector x with the vector (A  Ak)(1  x),

n. By the denition of the 2-norm, |(A  Ak)(1  x)| 



|xT (A  Ak)(1  x)|  n||A  Ak||2.

By Lemma 3.8, ||A  Ak||2 = k+1(A). The inequalities,

(k + 1)2

k+1  2

1 + 2

2 +    2

k+1  ||A||2

F =

(cid:88)

i,j

ij  n2

a2

imply that 2

k+1  n2

k+1 and hence ||A  Ak||2  n

k+1

proving Item 1.

Next we focus on Item 2. It is instructive to look at the special case when k=1 and A

is approximated by the rank one matrix A1. An even more special case when the left and

right-singular vectors u and v are identical is already NP-hard to solve exactly because

it subsumes the problem of whether for a set of n integers, {a1, a2, . . . , an}, there is a

partition into two subsets whose sums are equal. However, for that problem, there is

an ecient dynamic programming algorithm that nds a near-optimal solution. We will

build on that idea for the general rank k problem.

For Item 2, we want to maximize (cid:80)k

T (1  x)) over 0-1 vectors x. A

piece of notation will be useful. For any S  {1, 2, . . . n}, write ui(S) for the sum of coor-

dinates of the vector ui corresponding to elements in the set S, that is, ui(S) = (cid:80)

jS uij,

and similarly for vi. We will nd S to maximize (cid:80)k

i=1 iui(S)vi( S) using dynamic pro-

gramming.

i=1 i(xT ui)(vi

For a subset S of {1, 2, . . . , n}, dene the 2k-dimensional vector

w(S) = (cid:0)u1(S), v1( S), u2(S), v2( S), . . . , uk(S), vk( S)(cid:1).

If we had the list of all such vectors, we could nd (cid:80)k

i=1 iui(S)vi( S) for each of them

and take the maximum. There are 2n subsets S, but several S could have the same w(S)

and in that case it suces to list just one of them. Round each coordinate of each ui to

1

nk2 . Call the rounded vector ui. Similarly obtain vi. Let

the nearest integer multiple of

w(S) denote the vector (u1(S), v1( S), u2(S), v2( S), . . . , uk(S), vk( S)). We will construct

a list of all possible values of the vector w(S). Again, if several dierent Ss lead to the

64

same vector w(S), we will keep only one copy on the list. The list will be constructed by

dynamic programming. For the recursive step, assume we already have a list of all such

vectors for S  {1, 2, . . . , i} and wish to construct the list for S  {1, 2, . . . , i + 1}. Each

S  {1, 2, . . . , i} leads to two possible S(cid:48)  {1, 2, . . . , i + 1}, namely, S and S  {i + 1}.

In the rst case, the vector w(S(cid:48)) = (u1(S), v1( S) + v1,i+1, u2(S), v2( S) + v2,i+1, . . . , ...).

In the second case, it is w(S(cid:48)) = (u1(S) + u1,i+1, v1( S), u2(S) + u2,i+1, v2( S), . . . , ...). We

put in these two vectors for each vector in the previous list. Then, crucially, we prune -

i.e., eliminate duplicates.

n

Assume that k is constant. Now, we show that the error is at most

n2

as claimed.

k+1

Since ui and vi are unit length vectors, |ui(S)|, |vi( S)| 

n. Also |ui(S)  ui(S)| 

nk2 = 1

k2 and similarly for vi. To bound the error, we use an elementary fact: if a and b are

reals with |a|, |b|  M and we estimate a by a(cid:48) and b by b(cid:48) so that |aa(cid:48)|, |bb(cid:48)|    M ,

then a(cid:48)b(cid:48) is an estimate of ab in the sense





|ab  a(cid:48)b(cid:48)| = |a(b  b(cid:48)) + b(cid:48)(a  a(cid:48))|  |a||b  b(cid:48)| + (|b| + |b  b(cid:48)|)|a  a(cid:48)|  3M .

Using this,

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

k

(cid:88)

i=1

iui(S)vi( S) 

k

(cid:88)

i=1

(cid:12)

(cid:12)

(cid:12)

iui(S)vi(S)

(cid:12)

(cid:12)



 3k1

n/k2  3n3/2/k  n2/k,

and this meets the claimed error bound.

Next, we show that the running time is polynomially bounded. First, |ui(S)|, |vi(S)| 



n. Since ui(S) and vi(S) are all integer multiples of 1/(nk2), there are at most 2n3/2k2

2

possible values of ui(S) and vi(S) from which it follows that the list of w(S) never gets

larger than (2n3/2k2)2k which for xed k is polynomially bounded.

We summarize what we have accomplished.

Theorem 3.19 Given a directed graph G(V, E), a cut of size at least the maximum cut

can be computed in time polynomial in n for any xed k.

minus O

(cid:17)

(cid:16) n2



k

Note that achieving the same accuracy in time polynomial in n and k would give an

exact max cut in polynomial time.

3.10 Bibliographic Notes

Singular value decomposition is fundamental to numerical analysis and linear algebra.

There are many texts on these subjects and the interested reader may want to study

these. A good reference is [GvL96]. The material on clustering a mixture of Gaussians

in Section 3.9.3 is from [VW02]. Modeling data with a mixture of Gaussians is a stan-

dard tool in statistics. Several well-known heuristics like the expectation-minimization

65

algorithm are used to learn (t) the mixture model to data. Recently, in theoretical com-

puter science, there has been modest progress on provable polynomial-time algorithms

for learning mixtures. Some references are [DS07], [AK05], [AM05], and [MV10]. The

application to the discrete optimization problem is from [FK99]. The section on rank-

ing documents/webpages is from two inuential papers, one on hubs and authorities by

Jon Kleinberg [Kle99] and the other on pagerank by Page, Brin, Motwani and Winograd

[BMPW98].

66

3.11 Exercises

Exercise 3.1 (Least squares vertical error) In many experiments one collects the

value of a parameter at various instances of time. Let yi be the value of the parameter y

at time xi. Suppose we wish to construct the best linear approximation to the data in the

sense that we wish to minimize the mean square error. Here error is measured vertically

rather than perpendicular to the line. Develop formulas for m and b to minimize the mean

square error of the points {(xi, yi) |1  i  n} to the line y = mx + b.

Exercise 3.2 Given ve observed variables, height, weight, age, income, and blood pres-

sure of n people, how would one nd the best least squares t ane subspace of the form

a1 (height) + a2 (weight) + a3 (age) + a4 (income) + a5 (blood pressure) = a6

Here a1, a2, . . . , a6 are the unknown parameters. If there is a good best t 4-dimensional

ane subspace, then one can think of the points as lying close to a 4-dimensional sheet

rather than points lying in 5-dimensions. Why might it be better to use the perpendicular

distance to the ane subspace rather than vertical distance where vertical distance is

measured along the coordinate axis corresponding to one of the variables?

Exercise 3.3 Manually nd the best t lines (not subspaces which must contain the ori-

gin) through the points in the sets below. Subtract the center of gravity of the points in

the set from each of the points in the set and nd the best t line for the resulting points.

Does the best t line for the original data go through the origin?

1. (4,4) (6,2)

2. (4,2) (4,4) (6,2) (6,4)

3. (3,2.5) (3,5) (5,1) (5,3.5)

Exercise 3.4 Manually determine the best t line through the origin for each of the

following sets of points. Is the best t line unique? Justify your answer for each of the

subproblems.

1. {(0, 1) , (1, 0)}

2. {(0, 1) , (2, 0)}

Exercise 3.5 Manually nd the left and right-singular vectors, the singular values, and

the SVD decomposition of the matrices in Figure 3.6.

Exercise 3.6 Consider the matrix

A =

















1

1

2

2

1 2

1 2

67

(0,3)

(1,1)

(3,0)

M =





1 1

0 3

3 0





(1,3)

(0,2)

M =

(3,1)









0 2

2 0

1 3

3 1









Figure 3.6 a

(2,0)

Figure 3.6 b

Figure 3.6: SVD problem

1. Run the power method starting from x = (cid:0)1

(cid:1) for k = 3 steps. What does this give

1

as an estimate of v1?

2. What actually are the vis, is, and uis? It may be easiest to do this by computing

the eigenvectors of B = AT A.

3. Suppose matrix A is a database of restaurant ratings: each row is a person, each

column is a restaurant, and aij represents how much person i likes restaurant j.

What might v1 represent? What about u1? How about the gap 1  2?

Exercise 3.7 Let A be a square n  n matrix whose rows are orthonormal. Prove that

the columns of A are orthonormal.

Exercise 3.8 Suppose A is a n  n matrix with block diagonal structure with k equal size

blocks where all entries of the ith block are ai with a1 > a2 >    > ak > 0. Show that A

has exactly k nonzero singular vectors v1, v2, . . . , vk where vi has the value ( k

n )1/2 in the

coordinates corresponding to the ith block and 0 elsewhere. In other words, the singular

vectors exactly identify the blocks of the diagonal. What happens if a1 = a2 =    = ak?

In the case where the ai are equal, what is the structure of the set of all possible singular

vectors?

Hint: By symmetry, the top singular vectors components must be constant in each block.

Exercise 3.9 Interpret the rst right and left-singular vectors for the document term

matrix.

Exercise 3.10 Verify that the sum of r-rank one matrices

cixiyi

T can be written as

XCY T , where the xi are the columns of X, the yi are the columns of Y, and C is a

diagonal matrix with the constants ci on the diagonal.

r

(cid:80)

i=1

Exercise 3.11 Let (cid:80)r

(cid:12)

(cid:12)uT

(cid:12)uTA(cid:12)

(cid:12)

(cid:12).

1 A(cid:12)

(cid:12) = max

|u|=1

i=1 iuivi

T be the SVD of A. Show that (cid:12)

1 A(cid:12)

(cid:12)uT

(cid:12) = 1 and that

68

Exercise 3.12 If 1, 2, . . . , r are the singular values of A and v1, v2, . . . , vr are the

corresponding right-singular vectors, show that

1. AT A =

r

(cid:80)

i=1

2

i vivi

T

2. v1, v2, . . . vr are eigenvectors of AT A.

3. Assuming that the eigenvectors of AT A are unique up to multiplicative constants,

conclude that the singular vectors of A (which by denition must be unit length) are

unique up to sign.

Exercise 3.13 Let (cid:80)

i be the singular value decomposition of a rank r matrix A.

iuivT

i

Let Ak =

iuivT

i be a rank k approximation to A for some k < r. Express the following

k

(cid:80)

i=1

quantities in terms of the singular values {i, 1  i  r}.

1. ||Ak||2

F

2. ||Ak||2

2

3. ||A  Ak||2

F

4. ||A  Ak||2

2

Exercise 3.14 If A is a symmetric matrix with distinct singular values, show that the

left and right singular vectors are the same and that A = V DV T .

Exercise 3.15 Let A be a matrix. How would you compute

v1 = arg max

|Av|?

|v|=1

How would you use or modify your algorithm for nding v1 to compute the rst few

singular vectors of A.

Exercise 3.16 Use the power method to compute the singular value decomposition of the

matrix

A =

(cid:19)

(cid:18) 1 2

3 4

Exercise 3.17

1. Write a program to implement the power method for computing the

rst singular vector of a matrix. Apply your program to the matrix

A =















1

2

...

9

10

2

3   

3

4   

...

...

10 0   

0   

0

9

10

0

0















.

10

0

...

0

0

69

2. Modify the power method to nd the rst four singular vectors of a matrix A as

follows. Randomly select four vectors and nd an orthonormal basis for the space

spanned by the four vectors. Then multiply each of the basis vectors times A and

nd a new orthonormal basis for the space spanned by the resulting four vectors.

Apply your method to nd the rst four singular vectors of matrix A from part 1.

In Matlab the command orth nds an orthonormal basis for the space spanned by a

set of vectors.

Exercise 3.18 A matrix A is positive semi-denite if for all x, xT Ax  0.

1. Let A be a real valued matrix. Prove that B = AAT is positive semi-denite.

2. Let A be the adjacency matrix of a graph. The Laplacian of A is L = D  A where

D is a diagonal matrix whose diagonal entries are the row sums of A. Prove that

L is positive semi denite by showing that L = BT B where B is an m-by-n matrix

with a row for each edge in the graph, a column for each vertex, and we dene

bei =







1 if i is the endpoint of e with lesser index

1 if i is the endpoint of e with greater index

0 if i is not an endpoint of e

Exercise 3.19 Prove that the eigenvalues of a symmetric real valued matrix are real.

Exercise 3.20 Suppose A is a square invertible matrix and the SVD of A is A = (cid:80)

Prove that the inverse of A is (cid:80)

i

iuivT

i .

1

i

viuT

i .

i

iuivT

Exercise 3.21 Suppose A is square, but not necessarily invertible and has SVD A =

r

(cid:80)

i . Show that BAx = x for all x in the span of the right-

i=1

singular vectors of A. For this reason B is sometimes called the pseudo inverse of A and

can play the role of A1 in many applications.

i . Let B =

r

(cid:80)

i=1

viuT

1

i

Exercise 3.22

1. For any matrix A, show that k  ||A||F

k

.

2. Prove that there exists a matrix B of rank at most k such that ||A  B||2  ||A||F

k

.

3. Can the 2-norm on the left hand side in (2) be replaced by Frobenius norm?

Exercise 3.23 Suppose an n  d matrix A is given and you are allowed to preprocess

A. Then you are given a number of d-dimensional vectors x1, x2, . . . , xm and for each of

these vectors you must nd the vector Axj approximately, in the sense that you must nd a

|yj  Axj|  ||A||F |xj|. Here  >0 is a given error bound. Describe

vector yj satisfying

an algorithm that accomplishes this in time O (cid:0) d+n

(cid:1) per xj not counting the preprocessing

2

time. Hint: use Exercise 3.22.

70

Exercise 3.24 Find the values of ci to maximize

r

(cid:80)

i=1

c2

i = 1.

r

(cid:80)

i=1

i 2

c2

i where 1  2  . . . and

Exercise 3.25 (Document-Term Matrices): Suppose we have an m  n document-

term matrix A where each row corresponds to a document and has been normalized to

length one. Dene the similarity between two such documents by their dot product.

1. Consider a synthetic document whose sum of squared similarities with all docu-

ments in the matrix is as high as possible. What is this synthetic document and how

would you nd it?

2. How does the synthetic document in (1) dier from the center of gravity?

3. Building on (1), given a positive integer k, nd a set of k synthetic documents such

that the sum of squares of the mk similarities between each document in the matrix

and each synthetic document is maximized. To avoid the trivial solution of selecting

k copies of the document in (1), require the k synthetic documents to be orthogonal

to each other. Relate these synthetic documents to singular vectors.

4. Suppose that the documents can be partitioned into k subsets (often called clusters),

where documents in the same cluster are similar and documents in dierent clusters

are not very similar. Consider the computational problem of isolating the clusters.

This is a hard problem in general. But assume that the terms can also be partitioned

into k clusters so that for i (cid:54)= j, no term in the ith cluster occurs in a document

in the jth cluster. If we knew the clusters and arranged the rows and columns in

them to be contiguous, then the matrix would be a block-diagonal matrix. Of course

the clusters are not known. By a block of the document-term matrix, we mean

a submatrix with rows corresponding to the ithcluster of documents and columns

corresponding to the ithcluster of terms . We can also partition any n vector into

blocks. Show that any right-singular vector of the matrix must have the property

that each of its blocks is a right-singular vector of the corresponding block of the

document-term matrix.

5. Suppose now that the k singular values are all distinct. Show how to solve the

clustering problem.

Hint: (4) Use the fact that the right-singular vectors must be eigenvectors of AT A. Show

that AT A is also block-diagonal and use properties of eigenvectors.

Exercise 3.26 Let u be a xed vector. Show that maximizing xT uuT (1  x) subject to

xi  {0, 1} is equivalent to partitioning the coordinates of u into two subsets where the

sum of the elements in both subsets are as equal as possible.

71

Exercise 3.27 Read in a photo and convert to a matrix. Perform a singular value de-

composition of the matrix. Reconstruct the photo using only 5%, 10%, 25%, 50% of the

singular values.

1. Print the reconstructed photo. How good is the quality of the reconstructed photo?

2. What percent of the Forbenius norm is captured in each case?

Hint: If you use Matlab, the command to read a photo is imread. The types of les that

can be read are given by imformats. To print the le use imwrite. Print using jpeg format.

To access the le afterwards you may need to add the le extension .jpg. The command

imread will read the le in uint8 and you will need to convert to double for the SVD code.

Afterwards you will need to convert back to uint8 to write the le. If the photo is a color

photo you will get three matrices for the three colors used.

Exercise 3.28

1. Create a 100  100 matrix of random numbers between 0 and 1 such

that each entry is highly correlated with the adjacent entries. Find the SVD of A.

What fraction of the Frobenius norm of A is captured by the top 10 singular vectors?

How many singular vectors are required to capture 95% of the Frobenius norm?

2. Repeat (1) with a 100  100 matrix of statistically independent random numbers

between 0 and 1.

Exercise 3.29 Show that the running time for the maximum cut algorithm in Section

3.9.5 can be carried out in time O(n3 + poly(n)kk), where poly is some polynomial.

Exercise 3.30 Let x1, x2, . . . , xn be n points in d-dimensional space and let X be the

n  d matrix whose rows are the n points. Suppose we know only the matrix D of pairwise

distances between points and not the coordinates of the points themselves. The set of points

x1, x2, . . . , xn giving rise to the distance matrix D is not unique since any translation,

rotation, or reection of the coordinate system leaves the distances invariant. Fix the

origin of the coordinate system so that the centroid of the set of points is at the origin.

That is, (cid:80)n

i=1 xi = 0.

1. Show that the elements of XX T are given by

xixT

j = 

(cid:34)

1

2

d2

ij 

1

n

n

(cid:88)

k=1

d2

ik 

1

n

n

(cid:88)

k=1

d2

kj +

1

n2

n

(cid:88)

n

(cid:88)

k=1

l=1

(cid:35)

d2

kl

.

2. Describe an algorithm for determining the matrix X whose rows are the xi.

Exercise 3.31

72

1. Consider the pairwise distance matrix for twenty US cities given below. Use the

algorithm of Exercise 3.30 to place the cities on a map of the US. The algorithm is

called classical multidimensional scaling, cmdscale, in Matlab. Alternatively use the

pairwise distance matrix of 12 Chinese cities to place the cities on a map of China.

Note: Any rotation or a mirror image of the map will have the same pairwise

distances.

2. Suppose you had airline distances for 50 cities around the world. Could you use

these distances to construct a 3-dimensional world model?

B

O

S



400

851

1551

1769

1605

2596

1137

1255

1123

188

1282

271

2300

483

1038

2099

2699

2493

393

B

U

F

400



454

1198

1370

1286

2198

803

1181

731

292

883

279

1906

178

662

1699

2300

2117

292

C

H

I

851

454



803

920

940

1745

482

1188

355

713

432

666

1453

410

262

1260

1858

1737

597

D

A

L

1551

1198

803



663

225

1240

420

1111

862

1374

586

1299

887

1070

547

999

1483

1681

1185

D

E

N

1769

1370

920

663



879

831

879

1726

700

1631

488

1579

586

1320

796

371

949

1021

1494

H

O

U

1605

1286

940

225

879



1374

484

968

1056

1420

794

1341

1017

1137

679

1200

1645

1891

1220

L

A

2596

2198

1745

1240

831

1374



1603

2339

1524

2451

1315

2394

357

2136

1589

579

347

959

2300

M

E

M

1137

803

482

420

879

484

1603



872

699

957

529

881

1263

660

240

1250

1802

1867

765

M

I

A

1255

1181

1188

1111

1726

968

2339

872



1511

1092

1397

1019

1982

1010

1061

2089

2594

2734

923

M

I

M

1123

731

355

862

700

1056

1524

699

1511



1018

290

985

1280

743

466

987

1584

1395

934

Boston

Bualo

Chicago

Dallas

Denver

Houston

Los Angeles

Memphis

Miami

Minneapolis

New York

Omaha

Philadelphia

Phoenix

Pittsburgh

Saint Louis

Salt Lake City

San Francisco

Seattle

Washington D.C.

73

N

Y

188

292

713

1374

1631

1420

2451

957

1092

1018



1144

83

2145

317

875

1972

2571

2408

230

O

M

A

1282

883

432

586

488

794

1315

529

1397

290

1144



1094

1036

836

354

833

1429

1369

1014

P

H

I

271

279

666

1299

1579

1341

2394

881

1019

985

83

1094



2083

259

811

1925

2523

2380

123

P

H

O

2300

1906

1453

887

586

1017

357

1263

1982

1280

2145

1036

2083



1828

1272

504

653

1114

1973

P

I

T

483

178

410

1070

1320

1137

2136

660

1010

743

317

836

259

1828



559

1668

2264

2138

192

S

t

L

1038

662

262

547

796

679

1589

240

1061

466

875

354

811

1272

559



1162

1744

1724

712

S

L

C

2099

1699

1260

999

371

1200

579

1250

2089

987

1972

833

1925

504

1668

1162



600

701

1848

S

F

2699

2300

1858

1483

949

1645

347

1802

2594

1584

2571

1429

2523

653

2264

1744

600



678

2442

S

E

A

2493

2117

1737

1681

1021

1891

959

1867

2734

1395

2408

1369

2380

1114

2138

1724

701

678



2329

D

C

393

292

597

1185

1494

1220

2300

765

923

934

230

1014

123

1973

192

712

1848

2442

2329



Boston

Bualo

Chicago

Dallas

Denver

Houston

Los Angeles

Memphis

Miami

Minneapolis

New York

Omaha

Philadelphia

Phoenix

Pittsburgh

Saint Louis

Salt Lake City

San Francisco

Seattle

Washington D.C.

City

Beijing

Tianjin

Shanghai

Chongqing

Hohhot

Urumqi

Lhasa

Yinchuan

Nanning

Harbin

Changchun

Shenyang

Bei-

jing

0

125

1239

3026

480

3300

3736

1192

2373

1230

979

684

Tian-

jin

125

0

1150

1954

604

3330

3740

1316

2389

1207

955

661

Shang- Chong- Hoh- Urum-

hai

1239

1150

0

1945

1717

3929

4157

2092

1892

2342

2090

1796

qing

3026

1954

1945

0

1847

3202

2457

1570

993

3156

2905

2610

hot

480

604

1717

1847

0

2825

3260

716

2657

1710

1458

1164

qi

3300

3330

3929

3202

2825

0

2668

2111

4279

4531

4279

3985

Lha-

sa

3736

3740

4157

2457

3260

2668

0

2547

3431

4967

4715

4421

Yin-

chuan

1192

1316

2092

1570

716

2111

2547

0

2673

2422

2170

1876

Nan- Har- Chang-

ning

2373

2389

1892

993

2657

4279

3431

2673

0

3592

3340

3046

chun

979

955

2090

2905

1458

4279

4715

2170

3340

256

0

294

bin

1230

1207

2342

3156

1710

4531

4967

2422

3592

0

256

546

Shen-

yang

684

661

1796

2610

1164

3985

4421

1876

3046

546

294

0

Exercise 3.32 Ones data in a high dimensional space may lie on a lower dimensional

sheath. To test for this one might for each data point nd the set of closest data points

and calculate the vector distance from the data point to each of the close points. If the set

of these distance vectors is a lower dimensional space than the number of distance points,

then it is likely that the data is on a low dimensional sheath. To test the dimension of

the space of the distance vectors one might use the singular value decomposition to nd

the singular values. The dimension of the space is the number of large singular values.

The low singular values correspond to noise or slight curvature of the sheath. To test

this concept generate a data set of points that lie on a one dimensional curve in three

space. For each point nd maybe ten nearest points, form the matrix of distance, and do

a singular value decomposition on the matrix. Report what happens.

Using code such as the following to create the data.

function [ data, distance ] = create_sheath( n )

74

%creates n data points on a one dimensional sheath in three dimensional

%space

%

if nargin==0

n=100;

end

data=zeros(3,n);

for i=1:n

x=sin((pi/100)*i);

y=sqrt(1-x^2);

z=0.003*i;

data(:,i)=[x;y;z];

end

%subtract adjacent vertices

distance=zeros(3,10);

for i=1:5

distance(:,i)=data(:,i)-data(:,6);

distance(:,i+5)=data(:,i+6)-data(:,6);

end

end

75

4 Random Walks and Markov Chains

A random walk on a directed graph consists of a sequence of vertices generated from

a start vertex by probabilistically selecting an incident edge, traversing the edge to a new

vertex, and repeating the process.

We generally assume the graph is strongly connected, meaning that for any pair of

vertices x and y, the graph contains a path of directed edges starting at x and ending

at y. If the graph is strongly connected, then, as we will see, no matter where the walk

begins the fraction of time the walk spends at the dierent vertices of the graph converges

to a stationary probability distribution.

Start a random walk at a vertex x and think of the starting probability distribution as

putting a mass of one on x and zero on every other vertex. More generally, one could start

with any probability distribution p, where p is a row vector with nonnegative components

summing to one, with px being the probability of starting at vertex x. The probability

of being at vertex x at time t + 1 is the sum over each adjacent vertex y of being at y at

time t and taking the transition from y to x. Let p(t) be a row vector with a component

for each vertex specifying the probability mass of the vertex at time t and let p(t + 1) be

the row vector of probabilities at time t + 1. In matrix notation14

p(t)P = p(t + 1)

where the ijth entry of the matrix P is the probability of the walk at vertex i selecting

the edge to vertex j.

A fundamental property of a random walk is that in the limit, the long-term average

probability of being at a particular vertex is independent of the start vertex, or an initial

probability distribution over vertices, provided only that the underlying graph is strongly

connected. The limiting probabilities are called the stationary probabilities. This funda-

mental theorem is proved in the next section.

A special case of random walks, namely random walks on undirected graphs, has

important connections to electrical networks. Here, each edge has a parameter called

conductance, like electrical conductance. If the walk is at vertex x, it chooses an edge to

traverse next from among all edges incident to x with probability proportional to its con-

ductance. Certain basic quantities associated with random walks are hitting time, which

is the expected time to reach vertex y starting at vertex x, and cover time, which is the

expected time to visit every vertex. Qualitatively, for undirected graphs these quantities

are all bounded above by polynomials in the number of vertices. The proofs of these facts

will rely on the analogy between random walks and electrical networks.

14Probability vectors are represented by row vectors to simplify notation in equations like the one here.

76

random walk

Markov chain

graph

vertex

strongly connected

aperiodic

strongly connected

and aperiodic

undirected graph

stochastic process

state

persistent

aperiodic

ergodic

time reversible

Table 5.1: Correspondence between terminology of random walks and Markov chains

Aspects of the theory of random walks were developed in computer science with a

number of applications. Among others, these include dening the pagerank of pages on

the World Wide Web by their stationary probability. An equivalent concept called a

Markov chain had previously been developed in the statistical literature. A Markov chain

has a nite set of states. For each pair of states x and y, there is a transition probability

pxy of going from state x to state y where for each x, (cid:80)

y pxy = 1. A random walk in

the Markov chain starts at some state. At a given time step, if it is in state x, the next

state y is selected randomly with probability pxy. A Markov chain can be represented by

a directed graph with a vertex representing each state and an edge with weight pxy from

vertex x to vertex y. We say that the Markov chain is connected if the underlying directed

graph is strongly connected. That is, if there is a directed path from every vertex to every

other vertex. The matrix P consisting of the pxy is called the transition probability matrix

of the chain. The terms random walk and Markov chain are used interchangeably.

The correspondence between the terminologies of random walks and Markov chains is

given in Table 5.1.

A state of a Markov chain is persistent if it has the property that should the state ever

be reached, the random process will return to it with probability one. This is equivalent

to the property that the state is in a strongly connected component with no out edges.

For most of the chapter, we assume that the underlying directed graph is strongly con-

nected. We discuss here briey what might happen if we do not have strong connectivity.

Consider the directed graph in Figure 4.1b with three strongly connected components,

A, B, and C. Starting from any vertex in A, there is a nonzero probability of eventually

reaching any vertex in A. However, the probability of returning to a vertex in A is less

than one and thus vertices in A, and similarly vertices in B, are not persistent. From

any vertex in C, the walk eventually will return with probability one to the vertex, since

there is no way of leaving component C. Thus, vertices in C are persistent.

A connected Markov Chain is said to be aperiodic if the greatest common divisor of

the lengths of directed cycles is one. It is known that for connected aperiodic chains, the

77

A

A

B

(a)

B

(b)

C

C

Figure 4.1: (a) A directed graph with vertices having no out out edges and a strongly

connected component A with no in edges.

(b) A directed graph with three strongly connected components.

probability distribution of the random walk converges to a unique stationary distribution.

Aperiodicity is a technical condition needed in this proof. Here, we do not prove this

theorem and do not worry about aperiodicity at all. It turns out that if we take the av-

erage probability distribution of the random walk over the rst t steps, then this average

converges to a limiting distribution for connected chains (without assuming aperiodicity)

and this average is what one uses in practice. We prove this limit theorem and explain

its uses in what is called the Markov Chain Monte Carlo (MCMC) method.

Markov chains are used to model situations where all the information of the system

necessary to predict the future can be encoded in the current state. A typical example

is speech, where for a small k the current state encodes the last k syllables uttered by

the speaker. Given the current state, there is a certain probability of each syllable being

uttered next and these can be used to calculate the transition probabilities. Another

example is a gamblers assets, which can be modeled as a Markov chain where the current

state is the amount of money the gambler has on hand. The model would only be valid

if the gamblers bets depend only on current assets, not the past history.

Later in the chapter, we study the widely used Markov Chain Monte Carlo method

(MCMC). Here, the objective is to sample a large space according to some probability

distribution p. The number of elements in the space may be very large, say 10100. One de-

signs a Markov chain where states correspond to the elements of the space. The transition

probabilities of the chain are designed so that the stationary probability of the chain is the

78

probability distribution p with which we want to sample. One chooses samples by taking

a random walk until the probability distribution is close to the stationary distribution of

the chain and then selects the current state of the walk. The walk continues a number of

steps until the probability distribution is nearly independent of where the walk was when

the rst element was selected. A second point is then selected, and so on. Although it

is impossible to store the graph in a computer since it has 10100 vertices, to do the walk

one needs only store the current vertex of the walk and be able to generate the adjacent

vertices by some algorithm. What is critical is that the probability distribution of the

walk converges to the stationary distribution in time logarithmic in the number of states.

We mention two motivating examples. The rst is to select a point at random in

d-space according to a probability density such as a Gaussian. Put down a grid and let

each grid point be a state of the Markov chain. Given a probability density p, design

transition probabilities of a Markov chain so that the stationary distribution is p.

In

general, the number of states grows exponentially in the dimension d, but if the time

to converge to the stationary distribution grows polynomially in d, then one can do a

random walk on the graph until convergence to the stationary probability. Once the sta-

tionary probability has been reached, one selects a point. To select a set of points, one

must walk a number of steps between each selection so that the probability of the current

point is independent of the previous point. By selecting a number of points one can es-

timate the probability of a region by observing the number of selected points in the region.

A second example is from physics. Consider an n  n grid in the plane with a particle

at each grid point. Each particle has a spin of 1. A conguration is a n2 dimensional

vector v = (v1, v2, . . . , vn2), where vi is the spin of the ith particle. There are 2n2 spin con-

gurations. The energy of a conguration is a function f (v) of the conguration, not of

any single spin. A central problem in statistical mechanics is to sample spin congurations

according to their probability. It is easy to design a Markov chain with one state per spin

conguration so that the stationary probability of a state is proportional to the states

energy. If a random walk gets close to the stationary probability in time polynomial in n

rather than 2n2, then one can sample spin congurations according to their probability.

The Markov Chain has 2n2 states, one per conguration. Two states in the Markov

chain are adjacent if and only if the corresponding congurations v and u dier in just one

coordinate (ui = vi for all but one i). The Metropilis-Hastings random walk, described

in more detail in Section 4.2, has a transition probability from a conguration v to an

adjacent conguration u of

1

n2 min

(cid:18)

1,

(cid:19)

.

f (u)

f (v)

As we will see, the Markov Chain has a stationary probability proportional to the energy.

There are two more crucial facts about this chain. The rst is that to execute a step in

the chain, we do not need the whole chain, just the ratio f (u)

f (v) . The second is that under

suitable assumptions, the chain approaches stationarity in time polynomial in n.

79

A quantity called the mixing time, loosely dened as the time needed to get close to

the stationary distribution, is often much smaller than the number of states. In Section

4.4, we relate the mixing time to a combinatorial notion called normalized conductance

and derive upper bounds on the mixing time in several cases.

4.1 Stationary Distribution

Let p(t) be the probability distribution after t steps of a random walk. Dene the

long-term average probability distribution a(t) by

a(t) =

1

t

(cid:0)p(0) + p(1) +    + p(t  1)(cid:1).

The fundamental theorem of Markov chains asserts that for a connected Markov chain,

a(t) converges to a limit probability vector x that satises the equations xP = x. Before

proving the fundamental theorem of Markov chains, we rst prove a technical lemma.

Lemma 4.1 Let P be the transition probability matrix for a connected Markov chain.

The n  (n + 1) matrix A = [P  I , 1] obtained by augmenting the matrix P  I with an

additional column of ones has rank n.

Proof: If the rank of A = [P  I, 1] was less than n there would be a subspace of solu-

tions to Ax = 0 of at least two-dimensions. Each row in P sums to one, so each row in

P  I sums to zero. Thus x = (1, 0), where all but the last coordinate of x is 1, is one

solution to Ax = 0. Assume there was a second solution (x, ) perpendicular to (1, 0).

Then (P I)x+1 = 0 and for each i, xi = (cid:80)

j pijxj +. Each xi is a convex combination

of some xj plus . Let S be the set of i for which xi attains its maximum value. Since

x is perpendicular to 1, some xi is negative and thus S is not empty. Connectedness

implies that some xk of maximum value is adjacent to some xl of lower value. Thus,

xk > (cid:80)

j pkjxj. Therefore  must be greater than 0 in xk = (cid:80)

j pkjxj + ..

On the other hand, the same argument with T the set of i with xi taking its minimum

value implies  < 0. This contradiction falsies the assumption of a second solution,

thereby proving the lemma.

Theorem 4.2 (Fundamental Theorem of Markov Chains) For a connected Markov

chain there is a unique probability vector  satisfying P = . Moreover, for any starting

distribution, lim

t

a(t) exists and equals .

Proof: Note that a(t) is itself a probability vector, since its components are nonnegative

and sum to 1. Run one step of the Markov chain starting with distribution a(t); the

80

distribution after the step is a(t)P . Calculate the change in probabilities due to this step.

a(t)P  a(t) =

=

=

1

t

1

t

1

t

[p(0)P + p(1)P +    + p(t  1)P ] 

1

t

[p(0) + p(1) +    + p(t  1)]

[p(0) + p(1) +    + p(t  1)]

[p(1) + p(2) +    + p(t)] 

1

t

(p(t)  p(0)) .

Thus, b(t) = a(t)P  a(t) satises |b(t)|  2

t  0, as t  .

By Lemma 4.1 above, A = [P  I, 1] has rank n. The n  n submatrix B of A

consisting of all its columns except the rst is invertible. Let c(t) be obtained from

b(t) by removing the rst entry. Since a(t)P  a(t) = b(t) and B is obtained by

deleting the rst column of P  I and adding a column of 1s, a(t)B = [c(t), 1]. Then

a(t) = [c(t) , 1]B1  [0 , 1]B1 establishing the theorem with  = [0 , 1]B1.

We nish this section with the following lemma useful in establishing that a probability

distribution is the stationary probability distribution for a random walk on a connected

graph with edge probabilities.

Lemma 4.3 For a random walk on a strongly connected graph with probabilities on the

edges, if the vector  satises xpxy = ypyx for all x and y and (cid:80)

x x = 1, then  is

the stationary distribution of the walk.

Proof: Since  satises xpxy = ypyx, summing both sides, x = (cid:80)

ypyx and hence 

satises  = P. By Theorem 4.2,  is the unique stationary probability.

y

4.2 Markov Chain Monte Carlo

The Markov Chain Monte Carlo (MCMC) method is a technique for sampling a mul-

tivariate probability distribution p(x), where x = (x1, x2, . . . , xd). The MCMC method is

used to estimate the expected value of a function f (x)

E(f ) =

(cid:88)

x

f (x)p(x).

If each xi can take on two or more values, then there are at least 2d values for x, so an

explicit summation requires exponential time. Instead, one could draw a set of samples,

where each sample x is selected with probability p(x). Averaging f over these samples

provides an estimate of the sum.

To sample according to p(x), design a Markov Chain whose states correspond to the

possible values of x and whose stationary probability distribution is p(x). There are two

general techniques to design such a Markov Chain: the Metropolis-Hastings algorithm

81

and Gibbs sampling, which we will describe in the next two subsections. The Fundamen-

tal Theorem of Markov Chains, Theorem 4.2, states that the average of the function f

over states seen in a suciently long run is a good estimate of E(f ). The harder task

is to show that the number of steps needed before the long-run average probabilities are

close to the stationary distribution grows polynomially in d, though the total number of

states may grow exponentially in d. This phenomenon known as rapid mixing happens for

a number of interesting examples. Section 4.4 presents a crucial tool used to show rapid

mixing.

We used x  Rd to emphasize that distributions are multi-variate. From a Markov

chain perspective, each value x can take on is a state, i.e., a vertex of the graph on which

the random walk takes place. Henceforth, we will use the subscripts i, j, k, . . . to denote

states and will use pi instead of p(x1, x2, . . . , xd) to denote the probability of the state

corresponding to a given set of values for the variables. Recall that in the Markov chain

terminology, vertices of the graph are called states.

Recall the notation that p(t) is the row vector of probabilities of the random walk

being at each state (vertex of the graph) at time t. So, p(t) has as many components

as there are states and its ith component is the probability of being in state i at time t.

Recall the long-term t-step average is

a(t) =

1

t

[p(0) + p(1) +    + p(t  1)] .

(4.1)

The expected value of the function f under the probability distribution p is E(f ) =

(cid:80)

i fipi where fi is the value of f at state i. Our estimate of this quantity will be the

average value of f at the states seen in a t step walk. Call this estimate . Clearly, the

expected value of  is

E() =

(cid:88)

i

(cid:18)1

t

fi

t

(cid:88)

j=1

Prob(cid:0)walk is in state i at time j(cid:1)

(cid:19)

(cid:88)

=

fiai(t).

i

The expectation here is with respect to the coin tosses of the algorithm, not with respect

to the underlying distribution p. Let fmax denote the maximum absolute value of f . It is

easy to see that

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)  fmax

fipi  E()

|pi  ai(t)| = fmax||p  a(t)||1

(4.2)

(cid:88)

(cid:88)

i

i

where the quantity ||p  a(t)||1 is the l1 distance between the probability distributions p

and a(t), often called the total variation distance between the distributions. We will

build tools to upper bound ||p  a(t)||1. Since p is the stationary distribution, the t for

which ||p  a(t)||1 becomes small is determined by the rate of convergence of the Markov

chain to its steady state.

The following proposition is often useful.

82

Proposition 4.4 For two probability distributions p and q,

||p  q||1 = 2

(cid:88)

i

(pi  qi)+ = 2

(cid:88)

(qi  pi)+

i

where x+ = x if x  0 and x+ = 0 if x < 0.

The proof is left as an exercise.

4.2.1 Metropolis-Hasting Algorithm

The Metropolis-Hasting algorithm is a general method to design a Markov chain whose

stationary distribution is a given target distribution p. Start with a connected undirected

graph G on the set of states. If the states are the lattice points (x1, x2, . . . , xd) in Rd

with xi  {0, 1, 2, , . . . , n}, then G could be the lattice graph with 2d coordinate edges at

each interior vertex. In general, let r be the maximum degree of any vertex of G. The

transitions of the Markov chain are dened as follows. At state i select neighbor j with

probability 1

r . Since the degree of i may be less than r, with some probability no edge

is selected and the walk remains at i. If a neighbor j is selected and pj  pi, go to j. If

pj < pi, go to j with probability pj/pi and stay at i with probability 1  pj

. Intuitively,

pi

this favors heavier states with higher pi values. For i adjacent to j in G,

and

Thus,

pij =

(cid:16)

1,

min

1

r

(cid:17)

pj

pi

pii = 1 

(cid:88)

j(cid:54)=i

pij.

pipij =

(cid:16)

1,

min

pi

r

(cid:17)

pj

pi

=

1

r

min(pi, pj) =

(cid:16)

1,

min

pj

r

(cid:17)

pi

pj

= pjpji.

By Lemma 4.3, the stationary probabilities are indeed pi as desired.

2, p(b) = 1

4, p(c) = 1

8, and p(d) = 1

Example: Consider the graph in Figure 4.2. Using the Metropolis-Hasting algorithm,

assign transition probabilities so that the stationary probability of a random walk is

p(a) = 1

8. The maximum degree of any vertex is three,

so at a, the probability of taking the edge (a, b) is 1

6. The probability of taking the

3

edge (a, c) is 1

2

1

1 or 1

12. Thus, the probability

8

3

of staying at a is 2

3. The probability

of taking the edge from c to a is 1

3 and the probability of taking the edge from d to a is

3. Thus, the stationary probability of a is 1

2

1

2, which is the desired

2

probability.

1 or 1

3. The probability of taking the edge from b to a is 1

1 or 1

12 and of taking the edge (a, d) is 1

3 = 1

3 + 1

3 + 1

3 + 1

1

4

1

8

2

8

8

1

1

2

1

3

4

83

p(a) = 1

2

p(b) = 1

4

p(c) = 1

8

p(d) = 1

8

1

2

a

d

1

8

1

4

b

c

1

8

6

2

a  b

1

1

3

4

a  c

1

1

3

8

a  d 1

1

3

8

a  a 1 1

1 = 1

1 = 1

1 = 1

12  1

6  1

12

12

2

2

12 = 2

3

c  a 1

3

c  b

1

3

c  d

1

3

c  c

1 1

3  1

3  1

3 = 0

b  a

b  c

b  b

1

3

1

1

3

8

1 1

6

4

1 = 1

3  1

6 = 1

2

d  a 1

3

d  c

1

3

d  d 1 1

3  1

3 = 1

3

p(a) = p(a)p(a  a) + p(b)p(b  a) + p(c)p(c  a) + p(d)p(d  a)

= 1

2

2

3 + 1

4

1

3 + 1

8

1

3 + 1

8

1

3 = 1

2

p(b) = p(a)p(a  b) + p(b)p(b  b) + p(c)p(c  b)

3 = 1

2 + 1

6 + 1

= 1

2

4

4

1

1

1

8

p(c) = p(a)p(a  c) + p(b)p(b  c) + p(c)p(c  c) + p(d)p(d  c)

3 = 1

8 0 + 1

12 + 1

6 + 1

= 1

2

1

8

8

4

1

1

p(d) = p(a)p(a  d) + p(c)p(c  d) + p(d)p(d  d)

3 = 1

12 + 1

3 + 1

= 1

2

1

1

8

1

8

8

Figure 4.2: Using the Metropolis-Hasting algorithm to set probabilities for a random

walk so that the stationary probability will be the desired probability.

4.2.2 Gibbs Sampling

Gibbs sampling is another Markov Chain Monte Carlo method to sample from a

multivariate probability distribution. Let p (x) be the target distribution where x =

(x1, . . . , xd). Gibbs sampling consists of a random walk on an undirectd graph whose

vertices correspond to the values of x = (x1, . . . , xd) and in which there is an edge from

x to y if x and y dier in only one coordinate. Thus, the underlying graph is like a

d-dimensional lattice except that the vertices in the same coordinate line form a clique.

To generate samples of x = (x1, . . . , xd) with a target distribution p (x), the Gibbs

sampling algorithm repeats the following steps. One of the variables xi is chosen to be

updated. Its new value is chosen based on the marginal probability of xi with the other

variables xed. There are two commonly used schemes to determine which xi to update.

One scheme is to choose xi randomly, the other is to choose xi by sequentially scanning

from x1 to xd.

Suppose that x and y are two states that dier in only one coordinate. Without loss

84

5

8

3,1

2,1

1,1

1

6

1

8

1

3

7

12

3,2

2,2

1,2

1

6

1

6

1

4

1

3

3,3

1

12

2,3

1

12

1,3

1

6

5

12

3

8

3

4

p(1, 1) = 1

3

p(1, 2) = 1

4

p(1, 3) = 1

6

p(2, 1) = 1

8

p(2, 2) = 1

6

p(2, 3) = 1

12

p(3, 1) = 1

6

p(3, 2) = 1

6

p(3, 3) = 1

12

p(11)(12) = 1

2 ( 1

Calculation of edge probability p(11)(12)

d p12/(p11 + p12 + p13) = 1

4)/( 1

3

1

4

1

6) = 1

8/ 9

12 = 1

8

4

3 = 1

6

4

6

4

p(11)(12) = 1

3 = 1

2

3 = 1

p(11)(13) = 1

2

5 = 1

p(11)(21) = 1

2

5 = 2

p(11)(31) = 1

2

Edge probabilities.

1

4

1

6

1

8

1

6

8

8

9

10

15

p(12)(11) = 1

2

p(12)(13) = 1

2

p(12)(22) = 1

2

p(12)(32) = 1

2

1

3

1

6

1

6

1

6

9

4

4

9

3 = 2

3 = 1

7 = 1

7 = 1

12

12

7

7

p(13)(11) = 1

2

p(13)(12) = 1

2

p(13)(23) = 1

2

p(13)(33) = 1

2

1

3

1

4

1

12

1

12

4

9

6

4

3 = 2

3 = 1

1 = 1

1 = 1

3

3

8

8

p(21)(22) = 1

2

p(21)(23) = 1

2

p(21)(11) = 1

2

p(21)(31) = 1

2

8

9

8

1

6

1

12

8

1

3

1

6

3 = 2

3 = 1

5 = 4

5 = 2

15

15

9

8

1

4

p11p(11)(12) = 1

3

p11p(11)(13) = 1

3

p11p(11)(21) = 1

3

8

Verication of a few edges, pipij = pjpji.

2

9 = p12p(12)(11)

2

9 = p13p(13)(11)

4

15 = p21p(21)(11)

6 = 1

9 = 1

10 = 1

6

1

1

Note that the edge probabilities out of a state such as (1,1) do not add up to one.

That is, with some probability the walk stays at the state that it is in. For example,

p(11)(11) = 1  (p(11)(12) + p(11)(13) + p(11)(21) + p(11)(31)) = 1  1

6  1

24  1

32  1

24 = 9

32.

Figure 4.3: Using the Gibbs algorithm to set probabilities for a random walk so that

the stationary probability will be a desired probability.

85

of generality let that coordinate be the rst. Then, in the scheme where a coordinate is

randomly chosen to modify, the probability pxy of going from x to y is

pxy =

1

d

p(y1|x2, x3, . . . , xd).

The normalizing constant is 1/d since (cid:80)

y1

p(y1|x2, x3, . . . , xd) equals 1 and summing over

d coordinates

d

(cid:88)

(cid:88)

i=1

yi

p(yi|x1, x2, . . . , xi1, xi+1 . . . xd) = d

gives a value of d. Similarly,

pyx =

=

1

d

1

d

p(x1|y2, y3, . . . , yd)

p(x1|x2, x3, . . . , xd).

Here use was made of the fact that for j (cid:54)= 1, xj = yj.

It is simple to see that this chain has stationary probability proportional to p (x).

Rewrite pxy as

pxy =

=

=

1

d

1

d

1

d

p(y1|x2, x3, . . . , xd)p(x2, x3, . . . , xd)

p(x2, x3, . . . , xd)

p(y1, x2, x3, . . . , xd)

p(x2, x3, . . . , xd)

p(y)

p(x2, x3, . . . , xd)

again using xj = yj for j (cid:54)= 1. Similarly write

pyx =

1

d

p(x)

p(x2, x3, . . . , xd)

from which it follows that p(x)pxy = p(y)pyx. By Lemma 4.3 the stationary probability

of the random walk is p(x).

4.3 Areas and Volumes

Computing areas and volumes is a classical problem. For many regular gures in

two and three dimensions there are closed form formulae. In Chapter 2, we saw how to

compute volume of a high dimensional sphere by integration. For general convex sets in

d-space, there are no closed form formulae. Can we estimate volumes of d-dimensional

convex sets in time that grows as a polynomial function of d? The MCMC method answes

86

this question in the armative.

One way to estimate the area of the region is to enclose it in a rectangle and estimate

the ratio of the area of the region to the area of the rectangle by picking random points

in the rectangle and seeing what proportion land in the region. Such methods fail in high

dimensions. Even for a sphere in high dimension, a cube enclosing the sphere has expo-

nentially larger area, so exponentially many samples are required to estimate the volume

of the sphere.

It turns out, however, that the problem of estimating volumes of sets can be reduced

to the problem of drawing uniform random samples from sets. Suppose one wants to

estimate the volume of a convex set R. Create a concentric series of larger and larger

spheres15 S1, S2, . . . , Sk such that S1 is contained in R and Sk contains R. Then

Vol(R) = Vol(Sk  R) =

Vol(Sk  R)

Vol(Sk1  R)

Vol(Sk1  R)

Vol(Sk2  R)

 

Vol(S2  R)

Vol(S1  R)

Vol(S1)

If the radius of the sphere Si is 1 + 1

d times the radius of the sphere Si1, then we have:

1 

Vol(Si  R)

Vol(Si1  R)

 e

because Vol(Si)/Vol(Si1) = (cid:0)1 + 1

(cid:1)d < e, and the fraction of Si occupied by R is less

than or equal to the fraction of Si1 occupied by R (due to the convexity of R and the

fact that the center of the spheres lies in R). This implies that the ratio V ol(SiR)

V ol(Si1R) can

be estimated by rejection sampling, i.e., selecting points in Si  R uniformly at random

and computing the fraction in Si1  R, provided one can select points at random from a

d-dimensional convex region.

d

The number of spheres is at most

O(log1+(1/d) r) = O(rd)

where r is the ratio of the radius of Sk to the radius of S1. This means that it suces

to estimate each ratio to a factor of (1  (cid:15)

erd) in order to estimate the overall volume to

error 1  (cid:15).

It remains to show how to draw a uniform random sample from a d-dimensional convex

set. Here we will use the convexity of the set R and thus the sets Si R so that the Markov

chain technique will converge quickly to its stationary probability. To select a random

sample from a d-dimensional convex set, impose a grid on the region and do a random

walk on the grid points. At each time, pick one of the 2d coordinate neighbors of the

current grid point, each with probability 1/(2d) and go to the neighbor if it is still in the

15One could also use rectangles instead of spheres.

87

Si+1

Si

R

Figure 4.4: By sampling the area inside the dark line and determining the fraction of

points in the shaded region we compute V ol(Si+1R)

V ol(SiR) .

To sample we create a grid and assign a probability of one to each grid point inside the

dark lines and zero outside. Using Metropolis-Hasting edge probabilities the stationary

probability will be uniform for each point inside the region and we can sample points

uniformly and determine the fraction within the shaded region.

set; otherwise, stay put and repeat. If the grid length in each of the d coordinate directions

is at most some a, the total number of grid points in the set is at most ad. Although this

is exponential in d, the Markov chain turns out to be rapidly mixing (the proof is beyond

our scope here) and leads to polynomial time bounded algorithm to estimate the volume

of any convex set in Rd.

4.4 Convergence of Random Walks on Undirected Graphs

The Metropolis-Hasting algorithm and Gibbs sampling both involve random walks

on edge-weighted undirected graphs. Given an edge-weighted undirected graph, let wxy

denote the weight of the edge between nodes x and y, with wxy = 0 if no such edge exists.

Let wx = (cid:80)

y wxy. The Markov chain has transition probabilities pxy = wxy/wx. We

assume the chain is connected.

We now claim that the stationary distribution  of this walk has x proportional to

wx, i.e., x = wx/wtotal for wtotal = (cid:80)

x(cid:48) wx(cid:48). Specically, notice that

wxpxy = wx

wxy

wx

= wxy = wyx = wy

wyx

wy

= wypyx.

Therefore (wx/wtotal)pxy = (wy/wtotal)pyx and Lemma 4.3 implies that the values x =

wx/wtotal are the stationary probabilities.

An important question is how fast the walk starts to reect the stationary probability

of the Markov process. If the convergence time was proportional to the number of states,

algorithms such as Metropolis-Hasting and Gibbs sampling would not be very useful since

88

Figure 4.5: A network with a constriction. All edges have weight 1.

the number of states can be exponentially large.

There are clear examples of connected chains that take a long time to converge. A

chain with a constriction, see Figure 4.5, takes a long time to converge since the walk is

unlikely to cross the narrow passage between the two halves, both of which are reasonably

big. We will show in Theorem 4.5 that the time to converge is quantitatively related to

the tightest constriction.

We dene below a combinatorial measure of constriction for a Markov chain, called the

normalized conductance. We will relate normalized conductance to the time by which the

average probability distribution of the chain is guaranteed to be close to the stationary

probability distribution. We call this -mixing time:

Denition 4.1 Fix  > 0. The -mixing time of a Markov chain is the minimum integer t

such that for any starting distribution p, the 1-norm dierence between the t-step running

average probability distribution16 and the stationary distribution is at most .

Denition 4.2 For a subset S of vertices, let (S) denote (cid:80)

conductance (S) of S is

xS x. The normalized

(cid:80)

(x,y)(S, S)

xpxy

min (cid:0)(S), ( S)(cid:1) .

(S) =

There is a simple interpretation of (S). Suppose without loss of generality that (S) 

( S). Then, we may write (S) as

(S) =

(cid:88)

(cid:88)

pxy

.

xS

y S

(cid:124) (cid:123)(cid:122) (cid:125)

b

(cid:0)p(0) + p(1) +    + p(t  1)(cid:1) is called the running average distribution.

16Recall that a(t) = 1

t

x

(S)

(cid:124) (cid:123)(cid:122) (cid:125)

a

89

Here, a is the probability of being in x if we were in the stationary distribution restricted

to S and b is the probability of stepping from x to S in a single step. Thus, (S) is the

probability of moving from S to S in one step if we are in the stationary distribution

restricted to S.

It is easy to show that if we started in the distribution p0,x = s/(S) for x  S and

p0,x = 0 for x  S, the expected number of steps before we step into S is

1(S) + 2(1  (S))(S) + 3(1  (S))2(S) +    =

1

(S)

.

Clearly, to be close to the stationary distribution, we must at least get to S once. So,

mixing time is lower bounded by 1/(S). Since we could have taken any S, mixing time

is lower bounded by the minimum over all S of (S). We dene this quantity to be the

normalized conductance of the Markov Chain.

Denition 4.3 The normalized conductance of the Markov chain, denoted , is dened

by

 = min

SV,S(cid:54)={}

(S).

As we just argued, normalized conductance being high is a necessary condition for

rapid mixing. The theorem below proves the converse that normalized conductance being

high is sucient for mixing. Intuitively, if  is large, the walk rapidly leaves any subset

of states. But the proof of the theorem is quite dicult. After we prove it, we will see

examples where the mixing time is much smaller than the cover time. That is, the number

of steps before a random walk reaches a random state independent of its starting state is

much smaller than the average number of steps needed to reach every state. In fact for

some graphs, called expanders, the mixing time is logarithmic in the number of states.

Theorem 4.5 The -mixing time of a random walk on an undirected graph is

(cid:18) ln(1/min)

23

(cid:19)

O

where min is the minimum stationary probability of any state.

Proof: Let t = c ln(1/min)

23

, for a suitable constant c. Let

a = a(t) =

1

t

(cid:0)p(0) + p(1) +    + p(t  1)(cid:1)

be the running average distribution. We need to show that ||a  ||1  . Let

vi =

ai

i

,

90

g(x)

f (x)

G1 = {1}; G2 = {2, 3, 4}; G3 = {5}.

1

2

3

4

5

x

Figure 4.6: Bounding l1 distance.

and renumber states so that v1  v2  v3     . Thus, early indices i for which vi > 1

are states that currently have too much probability, and late indices i for which vi < 1

are states that currently have too little probability.

Intuitively, to show that ||a  ||1   it is enough to show that the values vi are

relatively at and do not drop too fast as we increase i. We begin by reducing our goal

to a formal statement of that form. Then, in the second part of the proof, we prove that

vi do not fall fast using the concept of probability ows.

We call a state i for which vi > 1 heavy since it has more probability according to

a than its stationary probability. Let i0 be the maximum i such that vi > 1; it is the last

heavy state. By Proposition (4.4):

||a  ||1 = 2

i0(cid:88)

i=1

(vi  1)i = 2

(cid:88)

(1  vi)i.

(4.3)

ii0+1

Let

i = 1 + 2 +    + i.

Dene a function f : [0, i0]  (cid:60) by f (x) = vi  1 for x  [i1, i). See Figure 4.6. Now,

i0(cid:88)

i=1

(vi  1)i =

(cid:90) i0

0

f (x) dx.

(4.4)

We make one more technical modication. We divide {1, 2, . . . , i0} into groups G1, G2, G3, . . . , Gr,

of contiguous subsets. We specify the groups later. Let ut = MaxiGtvi be the maximum

91

value of vi within Gt. Dene a new function g(x) by g(x) = ut  1 for x  iGt[i1, i);

see Figure 4.6. Since g(x)  f (x)

(cid:90) i0

0

f (x) dx 

(cid:90) i0

0

g(x) dx.

(4.5)

We now assert (with ur+1 = 1):

(cid:90) i0

0

g(x) dx =

r

(cid:88)

t=1

(G1  G2  . . .  Gt)(ut  ut+1).

(4.6)

This is just the statement that the area under g(x) in the gure is exactly covered by the

rectangles whose bottom sides are the dotted lines. We leave the formal proof of this to

the reader. We now focus on proving that

r

(cid:88)

t=1

(G1  G2  . . .  Gt)(ut  ut+1)  /2,

(4.7)

for a sub-division into groups we specify which suces by 4.3, 4.4, 4.5 and 4.6. While we

start the proof of (4.7) with a technical observation (4.8), its proof will involve two nice

ideas: the notion of probability ow and reckoning probability ow in two dierent ways.

First, the technical observation: if 2 (cid:80)

ii0+1(1vi)i   then we would be done by (4.3).

So assume now that (cid:80)

ii0+1 i  /2

and so, for any subset A of heavy nodes,

ii0+1(1  vi)i > /2 from which it follows that (cid:80)

Min((A), ( A)) 



2

(A).

(4.8)

We now dene the subsets. G1 will be just {1}. In general, suppose G1, G2, . . . , Gt1 have

already been dened. We start Gt at it = 1+ (end of Gt1). Let it = k. We will dene l,

the last element of Gt to be the largest integer greater than or equal to k and at most i0

so that

l

(cid:88)

j=k+1

j 

k

4

.

In Lemma 4.6 which follows this theorem prove that for groups G1, G2, . . . , Gr, u1.u2, . . . , ur, ur+1

as above

(G1  G2  . . . Gr)(ut  ut+1) 

8

t

.

Now to prove (4.7), we only need an upper bound on r, the number of groups. If Gt =

{k, k + 1, . . . , l}, with l < i0, then by denition of l, we have l+1  (1 + 

2 )k. So,

r  ln1+(/2)(1/1) + 2  ln(1/1)/(/2) + 2. This completes the proof of (4.7) and the

theorem.

We complete the proof of Theorem 4.5 with the proof of Lemma 4.6. The notation in the

lemma is that from the theorem.

92

Lemma 4.6 Suppose groups G1, G2, . . . , Gr, u1.u2, . . . , ur, ur+1 are as above. Then,

(G1  G2  . . . Gr)(ut  ut+1) 

8

t

.

Proof: This is the main lemma. The proof of the lemma uses a crucial idea of probability

ows. We will use two ways of calculating the probability ow from heavy states to light

states when we execute one step of the Markov chain starting at probabilities a. The

probability vector after that step is aP . Now, a  aP is the net loss of probability for

each state due to the step.

Consider a particular group Gt = {k, k + 1, . . . , l}, say. First consider the case when

k < i0. Let A = {1, 2, . . . , k}. The net loss of probability for each state from the set A in

one step is (cid:80)k

i=1(ai  (aP )i) which is at most 2

t by the proof of Theorem 4.2.

Another way to reckon the net loss of probability from A is to take the dierence of

the probability ow from A to A and the ow from A to A. For any i < j,

net-ow(i, j) = ow(i, j)  ow(j, i) = ipijvi  jpjivj = jpji(vi  vj)  0,

Thus, for any two states i and j, with i heavier than j, i.e., i < j, there is a non-negative

net ow from i to j. (This is intuitively reasonable since it says that probability is owing

from heavy to light states.) Since l  k, the ow from A to {k + 1, k + 2, . . . , l} minus

the ow from {k + 1, k + 2, . . . , l} to A is nonnegative. Since for i  k and j > l, we have

vi  vk and vj  vl+1, the net loss from A is at least

(cid:88)

(cid:88)

jpji(vi  vj)  (vk  vl+1)

jpji.

Thus,

Since

ik

j>l

ik

j>l

2

t

.

(vk  vl+1)

jpji 

(cid:88)

ik

j>l

k

(cid:88)

l

(cid:88)

l

(cid:88)

jpji 

i=1

j=k+1

j=k+1

j  (A)/4

and by the denition of , using (4.8)

(cid:88)

ik<j

jpji  Min((A), ( A))  k/2,

(4.9)

we have, (cid:80)

jpji = (cid:80)

ik<j jpji  (cid:80)

ik;jl jpji  k/4. Substituting this into the

ik

j>l

inequality (4.9) gives

vk  vl+1 

8

tk

,

(4.10)

proving the lemma provided k < i0. If k = i0, the proof is similar but simpler.

93

4.4.1 Using Normalized Conductance to Prove Convergence

We now apply Theorem 4.5 to some examples to illustrate how the normalized con-

ductance bounds the rate of convergence. In each case we compute the mixing time for

the uniform probability function on the vertices. Our rst examples will be simple graphs.

The graphs do not have rapid converge, but their simplicity helps illustrate how to bound

the normalized conductance and hence the rate of convergence.

A 1-dimensional lattice

Consider a random walk on an undirected graph consisting of an n-vertex path with

self-loops at the both ends. With the self loops, we have pxy = 1/2 on all edges (x, y),

and so the stationary distribution is a uniform 1

n over all vertices by Lemma 4.3. The set

with minimum normalized conductance is the set S with probability (S)  1

2 having the

smallest ratio of probability mass exiting it, (cid:80)

(x,y)(S, S) xpxy, to probability mass inside

it, (S). This set consists of the rst n/2 vertices, for which the numerator is 1

2n and

denominator is 1

2. Thus,

(S) =

1

n

.

By Theorem 4.5, for  a constant such as 1/100, after O(n2 log n/(cid:15)3) steps, ||at  ||1 

1/100. This graph does not have rapid convergence. The hitting time and the cover time

are O(n2). In many interesting cases, the mixing time may be much smaller than the

cover time. We will see such an example later.

A 2-dimensional lattice

Consider the n  n lattice in the plane where from each point there is a transition to

each of the coordinate neighbors with probability 1/4. At the boundary there are self-loops

with probability 1-(number of neighbors)/4. It is easy to see that the chain is connected.

Since pij = pji, the function fi = 1/n2 satises fipij = fjpji and by Lemma 4.3, f is the

stationary distribution. Consider any subset S consisting of at most half the states. If

|S|  n2

4 , then the subset with the fewest edges leaving it consists of some number of

columns plus perhaps one additional partial column. The number of edges leaving S is at

least n. Thus

(cid:88)

(cid:88)

ipij  (cid:0)n

(cid:1) = 

1

n2

(cid:19)

.

(cid:18) 1

n

Since |S|  n2

4 , in this case

iS

j S

(S)  

(cid:32)

1/n

min (cid:0) S

n2 , S

n2

(cid:1)

(cid:33)

= 

(cid:19)

.

(cid:18) 1

n

If |S| < n2

4 , the subset S of a given size that has the minimum number of edges leaving

consists of a square located at the lower left hand corner of the grid (Exercise 4.21). If

94

|S| is not a perfect square then the right most column of S is short. Thus at least 2(cid:112)|S|

points in S are adjacent to points in S. Each of these points contributes ipij = ( 1

n2 ) to

the ow(S, S). Thus,

(cid:88)

(cid:88)

iS

j S

ipij 

c(cid:112)|S|

n2

and

(cid:80)

c

min (cid:0)(S), ( S)(cid:1) 

(cid:112)|S|

Thus, in either case, after O(n2 ln n/(cid:15)3) steps, |a(t)  |1  (cid:15).

c(cid:112)|S|/n2

|S|/n2 =

j S ipij

(S) =

(cid:80)

iS

= 

(cid:19)

.

(cid:18) 1

n

A lattice in d-dimensions

Next consider the n  n      n lattice in d-dimensions with a self-loop at each

boundary point with probability 1  (number of neighbors)/2d. The self loops make all

i equal to nd. View the lattice as an undirected graph and consider the random walk

on this undirected graph. Since there are nd states, the cover time is at least nd and

thus exponentially dependent on d. It is possible to show (Exercise 4.22) that  is ( 1

dn).

Since all i are equal to nd, the mixing time is O(d3n2 ln n/3), which is polynomially

bounded in n and d.

The d-dimensional lattice is related to the Metropolis-Hastings algorithm and Gibbs

sampling although in those constructions there is a nonuniform probability distribution at

the vertices. However, the d-dimension lattice case suggests why the Metropolis-Hastings

and Gibbs sampling constructions might converge fast.

A clique

Consider an n vertex clique with a self loop at each vertex. For each edge, pxy = 1

n

and thus for each vertex, x = 1

n . Let S be a subset of the vertices. Then

x =

|S|

n

.

(cid:88)

xS

(cid:88)

(x,y)(S, S)

xpxy = xpxy|S||S| =

1

n2 |S||S|

and

(S) =

(cid:80)

min((cid:80)

(x,y)(S, S) xpxy

xS x, (cid:80)

x S x)

=

1

n2 |S||S|

n|S|, 1

n |S|)

min( 1

=

1

n

max(|S|, |S|) =

1

2

.

This gives a bound on the -mixing time of

(cid:33)

(cid:32)ln 1

min

2(cid:15)3

O

= O

(cid:18)ln n

(cid:15)3

(cid:19)

.

95

However, a walker on the clique starting from any probability distribution will in one step

be exactly at the stationary probability distribution.

A connected undirected graph

Next consider a random walk on a connected n vertex undirected graph where at each

vertex all edges are equally likely. The stationary probability of a vertex equals the degree

of the vertex divided by the sum of degrees. That is, if the degree of vertex x is dx and

the number of edges in the graph is m, then x = dx

2m . Notice that for any edge (x, y) we

have

(cid:19)

xpxy =

(cid:18) dx

2m

(cid:19) (cid:18) 1

dx

=

1

2m

.

Therefore, for any S, the total conductance of edges out of S is at least

2m  1

 is at least 1

O(m2 ln n/3) = O(n4 ln n/3).

1

2m , and so

= O(ln n). Thus, the mixing time is

m . Since min  1

n2 , ln 1

min

The Gaussian distribution on the interval [-1,1]

Consider the interval [1, 1]. Let  be a grid size specied later and let G be the

graph consisting of a path on the 2

 + 1 vertices {1, 1 + , 1 + 2, . . . , 1  , 1} having

self loops at the two ends. Let x = cex2 for x  {1, 1 + , 1 + 2, . . . , 1  , 1}

where  > 1 and c has been adjusted so that (cid:80)

x x = 1.

We now describe a simple Markov chain with the x as its stationary probability and

argue its fast convergence. With the Metropolis-Hastings construction, the transition

probabilities are

px,x+ =

(cid:32)

min

1,

1

2

e(x+)2

ex2

(cid:33)

and px,x =

(cid:32)

min

1,

e(x)2

ex2

(cid:33)

.

1

2

Let S be any subset of states with (S)  1

[k, 1] for k  2. It is easy to see that

2. First consider the case when S is an interval

(cid:90) 

(S) 

cex2 dx

x=(k1)

(cid:90) 

(k1)

(cid:32)

x

(k  1)

ce((k1))2

(k  1)



= O

(cid:33)

.

cex2 dx

Now there is only one edge from S to S and total conductance of edges out of S is

(cid:88)

(cid:88)

iS

j /S

ipij = kpk,(k1) = min(cek22, ce(k1)22) = cek22.

96

Using 2  k  1/,   1, and ( S)  1,

(S) =

ow(S, S)

min((S), ( S))

 cek22 (k  1)

ce((k1))2

 ((k  1)e2(2k1))  (eO()).

For the grid size less than the variance of the Gaussian distribution,  < 1

 , we have  < 1,

so eO() = (1), thus, (S)  (). Now, min  ce  e1/, so ln(1/min)  1/.

If S is not an interval of the form [k, 1] or [1, k], then the situation is only better

since there is more than one boundary point which contributes to ow(S, S). We do

not present this argument here. By Theorem 4.5 in (1/233) steps, a walk gets within

 of the steady state distribution.

In the uniform probability case the (cid:15)-mixing time is bounded by n2 log n. For compar-

ison, in the Gaussian case set  = 1/n and  = 1/3. This gives an (cid:15)-mixing time bound

of n3. In the Gaussian case with the entire initial probability on the rst vertex, the chain

begins to converge faster to the stationary probability than the uniform distribution case

since the chain favors higher degree vertices. However, ultimately the distribution must

reach the lower probability vertices on the other side of the Gaussians maximum and

here the chain is slower since it favors not leaving the higher probability vertices.

In these examples, we have chosen simple probability distributions. The methods ex-

tend to more complex situations.

4.5 Electrical Networks and Random Walks

In the next few sections, we study the relationship between electrical networks and

random walks on undirected graphs. The graphs have nonnegative weights on each edge.

A step is executed by picking a random edge from the current vertex with probability

proportional to the edges weight and traversing the edge.

An electrical network is a connected, undirected graph in which each edge (x, y) has

a resistance rxy > 0. In what follows, it is easier to deal with conductance dened as the

reciprocal of resistance, cxy = 1

, rather than resistance. Associated with an electrical

rxy

network is a random walk on the underlying graph dened by assigning a probability

pxy = cxy/cx to the edge (x, y) incident to the vertex x, where the normalizing constant cx

equals (cid:80)

cxy. Note that although cxy equals cyx, the probabilities pxy and pyx may not be

y

equal due to the normalization required to make the probabilities at each vertex sum to

one. We shall soon see that there is a relationship between current owing in an electrical

97

network and a random walk on the underlying graph.

Since we assume that the undirected graph is connected, by Theorem 4.2 there is

a unique stationary probability distribution.The stationary probability distribution is 

where x = cx

c0

cx. To see this, for all x and y

with c0 = (cid:80)

x

xpxy =

cx

c0

cxy

cx

=

cy

c0

cyx

cy

= ypyx

and hence by Lemma 4.3,  is the unique stationary probability.

Harmonic functions

Harmonic functions are useful in developing the relationship between electrical net-

works and random walks on undirected graphs. Given an undirected graph, designate

a nonempty set of vertices as boundary vertices and the remaining vertices as interior

vertices. A harmonic function g on the vertices is a function whose value at the boundary

vertices is xed to some boundary condition, and whose value at any interior vertex x is

a weighted average of its values at all the adjacent vertices y, with weights pxy satisfying

(cid:80)

y pxy = 1 for each x. Thus, if at every interior vertex x for some set of weights pxy

satisfying (cid:80)

gypxy, then g is an harmonic function.

y pxy = 1, gx = (cid:80)

y

Example: Convert an electrical network with conductances cxy to a weighted, undirected

graph with probabilities pxy. Let f be a function satisfying f P = f where P is the matrix

of probabilities. It follows that the function gx = fx

cx

is harmonic.

gx = fx

cx

= 1

cx

= 1

cx

(cid:80)

y

fy

(cid:80)

y

cxy

cy

fypyx = 1

cx

(cid:80)

y

fy

cyx

cy

= (cid:80)

y

fy

cy

cxy

cx

= (cid:80)

gypxy

y

A harmonic function on a connected graph takes on its maximum and minimum on

the boundary. This is easy to see for the following reason. Suppose the maximum does

not occur on the boundary. Let S be the set of vertices at which the maximum value is

attained. Since S contains no boundary vertices, S is nonempty. Connectedness implies

that there is at least one edge (x, y) with x  S and y  S. The value of the function at

x is the weighted average of the value at its neighbors, all of which are less than or equal

to the value at x and the value at y is strictly less, a contradiction. The proof for the

minimum value is identical.

There is at most one harmonic function satisfying a given set of equations and bound-

ary conditions. For suppose there were two solutions, f (x) and g(x). The dierence of two

98

6

1

8

1

6

5

4

5

8

5

3

5

3

Graph with boundary vertices

dark and boundary conditions

specied.

Values of harmonic function

satisfying boundary conditions

where the edge weights at

each vertex are equal

Figure 4.7: Graph illustrating an harmonic function.

solutions is itself harmonic. Since h(x) = f (x)g(x) is harmonic and has value zero on the

boundary, by the min and max principles it has value zero everywhere. Thus f (x) = g(x).

The analogy between electrical networks and random walks

There are important connections between electrical networks and random walks on

undirected graphs. Choose two vertices a and b. Attach a voltage source between a and b

so that the voltage va equals one volt and the voltage vb equals zero. Fixing the voltages

at va and vb induces voltages at all other vertices, along with a current ow through the

edges of the network. What we will show below is the following. Having xed the voltages

at the vertices a and b, the voltage at an arbitrary vertex x equals the probability that a

random walk that starts at x will reach a before it reaches b. We will also show there is

a related probabilistic interpretation of current as well.

Probabilistic interpretation of voltages

Before relating voltages and probabilities, we rst show that the voltages form a har-

monic function. Let x and y be adjacent vertices and let ixy be the current owing through

the edge from x to y. By Ohms law,

ixy =

vx  vy

rxy

= (vx  vy)cxy.

By Kirchhos law the currents owing out of each vertex sum to zero.

(cid:88)

y

ixy = 0

99

Replacing currents in the above sum by the voltage dierence times the conductance

yields

(cid:88)

(vx  vy)cxy = 0

or

y

(cid:88)

vx

cxy =

(cid:88)

vycxy.

y

y

Observing that (cid:80)

vx = (cid:80)

y

cxy = cx and that pxy = cxy

cx

, yields vxcx = (cid:80)

vypxycx. Hence,

y

vypxy. Thus, the voltage at each vertex x is a weighted average of the volt-

y

ages at the adjacent vertices. Hence the voltages form a harmonic function with {a, b} as

the boundary.

Let px be the probability that a random walk starting at vertex x reaches a before b.

Clearly pa = 1 and pb = 0. Since va = 1 and vb = 0, it follows that pa = va and pb = vb.

Furthermore, the probability of the walk reaching a from x before reaching b is the sum

over all y adjacent to x of the probability of the walk going from x to y in the rst step

and then reaching a from y before reaching b. That is

px =

(cid:88)

y

pxypy.

Hence, px is the same harmonic function as the voltage function vx and v and p satisfy the

same boundary conditions at a and b.. Thus, they are identical functions. The probability

of a walk starting at x reaching a before reaching b is the voltage vx.

Probabilistic interpretation of current

In a moment, we will set the current into the network at a to have a value which we will

equate with one random walk. We will then show that the current ixy is the net frequency

with which a random walk from a to b goes through the edge xy before reaching b. Let

ux be the expected number of visits to vertex x on a walk from a to b before reaching b.

Clearly ub = 0. Consider a node x not equal to a or b. Every time the walk visits x, it

must have come from some neighbor y. Thus, the expected number of visits to x before

reaching b is the sum over all neighbors y of the expected number of visits uy to y before

reaching b times the probability pyx of going from y to x. That is,

Since cxpxy = cypyx

ux =

(cid:88)

y

uypyx.

ux =

(cid:88)

y

uy

cxpxy

cy

100

and hence ux

cx

= (cid:80)

y

uy

cy

pxy. It follows that ux

cx

is harmonic with a and b as the boundary

. Now ux

cx

where the boundary conditions are ub = 0 and ua equals some xed value. Now, ub

= 0.

cb

Setting the current into a to one, xed the value of va. Adjust the current into a so that

va equals ua

and vx satisfy the same boundary conditions and thus are the same

ca

harmonic function. Let the current into a correspond to one walk. Note that if the walk

starts at a and ends at b, the expected value of the dierence between the number of times

the walk leaves a and enters a must be one. This implies that the amount of current into

a corresponds to one walk.

Next we need to show that the current ixy is the net frequency with which a random

walk traverses edge xy.

ixy = (vx  vy)cxy =

(cid:19)

(cid:18) ux

cx



uy

cy

cxy = ux

cxy

cx

 uy

cxy

cy

= uxpxy  uypyx

The quantity uxpxy is the expected number of times the edge xy is traversed from x to y

and the quantity uypyx is the expected number of times the edge xy is traversed from y to

x. Thus, the current ixy is the expected net number of traversals of the edge xy from x to y.

Eective resistance and escape probability

Set va = 1 and vb = 0. Let ia be the current owing into the network at vertex a and

out at vertex b. Dene the eective resistance re between a and b to be re = va

and

ia

the eective conductance ce to be ce = 1

. Dene the escape probability, pescape, to

re

be the probability that a random walk starting at a reaches b before returning to a. We

now show that the escape probability is ce

. For convenience, assume that a and b are

ca

not adjacent. A slight modication of the argument suces for the case when a and b are

adjacent.

(cid:88)

ia =

(va  vy)cay

Since va = 1,

y

(cid:88)

ia =

cay  ca

y

(cid:34)

= ca

1 

(cid:88)

y

(cid:88)

y

vy

cay

ca

(cid:35)

payvy

.

For each y adjacent to the vertex a, pay is the probability of the walk going from vertex

a to vertex y. Earlier we showed that vy is the probability of a walk starting at y going

to a before reaching b. Thus, (cid:80)

payvy is the probability of a walk starting at a returning

to a before reaching b and 1  (cid:80)

payvy is the probability of a walk starting at a reaching

y

y

101

b before returning to a. Thus, ia = capescape. Since va = 1 and ce = ia

va

ce = ia . Thus, ce = capescape and hence pescape = ce

ca

.

, it follows that

For a nite connected graph, the escape probability will always be nonzero. Consider

an innite graph such as a lattice and a random walk starting at some vertex a. Form a

series of nite graphs by merging all vertices at distance d or greater from a into a single

vertex b for larger and larger values of d. The limit of pescape as d goes to innity is the

probability that the random walk will never return to a. If pescape  0, then eventually

any random walk will return to a. If pescape  q where q > 0, then a fraction of the walks

never return. Thus, the escape probability terminology.

4.6 Random Walks on Undirected Graphs with Unit Edge Weights

We now focus our discussion on random walks on undirected graphs with uniform

edge weights. At each vertex, the random walk is equally likely to take any edge. This

corresponds to an electrical network in which all edge resistances are one. Assume the

graph is connected. We consider questions such as what is the expected time for a random

walk starting at a vertex x to reach a target vertex y, what is the expected time until the

random walk returns to the vertex it started at, and what is the expected time to reach

every vertex?

Hitting time

The hitting time hxy, sometimes called discovery time, is the expected time of a ran-

dom walk starting at vertex x to reach vertex y. Sometimes a more general denition is

given where the hitting time is the expected time to reach a vertex y from a given starting

probability distribution.

One interesting fact is that adding edges to a graph may either increase or decrease

hxy depending on the particular situation. Adding an edge can shorten the distance from

x to y thereby decreasing hxy or the edge could increase the probability of a random walk

going to some far o portion of the graph thereby increasing hxy. Another interesting

fact is that hitting time is not symmetric. The expected time to reach a vertex y from a

vertex x in an undirected graph may be radically dierent from the time to reach x from y.

We start with two technical lemmas. The rst lemma states that the expected time

to traverse a path of n vertices is  (n2).

Lemma 4.7 The expected time for a random walk starting at one end of a path of n

vertices to reach the other end is  (n2).

Proof: Consider walking from vertex 1 to vertex n in a graph consisting of a single path

of n vertices. Let hij, i < j, be the hitting time of reaching j starting from i. Now h12 = 1

102

and

hi,i+1 = 1

2 + 1

2(1 + hi1,i+1) = 1 + 1

2 (hi1,i + hi,i+1)

2  i  n  1.

Solving for hi,i+1 yields the recurrence

Solving the recurrence yields

hi,i+1 = 2 + hi1,i.

hi,i+1 = 2i  1.

To get from 1 to n, you need to rst reach 2, then from 2 (eventually) reach 3, then from

3 (eventually) reach 4, and so on. Thus by linearity of expectation,

h1,n =

n1

(cid:88)

i=1

hi,i+1 =

n1

(cid:88)

i=1

(2i  1)

n1

(cid:88)

n1

(cid:88)

1

i 

= 2

i=1

i=1

n (n  1)

2

 (n  1)

= 2

= (n  1)2 .

The next lemma shows that the expected time spent at vertex i by a random walk

from vertex 1 to vertex n in a chain of n vertices is 2(i  1) for 2  i  n  1.

Lemma 4.8 Consider a random walk from vertex 1 to vertex n in a chain of n vertices.

Let t(i) be the expected time spent at vertex i. Then

t (i) =







n  1

i = 1

2 (n  i) 2  i  n  1

1

i = n.

Proof: Now t (n) = 1 since the walk stops when it reaches vertex n. Half of the time when

the walk is at vertex n  1 it goes to vertex n. Thus t (n  1) = 2. For 3  i < n  1,

t (i) = 1

2t (2) + 1 and t (2) =

t (1) + 1

2 [t (i  1) + t (i + 1)] and t (1) and t (2) satisfy t (1) = 1

2t (3). Solving for t(i + 1) for 3  i < n  1 yields

t(i + 1) = 2t(i)  t(i  1)

which has solution t(i) = 2(n  i) for 3  i < n  1. Then solving for t(2) and t(1) yields

t (2) = 2 (n  2) and t (1) = n  1. Thus, the total time spent at vertices is

n  1 + 2 (1 + 2 +    + n  2) + 1 = (n  1) + 2

(n  1)(n  2)

2

+ 1 = (n  1)2 + 1

which is one more than h1n and thus is correct.

103

clique of

size n/2

x

(cid:124)

(cid:123)(cid:122)

n/2

y

(cid:125)

Figure 4.8:

hitting time.

Illustration that adding edges to a graph can either increase or decrease

Adding edges to a graph might either increase or decrease the hitting time hxy. Con-

sider the graph consisting of a single path of n vertices. Add edges to this graph to get the

graph in Figure 4.8 consisting of a clique of size n/2 connected to a path of n/2 vertices.

Then add still more edges to get a clique of size n. Let x be the vertex at the midpoint of

the original path and let y be the other endpoint of the path consisting of n/2 vertices as

shown in the gure. In the rst graph consisting of a single path of length n, hxy =  (n2).

In the second graph consisting of a clique of size n/2 along with a path of length n/2,

hxy =  (n3).To see this latter statement, note that starting at x, the walk will go down

the path towards y and return to x for n/2  1 times on average before reaching y for the

rst time, by Lemma 4.8. Each time the walk in the path returns to x, with probability

(n/2  1)/(n/2) it enters the clique and thus on average enters the clique (n) times

before starting down the path again. Each time it enters the clique, it spends (n) time

in the clique before returning to x. It then reenters the clique (n) times before starting

down the path to y. Thus, each time the walk returns to x from the path it spends (n2)

time in the clique before starting down the path towards y for a total expected time that

is (n3) before reaching y. In the third graph, which is the clique of size n, hxy =  (n).

Thus, adding edges rst increased hxy from n2 to n3 and then decreased it to n.

Hitting time is not symmetric even in the case of undirected graphs. In the graph of

Figure 4.8, the expected time, hxy, of a random walk from x to y, where x is the vertex of

attachment and y is the other end vertex of the chain, is (n3). However, hyx is (n2).

Commute time

The commute time, commute(x, y), is the expected time of a random walk starting at

x reaching y and then returning to x. So commute(x, y) = hxy + hyx. Think of going

from home to oce and returning home. Note that commute time is symmetric. We now

relate the commute time to an electrical quantity, the eective resistance. The eective

resistance between two vertices x and y in an electrical network is the voltage dierence

104

between x and y when one unit of current is inserted at vertex x and withdrawn from

vertex y.

Theorem 4.9 Given a connected, undirected graph, consider the electrical network where

each edge of the graph is replaced by a one ohm resistor. Given vertices x and y, the

commute time, commute(x, y), equals 2mrxy where rxy is the eective resistance from x

to y and m is the number of edges in the graph.

Proof: Insert at each vertex i a current equal to the degree di of vertex i. The total

current inserted is 2m where m is the number of edges. Extract from a specic vertex j

all of this 2m current (note: for this to be legal, the graph must be connected). Let vij

be the voltage dierence from i to j. The current into i divides into the di resistors at

vertex i. The current in each resistor is proportional to the voltage across it. Let k be a

vertex adjacent to i. Then the current through the resistor between i and k is vij  vkj,

the voltage drop across the resistor. The sum of the currents out of i through the resistors

must equal di, the current injected into i.

di =

(cid:88)

k adj

to i

(vij  vkj) = divij 

vkj.

(cid:88)

k adj

to i

Solving for vij

vij = 1 +

(cid:88)

k adj

to i

1

di

vkj =

(cid:88)

k adj

to i

1

di

(1 + vkj).

(4.11)

Now the hitting time from i to j is the average time over all paths from i to k adjacent

to i and then on from k to j. This is given by

hij =

(cid:88)

k adj

to i

1

di

(1 + hkj).

(4.12)

Subtracting (4.12) from (4.11), gives vij  hij = (cid:80)

1

di

(vkj  hkj). Thus, the function

k adj

to i

vij  hij is harmonic. Designate vertex j as the only boundary vertex. The value of

vij  hij at i = j, namely vjj  hjj, is zero, since both vjj and hjj are zero. So the function

vij  hij must be zero everywhere. Thus, the voltage vij equals the expected time hij from

i to j.

To complete the proof of Theorem 4.9, note that hij = vij is the voltage from i to j

when currents are inserted at all vertices in the graph and extracted at vertex j. If the

current is extracted from i instead of j, then the voltages change and vji = hji in the new

105

i











j



=

=

i











j



Insert current at each vertex

equal to degree of the vertex.

Extract 2m at vertex j, vij = hij.

(a)

Extract current from i instead of j.

For new voltages vji = hji.

(b)

=

i







j







Reverse currents in (b).

For new voltages vji = hji.

Since vji = vij, hji = vij.

(c)

i

2m

=

j

2m

=

Superpose currents in (a) and (c).

2mrij = vij = hij + hji = commute(i, j).

(d)

Figure 4.9: Illustration of proof that commute(x, y) = 2mrxy where m is the number of

edges in the undirected graph and rxy is the eective resistance between x and y.

setup. Finally, reverse all currents in this latter step. The voltages change again and for

the new voltages vji = hji. Since vji = vij, we get hji = vij.

Thus, when a current is inserted at each vertex equal to the degree of the vertex and

the current is extracted from j, the voltage vij in this set up equals hij. When we extract

the current from i instead of j and then reverse all currents, the voltage vij in this new set

up equals hji. Now, superpose both situations, i.e., add all the currents and voltages. By

linearity, for the resulting vij, which is the sum of the other two vijs, is vij = hij + hji. All

currents into or out of the network cancel except the 2m amps injected at i and withdrawn

at j. Thus, 2mrij = vij = hij + hji = commute(i, j) or commute(i, j) = 2mrij where rij

is the eective resistance from i to j.

The following corollary follows from Theorem 4.9 since the eective resistance ruv is

less than or equal to one when u and v are connected by an edge.

Corollary 4.10 If vertices x and y are connected by an edge, then hxy + hyx  2m where

m is the number of edges in the graph.

Proof: If x and y are connected by an edge, then the eective resistance rxy is less than

or equal to one.

106

Corollary 4.11 For vertices x and y in an n vertex graph, the commute time, commute(x, y),

is less than or equal to n3.

Proof: By Theorem 4.9 the commute time is given by the formula commute(x, y) =

2mrxy where m is the number of edges. In an n vertex graph there exists a path from x

to y of length at most n. Since the resistance can not be greater than that of any path

(cid:1)

from x to y, rxy  n. Since the number of edges is at most (cid:0)n

2

commute(x, y) = 2mrxy  2

(cid:18)n

2

(cid:19)

n = n3.

While adding edges into a graph can never increase the eective resistance between

two given nodes x and y, it may increase or decrease the commute time. To see this

consider three graphs: the graph consisting of a chain of n vertices, the graph of Figure

4.8, and the clique on n vertices.

Cover time

The cover time, cover(x, G) , is the expected time of a random walk starting at vertex x

in the graph G to reach each vertex at least once. We write cover(x) when G is understood.

The cover time of an undirected graph G, denoted cover(G), is

cover(G) = max

x

cover(x, G).

For cover time of an undirected graph, increasing the number of edges in the graph

may increase or decrease the cover time depending on the situation. Again consider three

graphs, a chain of length n which has cover time (n2), the graph in Figure 4.8 which has

cover time (n3), and the complete graph on n vertices which has cover time (n log n).

Adding edges to the chain of length n to create the graph in Figure 4.8 increases the

cover time from n2 to n3 and then adding even more edges to obtain the complete graph

reduces the cover time to n log n.

Note: The cover time of a clique is (n log n) since this is the time to select every

integer out of n integers with high probability, drawing integers at random. This is called

the coupon collector problem. The cover time for a straight line is (n2) since it is the

same as the hitting time. For the graph in Figure 4.8, the cover time is (n3) since one

takes the maximum over all start states and cover(x, G) =  (n3) where x is the vertex

of attachment.

Theorem 4.12 Let G be a connected graph with n vertices and m edges. The time for a

random walk to cover all vertices of the graph G is bounded above by 4m(n  1).

107

Proof: Consider a depth rst search of the graph G starting from some vertex z and let

T be the resulting depth rst search spanning tree of G. The depth rst search covers

every vertex. Consider the expected time to cover every vertex in the order visited by the

depth rst search. Clearly this bounds the cover time of G starting from vertex z. Note

that each edge in T is traversed twice, once in each direction.

cover (z, G) 

(cid:88)

hxy.

(x,y)T

(y,x)T

If (x, y) is an edge in T , then x and y are adjacent and thus Corollary 4.10 implies

hxy  2m. Since there are n  1 edges in the dfs tree and each edge is traversed twice,

once in each direction, cover(z)  4m(n  1). This holds for all starting vertices z. Thus,

cover(G)  4m(n  1).

The theorem gives the correct answer of n3 for the n/2 clique with the n/2 tail. It

gives an upper bound of n3 for the n-clique where the actual cover time is n log n.

Let rxy be the eective resistance from x to y. Dene the resistance re (G) of a graph

G by re (G) = max

x,y

(rxy).

Theorem 4.13 Let G be an undirected graph with m edges. Then the cover time for G

is bounded by the following inequality

m re (G)  cover(G)  6e m re (G) ln n + n

where e  2.718 is Eulers constant and re (G) is the resistance of G.

Proof: By denition re (G) = max

x,y

(rxy). Let u and v be the vertices of G for which

2commute(u, v). Note that 1

rxy is maximum. Then re (G) = ruv. By Theorem 4.9, commute(u, v) = 2mruv. Hence

mruv = 1

2commute(u, v) is the average of huv and hvu, which

is clearly less than or equal to max(huv, hvu). Finally, max(huv, hvu) is less than or equal

to max(cover(u, G), cover(v, G)) which is clearly less than the cover time of G. Putting

these facts together gives the rst inequality in the theorem.

m re (G) = mruv = 1

2commute(u, v)  max(huv, hvu)  cover(G)

For the second inequality in the theorem, by Theorem 4.9, for any x and y, commute(x, y)

equals 2mrxy which is less than or equal to 2m re (G), implying hxy  2m re (G). By

the Markov inequality, since the expected time to reach y starting at any x is less than

2m re (G), the probability that y is not reached from x in 2m re (G)e steps is at most

1

e . Thus, the probability that a vertex y has not been reached in 6e m re (G) log n steps

is at most 1

n3 because a random walk of length 6e mre (G) log n is a sequence of

e

3 log n random walks, each of length 2emre (G) and each possibly starting from dierent

3 ln n = 1

108

vertices. Suppose after a walk of 6em re (G) log n steps, vertices v1, v2, . . . , vl had not

been reached. Walk until v1 is reached, then v2, etc. By Corollary 4.11 the expected time

for each of these is n3, but since each happens only with probability 1/n3, we eectively

take O(1) time per vi, for a total time at most n. More precisely,

cover(G)  6em re (G) log n +

 6em re (G) log n +

(cid:88)

v

(cid:88)

v

Prob (v was not visited in the rst 6em re (G) steps) n3

1

n3 n3  6em re (G) + n.

4.7 Random Walks in Euclidean Space

Many physical processes such as Brownian motion are modeled by random walks.

Random walks in Euclidean d-space consisting of xed length steps parallel to the co-

ordinate axes are really random walks on a d-dimensional lattice and are a special case

of random walks on graphs. In a random walk on a graph, at each time unit an edge

from the current vertex is selected at random and the walk proceeds to the adjacent vertex.

Random walks on lattices

We now apply the analogy between random walks and current to lattices. Consider

a random walk on a nite segment n, . . . , 1, 0, 1, 2, . . . , n of a one dimensional lattice

starting from the origin. Is the walk certain to return to the origin or is there some prob-

ability that it will escape, i.e., reach the boundary before returning? The probability of

reaching the boundary before returning to the origin is called the escape probability. We

shall be interested in this quantity as n goes to innity.

Convert the lattice to an electrical network by replacing each edge with a one ohm

resistor. Then the probability of a walk starting at the origin reaching n or n before

returning to the origin is the escape probability given by

pescape =

ce

ca

where ce is the eective conductance between the origin and the boundary points and ca

is the sum of the conductances at the origin. In a d-dimensional lattice, ca = 2d assuming

that the resistors have value one. For the d-dimensional lattice

pescape =

1

2d re

In one dimension, the electrical network is just two series connections of n one-ohm re-

sistors connected in parallel. So as n goes to innity, re goes to innity and the escape

probability goes to zero as n goes to innity. Thus, the walk in the unbounded one

109

0

1

2

3

4

20

12

Number of resistors

in parallel

 

(a)

(b)

Figure 4.10: 2-dimensional lattice along with the linear network resulting from shorting

resistors on the concentric squares about the origin.

dimensional lattice will return to the origin with probability one. Note, however, that

the expected time to return to the origin having taken one step away, which is equal to

commute(1, 0), is innite (Theorem 4.9.

Two dimensions

For the 2-dimensional lattice, consider a larger and larger square about the origin for

the boundary as shown in Figure 4.10a and consider the limit of re as the squares get

larger. Shorting the resistors on each square can only reduce re . Shorting the resistors

results in the linear network shown in Figure 4.10b. As the paths get longer, the number

of resistors in parallel also increases. The resistance between vertex i and i + 1 is really

4(2i + 1) unit resistors in parallel. The eective resistance of 4(2i + 1) resistors in parallel

is 1/4(2i + 1). Thus,

re  1

4 + 1

12 + 1

20 +    = 1

4(1 + 1

3 + 1

5 +    ) = (ln n).

Since the lower bound on the eective resistance and hence the eective resistance goes

to innity, the escape probability goes to zero for the 2-dimensional lattice.

Three dimensions

In three dimensions, the resistance along any path to innity grows to innity but

the number of paths in parallel also grows to innity. It turns out there are a sucient

number of paths that re remains nite and thus there is a nonzero escape probability.

We will prove this now. First note that shorting any edge decreases the resistance, so

110

y

7

3

1

1

3

7

x

Figure 4.11: Paths in a 2-dimensional lattice obtained from the 3-dimensional construc-

tion applied in 2-dimensions.

we do not use shorting in this proof, since we seek to prove an upper bound on the

resistance. Instead we remove some edges, which increases their resistance to innity and

hence increases the eective resistance, giving an upper bound. To simplify things we

consider walks on a quadrant rather than the full grid. The resistance to innity derived

from only the quadrant is an upper bound on the resistance of the full grid.

The construction used in three dimensions is easier to explain rst in two dimensions,

see Figure 4.11. Draw dotted diagonal lines at x + y = 2n  1. Consider two paths

that start at the origin. One goes up and the other goes to the right. Each time a path

encounters a dotted diagonal line, split the path into two, one which goes right and the

other up. Where two paths cross, split the vertex into two, keeping the paths separate. By

a symmetry argument, splitting the vertex does not change the resistance of the network.

Remove all resistors except those on these paths. The resistance of the original network is

less than that of the tree produced by this process since removing a resistor is equivalent

to increasing its resistance to innity.

The distances between splits increase and are 1, 2, 4, etc. At each split the number

111

1

2

4

Figure 4.12: Paths obtained from 2-dimensional lattice. Distances between splits double

as do the number of parallel paths.

of paths in parallel doubles. See Figure 4.12. Thus, the resistance to innity in this two

dimensional example is

1

2

+

1

4

2 +

1

8

4 +    =

1

2

+

1

2

+

1

2

+    = .

In the analogous three dimensional construction, paths go up, to the right, and out of

the plane of the paper. The paths split three ways at planes given by x + y + z = 2n  1.

Each time the paths split the number of parallel segments triple. Segments of the paths

between splits are of length 1, 2, 4, etc. and the resistance of the segments are equal to

the lengths. The resistance out to innity for the tree is

1

3 + 1

92 + 1

274 +    = 1

3

(cid:0)1 + 2

3 + 4

9 +   (cid:1) = 1

3

= 1

1

1

2

3

The resistance of the three dimensional lattice is less. It is important to check that the

paths are edge-disjoint and so the tree is a subgraph of the lattice. Going to a subgraph is

equivalent to deleting edges which increases the resistance. That is why the resistance of

the lattice is less than that of the tree. Thus, in three dimensions the escape probability

is nonzero. The upper bound on re gives the lower bound

pescape = 1

2d

1

re

 1

6.

A lower bound on re gives an upper bound on pescape. To get the upper bound on

pescape, short all resistors on surfaces of boxes at distances 1, 2, 3,, etc. Then

re  1

6

This gives

(cid:2)1 + 1

9 + 1

25 +   (cid:3)  1.23

6  0.2

pescape = 1

2d

1

re

 5

6.

4.8 The Web as a Markov Chain

A modern application of random walks on directed graphs comes from trying to estab-

lish the importance of pages on the World Wide Web. Search Engines output an ordered

112

1

20.85i

pji

j

i

1

20.85i

0.15j

0.15i

i = 0.85jpji + 0.85

2 i

i = 1.48jpji

Figure 4.13: Impact on pagerank of adding a self loop

list of webpages in response to each search query. To do this, they have to solve two

problems at query time: (i) nd the set of all webpages containing the query term(s) and

(ii) rank the webpages and display them (or the top subset of them) in ranked order. (i)

is done by maintaining a reverse index which we do not discuss here. (ii) cannot be

done at query time since this would make the response too slow. So Search Engines rank

the entire set of webpages (in the billions) o-line and use that single ranking for all

queries. At query time, the webpages containing the query terms(s) are displayed in this

ranked order.

One way to do this ranking would be to take a random walk on the web viewed as a

directed graph (which we call the web graph) with an edge corresponding to each hyper-

text link and rank pages according to their stationary probability. Hypertext links are

one-way and the web graph may not be strongly connected. Indeed, for a node at the

bottom level there may be no out-edges. When the walk encounters this vertex the

walk disappears. Another diculty is that a vertex or a strongly connected component

with no in edges is never reached. One way to resolve these diculties is to introduce

a random restart condition. At each step, with some probability r, jump to a vertex se-

lected uniformly at random in the entire graph; with probability 1  r select an out-edge

at random from the current node and follow it. If a vertex has no out edges, the value

of r for that vertex is set to one. This makes the graph strongly connected so that the

stationary probabilities exist.

Pagerank

The pagerank of a vertex in a directed graph is the stationary probability of the vertex,

where we assume a positive restart probability of say r = 0.15. The restart ensures that

the graph is strongly connected. The pagerank of a page is the frequency with which the

page will be visited over a long period of time. If the pagerank is p, then the expected

time between visits or return time is 1/p. Notice that one can increase the pagerank of a

page by reducing the return time and this can be done by creating short cycles.

Consider a vertex i with a single edge in from vertex j and a single edge out. The

113

stationary probability  satises P = , and thus

Adding a self-loop at i, results in a new equation

i = jpji.

or

i = jpji +

1

2

i

i = 2 jpji.

Of course, j would have changed too, but ignoring this for now, pagerank is doubled by

the addition of a self-loop. Adding k self loops, results in the equation

i = jpji +

k

k + 1

i,

and again ignoring the change in j, we now have i = (k + 1)jpji. What prevents

one from increasing the pagerank of a page arbitrarily? The answer is the restart. We

neglected the 0.15 probability that is taken o for the random restart. With the restart

taken into account, the equation for i when there is no self-loop is

whereas, with k self-loops, the equation is

i = 0.85jpji

i = 0.85jpji + 0.85

k

k + 1

i.

Solving for i yields

0.85k + 0.85

0.15k + 1

which for k = 1 is i = 1.48jpji and in the limit as k   is i = 5.67jpji. Adding a

single loop only increases pagerank by a factor of 1.74.

jpji

i =

Relation to Hitting time

Recall the denition of hitting time hxy, which for two states x and y is the expected

time to reach y starting from x. Here, we deal with hy, the average time to hit y, starting

at a random node. Namely, hy = 1

x hxy, where the sum is taken over all n nodes x.

n

Hitting time hy is closely related to return time and thus to the reciprocal of page rank.

Return time is clearly less than the expected time until a restart plus hitting time. With

r as the restart value, this gives:

(cid:80)

Return time to y 

1

r

+ hy.

114

In the other direction, the fastest one could return would be if there were only paths of

length two (assume we remove all self-loops). A path of length two would be traversed

with at most probability (1  r)2. With probability r + (1  r) r = (2  r) r one restarts

and then hits v. Thus, the return time is at least 2 (1  r)2 + (2  r) r  (hitting time).

Combining these two bounds yields

2 (1  r)2 + (2  r) r(hitting time)  (return time) 

1

r

+ (hitting time) .

The relationship between return time and hitting time can be used to see if a vertex has

unusually high probability of short loops. However, there is no ecient way to compute

hitting time for all vertices as there is for return time. For a single vertex v, one can

compute hitting time by removing the edges out of the vertex v for which one is com-

puting hitting time and then run the pagerank algorithm for the new graph. The hitting

time for v is the reciprocal of the pagerank in the graph with the edges out of v removed.

Since computing hitting time for each vertex requires removal of a dierent set of edges,

the algorithm only gives the hitting time for one vertex at a time. Since one is probably

only interested in the hitting time of vertices with low hitting time, an alternative would

be to use a random walk to estimate the hitting time of low hitting time vertices.

Spam

Suppose one has a web page and would like to increase its pagerank by creating other

web pages with pointers to the original page. The abstract problem is the following. We

are given a directed graph G and a vertex v whose pagerank we want to increase. We may

add new vertices to the graph and edges from them to any vertices we want. We can also

add or delete edges from v. However, we cannot add or delete edges out of other vertices.

The pagerank of v is the stationary probability for vertex v with random restarts. If

we delete all existing edges out of v, create a new vertex u and edges (v, u) and (u, v),

then the pagerank will be increased since any time the random walk reaches v it will be

captured in the loop v  u  v. A search engine can counter this strategy by more

frequent random restarts.

A second method to increase pagerank would be to create a star consisting of the

vertex v at its center along with a large set of new vertices each with a directed edge to

v. These new vertices will sometimes be chosen as the target of the random restart and

hence the vertices increase the probability of the random walk reaching v. This second

method is countered by reducing the frequency of random restarts.

Notice that the rst technique of capturing the random walk increases pagerank but

does not eect hitting time. One can negate the impact on pagerank of someone capturing

the random walk by increasing the frequency of random restarts. The second technique

of creating a star increases pagerank due to random restarts and decreases hitting time.

115

One can check if the pagerank is high and hitting time is low in which case the pagerank

is likely to have been articially inated by the page capturing the walk with short cycles.

Personalized pagerank

In computing pagerank, one uses a restart probability, typically 0.15, in which at each

step, instead of taking a step in the graph, the walk goes to a vertex selected uniformly

at random. In personalized pagerank, instead of selecting a vertex uniformly at random,

one selects a vertex according to a personalized probability distribution. Often the distri-

bution has probability one for a single vertex and whenever the walk restarts it restarts

at that vertex. Note that this may make the graph disconnected.

Algorithm for computing personalized pagerank

First, consider the normal pagerank. Let  be the restart probability with which the

random walk jumps to an arbitrary vertex. With probability 1   the random walk

selects a vertex uniformly at random from the set of adjacent vertices. Let p be a row

vector denoting the pagerank and let A be the adjacency matrix with rows normalized to

sum to one. Then

or

p = 

n (1, 1, . . . , 1) + (1  ) pA

p[I  (1  )A] =



n

(1, 1, . . . , 1)

p = 

n (1, 1, . . . , 1) [I  (1  ) A]1.

Thus, in principle, p can be found by computing the inverse of [I  (1  )A]1. But

this is far from practical since for the whole web one would be dealing with matrices with

billions of rows and columns. A more practical procedure is to run the random walk and

observe using the basics of the power method in Chapter 3 that the process converges to

the solution p.

For the personalized pagerank, instead of restarting at an arbitrary vertex, the walk

restarts at a designated vertex. More generally, it may restart in some specied neighbor-

hood. Suppose the restart selects a vertex using the probability distribution s. Then, in

the above calculation replace the vector 1

n (1, 1, . . . , 1) by the vector s. Again, the compu-

tation could be done by a random walk. But, we wish to do the random walk calculation

for personalized pagerank quickly since it is to be performed repeatedly. With more care

this can be done, though we do not describe it here.

4.9 Bibliographic Notes

The material on the analogy between random walks on undirected graphs and electrical

networks is from [DS84] as is the material on random walks in Euclidean space. Addi-

116

tional material on Markov chains can be found in [MR95b], [MU05], and [per10]. For

material on Markov Chain Monte Carlo methods see [Jer98] and [Liu01].

The use of normalized conductance to prove convergence of Markov Chains is by

Sinclair and Jerrum, [SJ89] and Alon [Alo86]. A polynomial time bounded Markov chain

based method for estimating the volume of convex sets was developed by Dyer, Frieze and

Kannan [DFK91].

117

4.10 Exercises

Exercise 4.1 The Fundamental Theorem of Markov chains says that for a connected

Markov chain, the long-term average distribution a(t) converges to a stationary distribu-

tion. Does the t step distribution p(t) also converge for every connected Markov Chain?

Consider the following examples: (i) A two-state chain with p12 = p21 = 1. (ii) A three

state chain with p12 = p23 = p31 = 1 and the other pij = 0. Generalize these examples to

produce Markov Chains with many states.

Exercise 4.2 Does limt a(t)  a(t + 1) = 0 imply that a(t) converges to some value?

Hint: consider the average cumulative sum of the digits in the sequence 1021408116   

Exercise 4.3 What is the stationary probability for the following networks.

0.6

0.4 0.6

0.4 0.6

0.4 0.6

0.4

a

0.5

0

0.5

0

0.5

0

0.5

0

0.4

0.5

0.6

1

0.5

0.5

b

0.5

Exercise 4.4 A Markov chain is said to be symmetric if for all i and j, pij = pji. What

is the stationary distribution of a connected symmetric chain? Prove your answer.

Exercise 4.5 Prove |p  q|1 = 2 (cid:80)

(Proposition 4.4).

i(pi  qi)+ for probability distributions p and q,

Exercise 4.6 Let p(x), where x = (x1, x2, . . . , xd) xi  {0, 1}, be a multivariate probabil-

ity distribution. For d = 100, how would you estimate the marginal distribution

p(x1) =

(cid:88)

x2,...,xd

p(x1, x2, . . . , xd) ?

Exercise 4.7 Using the Metropolis-Hasting Algorithm create a Markov chain whose sta-

tionary probability is that given in the following table. Use the 3  3 lattice for the under-

lying graph.

x1x2

Prob

00

1/16

01

1/8

02

1/16

10

1/8

11

1/4

12

1/8

20

1/16

21

1/8

22

1/16

Exercise 4.8 Using Gibbs sampling create a 4  4 lattice where vertices in rows and

columns are cliques whose stationary probability is that given in the following table.

118

x/y

1

2

3

4

1

1

16

1

32

1

32

1

16

2

1

32

1

8

1

8

1

32

3

1

32

1

8

1

8

1

32

4

1

16

1

32

1

32

1

16

Note by symmetry there are only three types of vertices and only two types of rows or

columns.

Exercise 4.9 How would you integrate a high dimensional multivariate polynomial dis-

tribution over some convex region?

Exercise 4.10 Given a time-reversible Markov chain, modify the chain as follows. At

the current state, stay put (no move) with probability 1/2. With the other probability 1/2,

move as in the old chain. Show that the new chain has the same stationary distribution.

What happens to the convergence time in this modication?

Exercise 4.11 Let p be a probability vector (nonnegative components adding up to 1) on

the vertices of a connected graph which is suciently large that it cannot be stored in a

computer. Set pij (the transition probability from i to j) to pj for all i (cid:54)= j which are

adjacent in the graph. Show that the stationary probability vector is p. Is a random walk

an ecient way to sample according to a probability distribution that is close to p? Think,

for example, of the graph G being the n-dimensional hypercube with 2n vertices, and p as

the uniform distribution over those vertices.

6

3, 1

(cid:1). Repeat adding a self loop with probability 1

Exercise 4.12 Construct the edge probability for a three state Markov chain where each

pair of states is connected by an undirected edge so that the stationary probability is

(cid:0) 1

2, 1

Exercise 4.13 Consider a three state Markov chain with stationary probability (cid:0) 1

(cid:1).

Consider the Metropolis-Hastings algorithm with G the complete graph on these three

vertices. For each edge and each direction what is the expected probability that we would

actually make a move along the edge?

2 to the vertex with probability 1

2.

2, 1

3, 1

6

Exercise 4.14 Consider a distribution p over {0, 1}2 with p(00) = p(11) = 1

2 and p(01) =

p(10) = 0. Give a connected graph on {0, 1}2 that would be bad for running Metropolis-

Hastings and a graph that would be good for running Metropolis-Hastings. What would be

the problem with Gibbs sampling?

Exercise 4.15 Consider p(x) where x  {0, 1}100 such that p (0) = 1

for x (cid:54)= 0. How does Gibbs sampling behave?

2 and p (x) = 1/2

(21001)

119

Exercise 4.16 Given a connected graph G and an integer k how would you generate

connected subgraphs of G with k vertices with probability proportional to the number of

edges in the subgraph? A subgraph of G does not need to have all edges of G that join

vertices of the subgraph. The probabilities need not be exactly proportional to the number

of edges and you are not expected to prove your algorithm for this problem.

Exercise 4.17 Suppose one wishes to generate uniformly at random a regular, degree

three, undirected, not necessarily connected multi-graph with 1,000 vertices. A multi-

graph may have multiple edges between a pair of vertices and self loops. One decides to

do this by a Markov Chain Monte Carlo technique. In particular, consider a (very large)

network where each vertex corresponds to a regular degree three, 1,000 vertex multi-graph.

For edges, say that the vertices corresponding to two graphs are connected by an edge if

one graph can be obtained from the other by a ip of a pair of edges. In a ip, a pair of

edges (a, b) and (c, d) are replaced by (a, c) and (b, d).

1. Prove that the network whose vertices correspond to the desired graphs is connected.

That is, for any two 1000-vertex degree-3 multigraphs, it is possible to walk from

one to the other in this network.

2. Prove that the stationary probability of the random walk is uniform over all vertices.

3. Give an upper bound on the diameter of the network.

4. How would you modify the process if you wanted to uniformly generate connected

degree three multi-graphs?

In order to use a random walk to generate the graphs in a reasonable amount of time, the

random walk must rapidly converge to the stationary probability. Proving this is beyond

the material in this book.

Exercise 4.18 Construct, program, and execute an algorithm to estimate the volume of

a unit radius sphere in 20 dimensions by carrying out a random walk on a 20 dimensional

grid with 0.1 spacing.

Exercise 4.19 What is the mixing time for the undirected graphs

1. Two cliques connected by a single edge?

2. A graph consisting of an n vertex clique plus one additional vertex connected to one

vertex in the clique.

Exercise 4.20 What is the mixing time for

1. G(n, p) with p = log n

n ?

2. A circle with n vertices where at each vertex an edge has been added to another

vertex chosen at random. On average each vertex will have degree four, two circle

edges, and an edge from that vertex to a vertex chosen at random, and possible some

edges that are the ends of the random edges from other vertices.

120

Exercise 4.21 Find the (cid:15)-mixing time for a 2-dimensional lattice with n vertices in each

coordinate direction with a uniform probability distribution. To do this solve the following

problems.

1. The minimum number of edges leaving a set S of size greater than or equal to n2/4

is n.

2. The minimum number of edges leaving a set S of size less than or equal to n2/4 is



(cid:98)

S(cid:99).

3. Compute (S)

4. Compute 

5. Computer the (cid:15)-mixing time

Exercise 4.22 Find the (cid:15)-mixing time for a d-dimensional lattice with n vertices in each

coordinate direction with a uniform probability distribution. To do this, solve the following

problems.

1. Select a direction say x1 and push all elements of S in each column perpendicular

to x1 = 0 as close to x1 = 0 as possible. Prove that the number of edges leaving S

is at least as large as the number leaving the modied version of S.

2. Repeat step one for each direction. Argue that for a direction say x1, as x1 gets

larger a set in the perpendicular plane is contained in the previous set.

3. Optimize the arrangements of elements in the plane x1 = 0 and move elements from

farthest out plane in to make all planes the same shape as x1 = 0 except for some

leftover elements of S in the last plane. Argue that this does not increase the number

of edges out.

4. What congurations might we end up with?

5. Argue that for a given size, S has at least as many edges as the modied version of

S.

6. What is (S) for a modied form S?

7. What is  for a d-dimensional lattice?

8. What is the (cid:15)-mixing time?

Exercise 4.23

1. What is the set of possible harmonic functions on a connected graph if there are only

interior vertices and no boundary vertices that supply the boundary condition?

121

2. Let qx be the stationary probability of vertex x in a random walk on an undirected

graph where all edges at a vertex are equally likely and let dx be the degree of vertex

x. Show that qx

dx

is a harmonic function.

3. If there are multiple harmonic functions when there are no boundary conditions, why

is the stationary probability of a random walk on an undirected graph unique?

4. What is the stationary probability of a random walk on an undirected graph?

Exercise 4.24 In Section 4.5, given an electrical network, we dene an associated Markov

chain such that voltages and currents in the electrical network corresponded to properties

of the Markov chain. Can we go in the reverse order and for any Markov chain construct

the equivalent electrical network?

Exercise 4.25 What is the probability of reaching vertex 1 before vertex 5 when starting

a random walk at vertex 4 in each of the following graphs.

1.

2.

1

2

3

4

5

1

2

3

4

6

5

Exercise 4.26 Consider the electrical resistive network in Figure 4.14 consisting of ver-

tices connected by resistors. Kircho s law states that the currents at each vertex sum to

zero. Ohms law states that the voltage across a resistor equals the product of the resis-

tance times the current through it. Using these laws calculate the eective resistance of

the network.

Exercise 4.27 Consider the electrical network of Figure 4.15.

1. Set the voltage at a to one and at b to zero. What are the voltages at c and d?

2. What is the current in the edges a to c, a to d, c to d. c to b and d to b?

3. What is the eective resistance between a and b?

4. Convert the electrical network to a graph. What are the edge probabilities at each

vertex so that the probability of a walk starting at c (d) reaches a before b equals the

voltage at c (the voltage at d).?

122

R1

R3

i1

R2

i2

Figure 4.14: An electrical network of resistors.

a

R=1

R=2

c

d

R=2

R=1

b

R=1

Figure 4.15: An electrical network of resistors.

5. What is the probability of a walk starting at c reaching a before b? a walk starting

at d reaching a before b?

6. What is the net frequency that a walk from a to b goes through the edge from c to

d?

7. What is the probability that a random walk starting at a will return to a before

reaching b?

Exercise 4.28 Consider a graph corresponding to an electrical network with vertices a

and b. Prove directly that ce

must be less than or equal to one. We know that this is the

ca

escape probability and must be at most 1. But, for this exercise, do not use that fact.

Exercise 4.29 (Thomsons Principle) The energy dissipated by the resistance of edge xy

in an electrical network is given by i2

xyrxy. The total energy dissipation in the network

is E = 1

2 accounts for the fact that the dissipation in each edge is

2

counted twice in the summation. Show that the actual current distribution is the distribu-

tion satisfying Ohms law that minimizes energy dissipation.

xyrxy where the 1

i2

(cid:80)

x,y

123

u

v

u

v

u

v

Figure 4.16: Three graphs

(a)

(b)

(c)

1

1

1

2

2

2

3

3

3

4

4

4

Figure 4.17: Three graph

Exercise 4.30 (Rayleighs law) Prove that reducing the value of a resistor in a network

cannot increase the eective resistance. Prove that increasing the value of a resistor cannot

decrease the eective resistance. You may use Thomsons principle Exercise 4.29.

Exercise 4.31 What is the hitting time huv for two adjacent vertices on a cycle of length

n? What is the hitting time if the edge (u, v) is removed?

Exercise 4.32 What is the hitting time huv for the three graphs if Figure 4.16.

Exercise 4.33 Show that adding an edge can either increase or decrease hitting time by

calculating h24 for the three graphs in Figure 4.17.

Exercise 4.34 Consider the n vertex connected graph shown in Figure 4.18 consisting

of an edge (u, v) plus a connected graph on n  1 vertices and m edges. Prove that

huv = 2m + 1 where m is the number of edges in the n  1 vertex subgraph.

Exercise 4.35 Consider a random walk on a clique of size n. What is the expected

number of steps before a given vertex is reached?

124

n  1

vertices

m edges

u

v

Figure 4.18: A connected graph consisting of n  1 vertices and m edges along with a

single edge (u, v).

Exercise 4.36 What is the most general solution to the dierence equation t(i + 2) 

5t(i + 1) + 6t(i) = 0. How many boundary conditions do you need to make the solution

unique?

Exercise 4.37 Given the dierence equation akt(i + k) + ak1t(i + k  1) +    + a1t(i +

1)+a0t(i) = 0 the polynomial aktk +akitk1 +  +a1t+a0 = 0 is called the characteristic

polynomial.

1. If the equation has a set of r distinct roots, what is the most general form of the

solution?

2. If the roots of the characteristic polynomial are not distinct what is the most general

form of the solution?

3. What is the dimension of the solution space?

4. If the dierence equation is not homogeneous (i.e., the right hand side is not 0) and

f(i) is a specic solution to the nonhomogeneous dierence equation, what is the full

set of solutions to the nonhomogeneous dierence equation?

Exercise 4.38 Show that adding an edge to a graph can either increase or decrease com-

mute time.

Exercise 4.39 Consider the set of integers {1, 2, . . . , n}.

1. What is the expected number of draws with replacement until the integer 1 is drawn.

2. What is the expected number of draws with replacement so that every integer is

drawn?

Exercise 4.40 For each of the three graphs below what is the return time starting at

vertex A? Express your answer as a function of the number of vertices, n, and then

express it as a function of the number of edges m.

125

A B

n vertices

a

A

B

 n  2 

b

A

B

n  1

clique

c

Exercise 4.41 Suppose that the clique in Exercise 4.40 was replaced by an arbitrary graph

with m  1 edges. What would be the return time to A in terms of m, the total number of

edges.

Exercise 4.42 Suppose that the clique in Exercise 4.40 was replaed by an arbitrary graph

with m  d edges and there were d edges from A to the graph. What would be the expected

length of a random path starting at A and ending at A after returning to A exactly d

times.

Exercise 4.43 Given an undirected graph with a component consisting of a single edge

nd two eigenvalues of the Laplacian L = D  A where D is a diagonal matrix with vertex

degrees on the diagonal and A is the adjacency matrix of the graph.

Exercise 4.44 A researcher was interested in determining the importance of various

edges in an undirected graph. He computed the stationary probability for a random walk

on the graph and let pi be the probability of being at vertex i. If vertex i was of degree

di, the frequency that edge (i, j) was traversed from i to j would be 1

pi and the frequency

di

that the edge was traversed in the opposite direction would be 1

pj. Thus, he assigned an

dj

importance of

(cid:12)

(cid:12)

(cid:12)

1

di

pi  1

dj

pj

(cid:12)

(cid:12)

(cid:12) to the edge. What is wrong with his idea?

Exercise 4.45 Prove that two independent random walks starting at the origin on a two

dimensional lattice will eventually meet with probability one.

Exercise 4.46 Suppose two individuals are ipping balanced coins and each is keeping

tract of the number of heads minus the number of tails. At some time will both individuals

counts be the same?

Exercise 4.47 Consider the lattice in 2-dimensions. In each square add the two diagonal

edges. What is the escape probability for the resulting graph?

Exercise 4.48 Determine by simulation the escape probability for the 3-dimensional lat-

tice.

Exercise 4.49 What is the escape probability for a random walk starting at the root of

an innite binary tree?

126

E

D

A

C

B

D

A

C

B

Figure 4.19: An undirected and a directed graph.

Exercise 4.50 Consider a random walk on the positive half line, that is the integers

0, 1, 2, . . .. At the origin, always move right one step. At all other integers move right

with probability 2/3 and left with probability 1/3. What is the escape probability?

Exercise 4.51 Consider the graphs in Figure 4.19. Calculate the stationary distribution

for a random walk on each graph and the ow through each edge. What condition holds

on the ow through edges in the undirected graph? In the directed graph?

Exercise 4.52 Create a random directed graph with 200 vertices and roughly eight edges

per vertex. Add k new vertices and calculate the pagerank with and without directed edges

from the k added vertices to vertex 1. How much does adding the k edges change the

pagerank of vertices for various values of k and restart frequency? How much does adding

a loop at vertex 1 change the pagerank? To do the experiment carefully one needs to

consider the pagerank of a vertex to which the star is attached. If it has low pagerank its

page rank is likely to increase a lot.

Exercise 4.53 Repeat the experiment in Exercise 4.52 for hitting time.

Exercise 4.54 Search engines ignore self loops in calculating pagerank. Thus, to increase

pagerank one needs to resort to loops of length two. By how much can you increase the

page rank of a page by adding a number of loops of length two?

Exercise 4.55 Number the vertices of a graph {1, 2, . . . , n}. Dene hitting time to be the

expected time from vertex 1. In (2) assume that the vertices in the cycle are sequentially

numbered.

1. What is the hitting time for a vertex in a complete directed graph with self loops?

2. What is the hitting time for a vertex in a directed cycle with n vertices?

127

Create exercise relating strongly connected and full rank

Full rank implies strongly connected.

Strongly connected does not necessarily imply full rank









0 0 1

0 0 1

1 1 0

Is graph aperiodic i 1 > 2?

Exercise 4.56 Using a web browser bring up a web page and look at the source html.

How would you extract the urls of all hyperlinks on the page if you were doing a crawl

of the web? With Internet Explorer click on source under view to access the html

representation of the web page. With Firefox click on page source under view.

Exercise 4.57 Sketch an algorithm to crawl the World Wide Web. There is a time delay

between the time you seek a page and the time you get it. Thus, you cannot wait until the

page arrives before starting another fetch. There are conventions that must be obeyed if

one were to actually do a search. Sites specify information as to how long or which les

can be searched. Do not attempt an actual search without guidance from a knowledgeable

person.

128

5 Machine Learning

5.1 Introduction

Machine learning algorithms are general purpose tools for generalizing from data.

They have proven to be able to solve problems from many disciplines without detailed

domain-specic knowledge. To date they have been highly successful for a wide range of

tasks including computer vision, speech recognition, document classication, automated

driving, computational science, and decision support.

The core problem. A core problem underlying many machine learning applications

is learning a good classication rule from labeled data. This problem consists of a do-

main of interest X , called the instance space, such as the set of email messages or patient

records, and a classication task, such as classifying email messages into spam versus

non-spam or determining which patients will respond well to a given medical treatment.

We will typically assume our instance space X = {0, 1}d or X = Rd, corresponding to

data that is described by d Boolean or real-valued features. Features for email messages

could be the presence or absence of various types of words, and features for patient records

could be the results of various medical tests. To perform the learning task, our learning

algorithm is given a set S of labeled training examples, which are points in X along with

their correct classication. This training data could be a collection of email messages,

each labeled as spam or not spam, or a collection of patients, each labeled by whether

or not they responded well to the given medical treatment. Our algorithm then aims to

use the training examples to produce a classication rule that will perform well over new

data, i.e., new points in X . A key feature of machine learning, which distinguishes it

from other algorithmic tasks, is that our goal is generalization: to use one set of data in

order to perform well on new data we have not seen yet. We focus on binary classication

where items in the domain of interest are classied into two categories (called the positive

class and the negative class), as in the medical and spam-detection examples above, but

nearly all the techniques described here will also apply to multi-way classication.

How to learn. A high-level approach that many algorithms we discuss will follow is

to try to nd a simple rule with good performance on the training data. For instance,

in the case of classifying email messages, we might nd a set of highly indicative words

such that every spam email in the training data has at least one of these words and none

of the non-spam emails has any of them; in this case, the rule if the message has any of

these words then it is spam, else it is not would be a simple rule that performs well on

the training data. Or, we might nd a way of weighting words with positive and negative

weights such that the total weighted sum of words in the email message is positive on the

spam emails in the training data, and negative on the non-spam emails. We will then

argue that so long as the training data is representative of what future data will look

like, we can be condent that any suciently simple rule that performs well on the

training data will also perform well on future data. To make this into a formal math-

129

margin

Figure 5.1: Margin of a linear separator.

ematical statement, we need to be precise about what we mean by simple as well as

what it means for training data to be representative of future data. In fact, we will see

several notions of complexity, including bit-counting and VC-dimension, that will allow

us to make mathematical statements of this form. These statements can be viewed as

formalizing the intuitive philosophical notion of Occams razor.

5.2 The Perceptron algorithm

To help ground our discussion, we begin by describing a specic interesting learning

algorithm, the Perceptron algorithm, for the problem of assigning positive and negative

weights to features (such as words) so that each positive example has a positive sum of

feature weights and each negative example has a negative sum of feature weights.

More specically, the Perceptron algorithm is an ecient algorithm for nding a lin-

ear separator in d-dimensional space, with a running time that depends on the margin

of separation of the data. We are given as input a set S of training examples (points in

d-dimensional space), each labeled as positive or negative, and our assumption is that

there exists a vector w such that for each positive example x  S we have xT w  1

and for each negative example x  S we have xT w  1. Note that the quantity

xT w/|w| is the distance of the point x to the hyperplane xT w = 0. Thus, we can view

our assumption as stating that there exists a linear separator through the origin with all

positive examples on one side, all negative examples on the other side, and all examples

at distance at least  = 1/|w| from the separator. This quantity  is called the margin

of separation (see Figure 5.1).

The goal of the Perceptron algorithm is to nd a vector w such that xT w > 0 for all

positive examples x  S, and xT w < 0 for all negative examples x  S. It does so via

130

the following update rule:

The Perceptron Algorithm: Start with the all-zeroes weight vector w = 0. Then repeat

the following until xT w has the correct sign for all x  S (positive for positive examples and

negative for negative examples):

1. Let x  S be an example for which xT w does not have the correct sign.

2. Update as follows:

(a) If x is a positive example, let w  w + x.

(b) If x is a negative example, let w  w  x.

While simple, the Perceptron algorithm indeed will nd a linear separator whenever

one exists, making at most (R/)2 updates where R = maxxS |x|. Thus, if there exists

a hyperplane through the origin that correctly separates the positive examples from the

negative examples by a large margin relative to the radius of the smallest ball enclosing

the data, then the total number of updates will be small.

Theorem 5.1 If there exists a vector w such that xT w  1 for all positive examples

x  S and xT w  1 for all negative examples x  S (i.e., a linear separator of margin

 = 1/|w|), then the number of updates made by the Perceptron algorithm is at most

R2|w|2, where R = maxxS |x|.

To get a feel for this bound, notice that if we multiply all entries in all the x  S by

100, we can divide all entries in w by 100 and it will still satisfy the ifcondition. So

the bound is invariant to this kind of scaling, i.e., to our units of measurement.

Proof of Theorem 5.1: Fix some w satisfying the if condition of the theorem. We

will keep track of two quantities, wT w and |w|2. First of all, each time we make an

update, wT w increases by at least 1. That is because if x is a positive example, then

(w + x)T w = wT w + xT w  wT w + 1,

by denition of w. Similarly, if x is a negative example, then

(w  x)T w = wT w  xT w  wT w + 1.

Next, on each update, we claim that |w|2 increases by at most R2. Let us rst consider

updates on positive examples. If we update on a positive example x then we have

(w + x)T (w + x) = |w|2 + 2xT w + |x|2  |w|2 + |x|2  |w|2 + R2,

where the middle inequality comes from the fact that we only perform an update on a

positive example when xT w  0. Similarly, if we update on a negative example x then

we have

(w  x)T (w  x) = |w|2  2xT w + |x|2  |w|2 + |x|2  |w|2 + R2.

131

Note that it is important here that we only update on examples for which xT w has the

incorrect sign.



So, if we make M updates, then wT w  M , and |w|2  M R2, or equivalently,

M . Finally, we use the fact that wT w/|w|  |w| which is just saying that

|w|  R

the projection of w in the direction of w cannot be larger than the length of w. This

gives us:



M/|w|  R



M

M  R|w|

M  R2|w|2

as desired.

What if there is no w that perfectly separates the positive and negative examples? In

Section 5.8 we will address this in the context of an online learning model, and we will see

that the Perceptron algorithm enjoys strong guarantees even if the best w is not quite

perfect, as a function of a quantity called the hinge loss of w.

In the next section, we consider a related issue. Suppose the positive and negative

examples are not linearly separable (there is no hyperplane with the positives on one side

and the negatives on the other) but they are separable by some other simple curve such

as a circle. In that case, we can use a technique known as kernel functions.

5.3 Kernel Functions

Suppose that instead of a linear separator decision boundary, the boundary between im-

portant emails and unimportant emails looks more like a circle, for example as in Figure

5.2.

A powerful idea for addressing situations like this is to use what are called kernel

functions, or sometimes the kernel trick. Here is the idea. Suppose you have a function

K, called a kernel, over pairs of data points such that for some function  : Rd  RN ,

where perhaps N (cid:29) d, we have K(x, x(cid:48)) = (x)T (x(cid:48)).

In that case, if we can write

the Perceptron algorithm so that it only interacts with the data via dot-products, and

then replace every dot-product with an invocation of K, then we can act as if we had

performed the function  explicitly without having to actually compute .

For example, consider K(x, x(cid:48)) = (1 + xT x(cid:48))k for some integer k  1. It turns out this

corresponds to a mapping  into a space of dimension N  dk. For example, in the case

d = 2, k = 2 we have (using xi to denote the ith coordinate of x):

K(x, x(cid:48)) = (1 + x1x(cid:48)

= 1 + 2x1x(cid:48)

= (x)T (x(cid:48))

2)2

1 + x2x(cid:48)

2 + x2

1 + 2x2x(cid:48)

1x(cid:48)2

1 + 2x1x2x(cid:48)

1x(cid:48)

2 + x2

2x(cid:48)2

2

132

y



x

Figure 5.2: Data that is not linearly separable in the input space R2 but that is linearly

2), corresponding to the

separable in the -space, (x) = (1,

kernel function K(x, x(cid:48)) = (1 + x1x(cid:48)

2x1x2, x2

2x2, x2

1,

2x1,

2)2.

1 + x2x(cid:48)













2x2, x2

1,

2x1,

for (x) = (1,

2). Notice also that a linear separator in this

space could correspond to a more complicated decision boundary such as an ellipse in

the original space. For instance, the hyperplane (x)T w = 0 for w = (4, 0, 0, 1, 0, 1)

corresponds to the circle x2

2 = 4 in the original space, such as in Figure 5.2.

1 + x2

2x1x2, x2

The point of this is that if in the higher-dimensional -space there is a w such

that the bound of Theorem 5.1 is small, then the algorithm will halt after not too many

updates (and later we will see that under reasonable assumptions on the data, this implies

we can be condent in its ability to perform well on new data as well). But the nice thing

is we didnt have to computationally perform the mapping !

So, how can we view the Perceptron algorithm as only interacting with data via dot-

products? Notice that w is always a linear combination of data points. For example, if

we made updates on examples x1, x2, and x5, and these examples were positive, positive,

and negative respectively, we would have w = x1 + x2  x5. So, if we keep track of w

this way, then to classify a new example x, we can write xT w = xT x1 + xT x2  xT x5.

So if we just replace each of these dot-products with K, we are running the algorithm

as if we had explicitly performed the  mapping. This is called kernelizing the algorithm.

Many dierent pairwise functions on examples are legal kernel functions. One easy

way to create a kernel function is by combining other kernel functions together, via the

following theorem.

Theorem 5.2 Suppose K1 and K2 are kernel functions. Then

1. For any constant c  0, cK1 is a legal kernel. In fact, for any scalar function f ,

the function K3(x, x(cid:48)) = f (x)f (x(cid:48))K1(x, x(cid:48)) is a legal kernel.

2. The sum K1 + K2, is a legal kernel.

133

3. The product, K1K2, is a legal kernel.

You will prove Theorem 5.2 in Exercise 5.9. Notice that this immediately implies that the

function K(x, x(cid:48)) = (1 + xT x(cid:48))k is a legal kernel by using the fact that K1(x, x(cid:48)) = 1 is a

legal kernel, K2(x, x(cid:48)) = xT x(cid:48) is a legal kernel, then adding them, and then multiplying

that by itself k times. Another popular kernel is the Gaussian kernel, dened as:

K(x, x(cid:48)) = ec|xx(cid:48)|2.

If we think of a kernel as a measure of similarity, then this kernel denes the similarity

between two data objects as a quantity that decreases exponentially with the squared

distance between them. The Gaussian kernel can be shown to be a true kernel func-

tion by rst writing it as f (x)f (x(cid:48))e2cxT x(cid:48) for f (x) = ec|x|2 and then taking the Taylor

expansion of e2cxT x(cid:48), applying the rules in Theorem 5.2. Technically, this last step re-

quires considering countably innitely many applications of the rules and allowing for

innite-dimensional vector spaces.

5.4 Generalizing to New Data

So far, we have focused on the problem of nding a classication rule that performs well

on a given set S of training data. But what we really want our classication rule to do

is to perform well on new data we have not seen yet. To make guarantees of this form,

we need some assumption that our training data is somehow representative of what new

data will look like; formally, we will assume they are drawn from the same probability

distribution. Additionally, we will see that we will want our algorithms classication

rule to be simple in some way. Together, these two conditions will allow us to make

generalization guarantees: guarantees on the ability of our learned classication rule to

perform well on new unseen data.

Formalizing the problem. To formalize the learning problem, assume there is some

probability distribution D over the instance space X , such that (a) our training set S con-

sists of points drawn independently at random from D, and (b) our objective is to predict

well on new points that are also drawn from D. This is the sense in which we assume that

our training data is representative of future data. Let c, called the target concept, denote

the subset of X corresponding to the positive class for the binary classication we are

aiming to make. For example, c would correspond to the set of all patients who respond

well to a given treatment in a medical scenario, or it could correspond to the set of all

spam emails in a spam-detection scenario. So, each point in our training set S is labeled

according to whether or not it belongs to c and our goal is to produce a set h  X , called

our hypothesis, which is close to c with respect to distribution D. The true error of h is

errD(h) = Prob(h(cid:52)c) where (cid:52) denotes symmetric dierence, and probability mass is

according to D. In other words, the true error of h is the probability it incorrectly clas-

sies a data point drawn at random from D. Our goal is to produce h of low true error.

The training error of h, denoted errS(h), is the fraction of points in S on which h and

134

c disagree. That is, errS(h) = |S  (h(cid:52)c)|/|S|. Training error is also called empirical

error. Note that even though S is assumed to consist of points randomly drawn from D,

it is possible for a hypothesis h to have low training error or even to completely agree with

c over the training sample, and yet have high true error. This is called overtting the

training data. For instance, a hypothesis h that simply consists of listing the positive ex-

amples in S, which is equivalent to a rule that memorizes the training sample and predicts

positive on an example if and only if it already appeared positively in the training sample,

would have zero training error. However, this hypothesis likely would have high true error

and therefore would be highly overtting the training data. More generally, overtting is

a concern because algorithms will typically be optimizing over the training sample. To

design and analyze algorithms for learning, we will have to address the issue of overtting.

To analyze overtting, we introduce the notion of an hypothesis class, also called a

concept class or set system. An hypothesis class H over X is a collection of subsets of

X , called hypotheses. For instance, the class of intervals over X = R is the collection

{[a, b]|a  b}. The class of linear separators over X = Rd is the collection

{{x  Rd|w  x  w0}|w  Rd, w0  R};

that is, it is the collection of all sets in Rd that are linearly separable from their comple-

ment. In the case that X is the set of 4 points in the plane {(1, 1), (1, 1), (1, 1), (1, 1)},

the class of linear separators contains 14 of the 24 = 16 possible subsets of X .17 Given

an hypothesis class H and training set S, what we typically aim to do algorithmically

is to nd the hypothesis in H that most closely agrees with c over S. For example, we

saw that the Perceptron algorithm will nd a linear separator that agrees with the target

function over S so long as S is linearly separable. To address overtting, we argue that if

S is large enough compared to some property of H, then with high probability all h  H

have their training error close to their true error, so that if we nd a hypothesis whose

training error is low, we can be condent its true error will be low as well.

Before giving our rst result of this form, we note that it will often be convenient to

associate each hypotheses with its {1, 1}-valued indicator function

h(x) =

(cid:26) 1 x  h

1 x (cid:54) h

In this notation the true error of h is errD(h) = ProbxD[h(x) (cid:54)= c(x)] and the training

error is errS(h) = ProbxS[h(x) (cid:54)= c(x)].

5.5 Overtting and Uniform Convergence

We now present two generalization guarantees that explain how one can guard against

overtting. To keep things simple, we assume our hypothesis class H is nite. Later, we

17The only two subsets that are not in the class are the sets {(1, 1), (1, 1)} and {(1, 1), (1, 1)}.

135

will see how to extend these results to innite classes as well. Given a class of hypotheses

H, the rst result states that for any given (cid:15) greater than zero, so long as the training

data set is large compared to 1

(cid:15) ln(|H|), it is unlikely any hypothesis h  H will have zero

training error but have true error greater than (cid:15). This means that with high probability,

any hypothesis that our algorithms nds that agrees with the target hypothesis on the

training data will have low true error. The second result states that if the training data

set is large compared to 1

(cid:15)2 ln(|H|), then it is unlikely that the training error and true

error will dier by more than (cid:15) for any hypothesis in H. This means that if we nd an

hypothesis in H whose training error is low, we can be condent its true error will be low

as well, even if its training error is not zero.

The basic idea is the following. If we consider some h with large true error, and we

select an element x  X at random according to D, there is a reasonable chance that

x will belong to the symmetric dierence h(cid:52)c.

If we select a large enough training

sample S with each point drawn independently from X according to D, the chance that

S is completely disjoint from h(cid:52)c will be incredibly small. This is just for a single

hypothesis h but we can now apply the union bound over all h  H of large true error,

when H is nite. We formalize this below.

Theorem 5.3 Let H be an hypothesis class and let (cid:15) and  be greater than zero. If a

training set S of size

n 

(cid:0) ln |H| + ln(1/)(cid:1),

1

(cid:15)

is drawn from distribution D, then with probability greater than or equal to 1   every

h in H with true error errD(h)  (cid:15) has training error errS(h) > 0. Equivalently, with

probability greater than or equal to 1  , every h  H with training error zero has true

error less than (cid:15).

Proof: Let h1, h2, . . . be the hypotheses in H with true error greater than or equal to (cid:15).

These are the hypotheses that we dont want to output. Consider drawing the sample S

of size n and let Ai be the event that hi is consistent with S. Since every hi has true error

greater than or equal to (cid:15)

Prob(Ai)  (1  (cid:15))n.

In other words, if we x hi and draw a sample S of size n, the chance that hi makes no

mistakes on S is at most the probability that a coin of bias (cid:15) comes up tails n times in a

row, which is (1  (cid:15))n. By the union bound over all i we have

Prob (iAi)  |H|(1  (cid:15))n.

Using the fact that (1  (cid:15))  e(cid:15), the probability that any hypothesis in H with true error

greater than or equal to (cid:15) has training error zero is at most |H|e(cid:15)n. Replacing n by the

sample size bound from the theorem statement, this is at most |H|e ln |H|ln(1/) =  as

desired.

136

Spam

(cid:125)(cid:124)

Not spam

(cid:125)(cid:124)

(cid:122)

(cid:122)

x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x16

(cid:123)

(cid:123)

emails

0

0

0

1

0

0

0

0

0

0



0

(cid:108)

0



0

1

0

0

1

1



1

(cid:108)

1



1

1



1

(cid:108)

0



1

1

1

0

1

1

1

1

target concept

hypothesis hi

Figure 5.3: The hypothesis hi disagrees with the truth in one quarter of the emails. Thus

with a training set |S|, the probability that the hypothesis will survive is (1  0.25)|S|

The conclusion of Theorem 5.3 is sometimes called a PAC-learning guarantee since

it states that if we can nd an h  H consistent with the sample, then this h is Probably

Approximately Correct.

Theorem 5.3 addressed the case where there exists a hypothesis in H with zero train-

ing error. What if the best hi in H has 5% error on S? Can we still be condent that its

true error is low, say at most 10%? For this, we want an analog of Theorem 5.3 that says

for a suciently large training set S, every hi  H has training error within (cid:15) of the

true error with high probability. Such a statement is called uniform convergence because

we are asking that the training set errors converge to their true errors uniformly over all

sets in H. To see intuitively why such a statement should be true for suciently large

S and a single hypothesis hi, consider two strings that dier in 10% of the positions and

randomly select a large sample of positions. The number of positions that dier in the

sample will be close to 10%.

To prove uniform convergence bounds, we use a tail inequality for sums of independent

Bernoulli random variables (i.e., coin tosses). The following is particularly convenient and

is a variation on the Cherno bounds in Section 12.6.1 of the appendix.

Theorem 5.4 (Hoeding bounds) Let x1, x2, . . . , xn be independent {0, 1}-valued ran-

dom variables with Prob(xi = 1) = p. Let s = (cid:80)

i xi (equivalently, ip n coins of bias p

and let s be the total number of heads). For any 0    1,

Prob(s/n > p + )  e2n2

Prob(s/n < p  )  e2n2.

Theorem 5.4 implies the following uniform convergence analog of Theorem 5.3.

Theorem 5.5 (Uniform convergence) Let H be a hypothesis class and let (cid:15) and  be

greater than zero. If a training set S of size

n 

1

2(cid:15)2

(cid:0) ln |H| + ln(2/)(cid:1),

137

is drawn from distribution D, then with probability greater than or equal to 1  , every h

in H satises |errS(h)  errD(h)|  (cid:15).

Proof: First, x some h  H and let xj be the indicator random variable for the event

that h makes a mistake on the jth example in S. The xj are independent {0, 1} random

variables and the probability that xi equals 1 is the true error of h, and the fraction of the

xjs equal to 1 is exactly the training error of h. Therefore, Hoeding bounds guarantee

that the probability of the event Ah that |errD(h)  errS(h)| > (cid:15) is less than or equal to

2e2n(cid:15)2. Applying the union bound to the events Ah over all h  H, the probability that

there exists an h  H with the dierence between true error and empirical error greater

than (cid:15) is less than or equal to 2|H|e2n(cid:15)2. Using the value of n from the theorem statement,

the right-hand-side of the above inequality is at most  as desired.

Theorem 5.5 justies the approach of optimizing over our training sample S even if we

are not able to nd a rule of zero training error. If our training set S is suciently large,

with high probability, good performance on S will translate to good performance on D.

Note that Theorems 5.3 and 5.5 require |H| to be nite in order to be meaningful.

The notion of growth functions and VC-dimension in Section 5.11 extend Theorem 5.5 to

certain innite hypothesis classes.

5.6 Illustrative Examples and Occams Razor

We now present some examples to illustrate the use of Theorem 5.3 and 5.5 and also

use these theorems to give a formal connection to the notion of Occams razor.

5.6.1 Learning Disjunctions

Consider the instance space X = {0, 1}d and suppose we believe that the target concept

can be represented by a disjunction (an OR) over features, such as c = {x|x1 = 1  x4 =

1  x8 = 1}, or more succinctly, c = x1  x4  x8. For example, if we are trying to predict

whether an email message is spam or not, and our features correspond to the presence

or absence of dierent possible indicators of spam-ness, then this would correspond to

the belief that there is some subset of these indicators such that every spam email has at

least one of them and every non-spam email has none of them. Formally, let H denote

the class of disjunctions, and notice that |H| = 2d. So, by Theorem 5.3, it suces to nd

a consistent disjunction over a sample S of size

|S| =

1

(cid:15)

(cid:0)d ln(2) + ln(1/)(cid:1).

How can we eciently nd a consistent disjunction when one exists? Here is a simple

algorithm.

138

Simple Disjunction Learner: Given sample S, discard all features that are set to 1 in

any negative example in S. Output the concept h that is the OR of all features that remain.

Lemma 5.6 The Simple Disjunction Learner produces a disjunction h that is consis-

tent with the sample S (i.e., with errS(h) = 0) whenever the target concept is indeed a

disjunction.

Proof: Suppose target concept c is a disjunction. Then for any xi that is listed in c,

xi will not be set to 1 in any negative example by denition of an OR. Therefore, h will

include xi as well. Since h contains all variables listed in c, this ensures that h will

correctly predict positive on all positive examples in S. Furthermore, h will correctly

predict negative on all negative examples in S since by design all features set to 1 in any

negative example were discarded. Therefore, h is correct on all examples in S.

Thus, combining Lemma 5.6 with Theorem 5.3, we have an ecient algorithm for

PAC-learning the class of disjunctions.

5.6.2 Occams Razor

Occams razor is the notion, stated by William of Occam around AD 1320, that in general

one should prefer simpler explanations over more complicated ones.18 Why should one

do this, and can we make a formal claim about why this is a good idea? What if each of

us disagrees about precisely which explanations are simpler than others? It turns out we

can use Theorem 5.3 to make a mathematical statement of Occams razor that addresses

these issues.

First, what do we mean by a rule being simple? Lets assume that each of us has

some way of describing rules, using bits (since we are computer scientists). The methods,

also called description languages, used by each of us may be dierent, but one fact we can

say for certain is that in any given description language, there are at most 2b rules that

can be described using fewer than b bits (because 1 + 2 + 4 + . . . + 2b1 < 2b). Therefore,

by setting H to be the set of all rules that can be described in fewer than b bits and

plugging into Theorem 5.3, we have the following:

Theorem 5.7 (Occams razor) Fix any description language, and consider a training

sample S drawn from distribution D. With probability at least 1  , any rule h with

errS(h) = 0 that can be described using fewer than b bits will have errD(h)  (cid:15) for

|S| = 1

(cid:15) [b ln(2) + ln(1/)]. Equivalently, with probability at least 1  , all rules with

errS(h) = 0 that can be described in fewer than b bits will have errD(h)  b ln(2)+ln(1/)

.

|S|

For example, using the fact that ln(2) < 1 and ignoring the low-order ln(1/) term, this

means that if the number of bits it takes to write down a rule consistent with the training

data is at most 10% of the number of data points in our sample, then we can be condent

18The statement more explicitly was that Entities should not be multiplied unnecessarily.

139

Figure 5.4: A decision tree with three internal nodes and four leaves. This tree corre-

sponds to the Boolean function x1 x2  x1x2x3  x2 x3.

it will have error at most 10% with respect to D. What is perhaps surprising about this

theorem is that it means that we can each have dierent ways of describing rules and yet

all use Occams razor. Note that the theorem does not say that complicated rules are

necessarily bad, or even that given two rules consistent with the data that the complicated

rule is necessarily worse. What it does say is that Occams razor is a good policy in that

simple rules are unlikely to fool us since there are just not that many simple rules.

5.6.3 Application: Learning Decision Trees

One popular practical method for machine learning is to learn a decision tree; see Figure

5.4. While nding the smallest decision tree that ts a given training sample S is NP-

hard, there are a number of heuristics that are used in practice.19 Suppose we run such

a heuristic on a training set S and it outputs a tree with k nodes. Such a tree can be

described using O(k log d) bits: log2(d) bits to give the index of the feature in the root,

O(1) bits to indicate for each child if it is a leaf and if so what label it should have, and

then O(kL log d) and O(kR log d) bits respectively to describe the left and right subtrees,

where kL is the number of nodes in the left subtree and kR is the number of nodes in the

right subtree. So, by Theorem 5.7, we can be condent the true error is low if we can

produce a consistent tree with fewer than (cid:15)|S|/ log(d) nodes.

19For instance, one popular heuristic, called ID3, selects the feature to put inside any given node v

by choosing the feature of largest information gain, a measure of how much it is directly improving

prediction. Formally, using Sv to denote the set of examples in S that reach node v, and supposing that

feature xi partitions Sv into S0

v (the examples in Sv with xi = 0 and xi = 1, respectively), the

information gain of xi is dened as: Ent(Sv)  [ |S0

v|

v )]. Here, Ent(S(cid:48)) is the binary

|Sv| Ent(S0

entropy of the label proportions in set S(cid:48); that is, if a p fraction of the examples in S(cid:48) are positive, then

Ent(S(cid:48)) = p log2(1/p) + (1  p) log2(1/(1  p)), dening 0 log2(0) = 0. This then continues until all leaves

are purethey have only positive or only negative examples.

v ) + |S1

v|

|Sv| Ent(S1

v and S1

140

5.7 Regularization: Penalizing Complexity

Theorems 5.5 and 5.7 suggest the following idea. Suppose that there is no simple rule

that is perfectly consistent with the training data, but we notice there are very simple

rules with training error 20%, say, and then some more complex rules with training error

10%, and so on. In this case, perhaps we should optimize some combination of training er-

ror and simplicity. This is the notion of regularization, also called complexity penalization.

Specically, a regularizer is a penalty term that penalizes more complex hypotheses.

Given our theorems so far, a natural measure of complexity of a hypothesis is the number

of bits we need to write it down.20 Consider now xing some description language, and let

Hi denote those hypotheses that can be described in i bits in this language, so |Hi|  2i.

Let i = /2i. Rearranging the bound of Theorem 5.5, we know that with probability at

. Now, applying the

least 1  i, all h  Hi satisfy errD(h)  errS(h) +

union bound over all i, using the fact that 1 + 2 + 3 + . . . = , and also the fact that

ln(|Hi|) + ln(2/i)  i ln(4) + ln(2/), gives the following corollary.

(cid:113) ln(|Hi|)+ln(2/i)

2|S|

Corollary 5.8 Fix any description language, and consider a training sample S drawn

from distribution D. With probability greater than or equal to 1  , all hypotheses h

satisfy

errD(h)  errS(h) +

(cid:115)

size(h) ln(4) + ln(2/)

2|S|

where size(h) denotes the number of bits needed to describe h in the given language.

It tells us that rather than

Corollary 5.8 gives us the tradeo we were looking for.

searching for a rule of low training error, we instead may want to search for a rule with

a low right-hand-side in the displayed formula. If we can nd one for which this quantity

is small, we can be condent true error will be low as well.

5.8 Online Learning

So far we have been considering what is often called the batch learning scenario. You are

given a batch of datathe training sample Sand your goal is to use it to produce

a hypothesis h that will have low error on new data, under the assumption that both S

and the new data are sampled from some xed distribution D. We now switch to the

more challenging online learning scenario where we remove the assumption that data is

sampled from a xed probability distribution, or from any probabilistic process at all.

Specically, the online learning scenario proceeds as follows. At each time t = 1, 2, . . .,

two events occur:

20Later we will see support vector machines that use a regularizer for linear separators based on the

margin of separation of data.

141

1. The algorithm is presented with an arbitrary example xt  X and is asked to make

a prediction (cid:96)t of its label.

2. The algorithm is told the true label of the example c(xt) and is charged for a

mistake if c(xt) (cid:54)= (cid:96)t.

The goal of the learning algorithm is to make as few mistakes as possible in total. For

example, consider an email classier that when a new email message arrives must classify

it as important or it can wait. The user then looks at the email and informs the

algorithm if it was incorrect. We might not want to model email messages as independent

random objects from a xed probability distribution, because they often are replies to

previous emails and build on each other. Thus, the online learning model would be more

appropriate than the batch model for this setting.

Intuitively, the online learning model is harder than the batch model because we have

removed the requirement that our data consists of independent draws from a xed proba-

bility distribution. Indeed, we will see shortly that any algorithm with good performance

in the online model can be converted to an algorithm with good performance in the batch

model. Nonetheless, the online model can sometimes be a cleaner model for design and

analysis of algorithms.

5.8.1 An Example: Learning Disjunctions

As a simple example, lets revisit the problem of learning disjunctions in the online model.

We can solve this problem by starting with a hypothesis h = x1  x2  . . .  xd and using

it for prediction. We will maintain the invariant that every variable in the target disjunc-

tion is also in our hypothesis, which is clearly true at the start. This ensures that the

only mistakes possible are on examples x for which h(x) is positive but c(x) is negative.

When such a mistake occurs, we simply remove from h any variable set to 1 in x. Since

such variables cannot be in the target function (since x was negative), we maintain our

invariant and remove at least one variable from h. This implies that the algorithm makes

at most d mistakes total on any series of examples consistent with a disjunction.

In fact, we can show this bound is tight by showing that no deterministic algorithm

can guarantee to make fewer than d mistakes.

Theorem 5.9 For any deterministic algorithm A there exists a sequence of examples 

and disjunction c such that A makes at least d mistakes on sequence  labeled by c.

Proof: Let  be the sequence e1, e2, . . . , ed where ej is the example that is zero everywhere

except for a 1 in the jth position. Imagine running A on sequence  and telling A it made

a mistake on every example; that is, if A predicts positive on ej we set c(ej) = 1 and if

A predicts negative on ej we set c(ej) = +1. This target corresponds to the disjunction

of all xj such that A predicted negative on ej, so it is a legal disjunction. Since A is

142

deterministic, the fact that we constructed c by running A is not a problem:

it would

make the same mistakes if re-run from scratch on the same sequence and same target.

Therefore, A makes d mistakes on this  and c.

5.8.2 The Halving Algorithm

If we are not concerned with running time, a simple algorithm that guarantees to make at

most log2(|H|) mistakes for a target belonging to any given class H is called the halving

algorithm. This algorithm simply maintains the version space V  H consisting of all

h  H consistent with the labels on every example seen so far, and predicts based on

majority vote over these functions. Each mistake is guaranteed to reduce the size of the

version space V by at least half (hence the name), thus the total number of mistakes is

at most log2(|H|). Note that this can be viewed as the number of bits needed to write a

function in H down.

5.8.3 The Perceptron Algorithm

Earlier we described the Perceptron algorithm as a method for nding a linear separator

consistent with a given training set S. However, the Perceptron algorithm also operates

naturally in the online setting as well.

Recall that the basic assumption of the Perceptron algorithm is that the target func-

tion can be described by a vector w such that for each positive example x we have

xT w  1 and for each negative example x we have xT w  1. Recall also that we can

interpret xT w/|w| as the distance of x to the hyperplane xT w = 0. Thus, we can view

our assumption as stating that there exists a linear separator through the origin with all

positive examples on one side, all negative examples on the other side, and all examples at

distance at least  = 1/|w| from the separator, where  is called the margin of separation.

The guarantee of the Perceptron algorithm will be that the total number of mistakes is

at most (R/)2 where R = maxt |xt| over all examples xt seen so far. Thus, if there exists

a hyperplane through the origin that correctly separates the positive examples from the

negative examples by a large margin relative to the radius of the smallest ball enclosing

the data, then the total number of mistakes will be small. The algorithm, restated in the

143

online setting, is as follows.

The Perceptron Algorithm: Start with the all-zeroes weight vector w = 0. Then, for

t = 1, 2, . . . do:

1. Given example xt, predict sgn(xT

t w).

2. If the prediction was a mistake, then update:

(a) If xt was a positive example, let w  w + xt.

(b) If xt was a negative example, let w  w  xt.

The Perceptron algorithm enjoys the following guarantee on its total number of mis-

takes.

t w  1 for the positive examples and xT

Theorem 5.10 On any sequence of examples x1, x2, . . ., if there exists a vector w such

t w  1 for the negative examples (i.e.,

that xT

a linear separator of margin  = 1/|w|), then the Perceptron algorithm makes at most

R2|w|2 mistakes, where R = maxt |xt|.

Proof: Fix some consistent w. We will keep track of two quantities, wT w and |w|2.

First of all, each time we make a mistake, wT w increases by at least 1. That is because

if xt is a positive example, then

(w + xt)T w = wT w + xT

t w  wT w + 1,

by denition of w. Similarly, if xt is a negative example, then

(w  xt)T w = wT w  xT

t w  wT w + 1.

Next, on each mistake, we claim that |w|2 increases by at most R2. Let us rst consider

mistakes on positive examples. If we make a mistake on a positive example xt then we

have

(w + xt)T (w + xt) = |w|2 + 2xT

t w + |xt|2  |w|2 + |xt|2  |w|2 + R2,

where the middle inequality comes from the fact that we made a mistake, which means

that xT

t w  0. Similarly, if we make a mistake on a negative example xt then we have

(w  xt)T (w  xt) = |w|2  2xT

t w + |xt|2  |w|2 + |xt|2  |w|2 + R2.

Note that it is important here that we only update on a mistake.

So, if we make M mistakes, then wT w  M , and |w|2  M R2, or equivalently,

M . Finally, we use the fact that wT w/|w|  |w| which is just saying that



|w|  R

144

the projection of w in the direction of w cannot be larger than the length of w. This

gives us:



M/|w|  R



M

M  R|w|

M  R2|w|2

as desired.

5.8.4 Extensions: Inseparable Data and Hinge Loss

We assumed above that there exists a perfect w that correctly classies all the exam-

ples, e.g., correctly classies all the emails into important versus non-important. This

is rarely the case in real-life data. What if even the best w isnt quite perfect? We

if there is an example that w doesnt cor-

can see what this does to the above proof:

rectly classify, then while the second part of the proof still holds, the rst part (the dot

product of w with w increasing) breaks down. However, if this doesnt happen too of-

t w is just a little bit wrong then we will only make a few more mistakes.

ten, and also xT

To make this formal, dene the hinge-loss of w on a positive example xt as max(0, 1

xT

t w  1 as desired then the hinge-loss is zero; else, the hinge-

t w). In other words, if xT

loss is the amount the LHS is less than the RHS.21 Similarly, the hinge-loss of w on a

t w). Given a sequence of labeled examples S, dene

negative example xt is max(0, 1 + xT

the total hinge-loss Lhinge(w, S) as the sum of hinge-losses of w on all examples in S.

We now get the following extended theorem.

Theorem 5.11 On any sequence of examples S = x1, x2, . . ., the Perceptron algorithm

makes at most

min

w

mistakes, where R = maxt |xt|.

(cid:0)R2|w|2 + 2Lhinge(w, S)(cid:1)

Proof: As before, each update of the Perceptron algorithm increases |w|2 by at most R2,

so if the algorithm makes M mistakes, we have |w|2  M R2.

What we can no longer say is that each update of the algorithm increases wT w by

at least 1. Instead, on a positive example we are increasing wT w by xT

t w (it could

be negative), which is at least 1  Lhinge(w, xt). Similarly, on a negative example we

increase wT w by xT

t w, which is also at least 1  Lhinge(w, xt). If we sum this up

over all mistakes, we get that at the end we have wT w  M  Lhinge(w, S), where we

are using here the fact that hinge-loss is never negative so summing over all of S is only

larger than summing over the mistakes that w made.

21This is called hinge-loss because as a function of xT

t w it looks like a hinge.

145

Finally, we just do some algebra. Let L = Lhinge(w, S). So we have:

wT w/|w|  |w|

(wT w)2  |w|2|w|2

(M  L)2  M R2|w|2

M 2  2M L + L2  M R2|w|2

M  2L + L2/M  R2|w|2

M  R2|w|2 + 2L  L2/M  R2|w|2 + 2L

as desired.

5.9 Online to Batch Conversion

Suppose we have an online algorithm with a good mistake bound, such as the Perceptron

algorithm. Can we use it to get a guarantee in the distributional (batch) learning setting?

Intuitively, the answer should be yes since the online setting is only harder. Indeed, this

intuition is correct. We present here two natural approaches for such online to batch

conversion.

Conversion procedure 1: Random Stopping. Suppose we have an online algorithm

A with mistake-bound M . Say we run the algorithm in a single pass on a sample S of size

M/(cid:15). Let Xt be the indicator random variable for the event that A makes a mistake on

example xt. Since (cid:80)|S|

t=1 Xt]  M

where the expectation is taken over the random draw of S from D|S|. By linearity of

expectation, and dividing both sides by |S| we therefore have:

t=1 Xt  M for any set S, we certainly have that E[(cid:80)|S|

1

|S|

|S|

(cid:88)

t=1

E[Xt]  M/|S| = (cid:15).

(5.1)

Let ht denote the hypothesis used by algorithm A to predict on the tth example. Since

the tth example was randomly drawn from D, we have E[errD(ht)] = E[Xt]. This means

that if we choose t at random from 1 to |S|, i.e., stop the algorithm at a random time, the

expected error of the resulting prediction rule, taken over the randomness in the draw of

S and the choice of t, is at most (cid:15) as given by equation (5.1). Thus we have:

Theorem 5.12 (Online to Batch via Random Stopping) If an online algorithm A

with mistake-bound M is run on a sample S of size M/(cid:15) and stopped at a random time

between 1 and |S|, the expected error of the hypothesis h produced satises E[errD(h)]  (cid:15).

Conversion procedure 2: Controlled Testing. A second natural approach to us-

ing an online learning algorithm A in the distributional setting is to just run a series of

controlled tests. Specically, suppose that the initial hypothesis produced by algorithm

A is h1. Dene i = /(i + 2)2 so we have (cid:80)

6  1)  . We draw a set of

i=0 i = ( 2

146

(cid:15) log( 1

1

n1 = 1

) random examples and test to see whether h1 gets all of them correct. Note

that if errD(h1)  (cid:15) then the chance h1 would get them all correct is at most (1(cid:15))n1  1.

So, if h1 indeed gets them all correct, we output h1 as our hypothesis and halt. If not,

we choose some example x1 in the sample on which h1 made a mistake and give it to

algorithm A. Algorithm A then produces some new hypothesis h2 and we again repeat,

testing h2 on a fresh set of n2 = 1

) random examples, and so on.

(cid:15) log( 1

2

In general, given ht we draw a fresh set of nt = 1

) random examples and test

to see whether ht gets all of them correct. If so, we output ht and halt; if not, we choose

some xt on which ht(xt) was incorrect and give it to algorithm A. By choice of nt, if ht

had error rate (cid:15) or larger, the chance we would mistakenly output it is at most t. By

choice of the values t, the chance we ever halt with a hypothesis of error (cid:15) or larger is at

most 1 + 2 + . . .  . Thus, we have the following theorem.

(cid:15) log( 1

t

Theorem 5.13 (Online to Batch via Controlled Testing) Let A be an online learn-

(cid:15) log( M

ing algorithm with mistake-bound M . Then this procedure will halt after O( M

 ))

examples and with probability at least 1   will produce a hypothesis of error at most (cid:15).

Note that in this conversion we cannot re-use our samples: since the hypothesis ht depends

on the previous data, we need to draw a fresh set of nt examples to use for testing it.

5.10 Support-Vector Machines

In a batch setting, rather than running the Perceptron algorithm and adapting it via

one of the methods above, another natural idea would be just to solve for the vector w

that minimizes the right-hand-side in Theorem 5.11 on the given dataset S. This turns

out to have good guarantees as well, though they are beyond the scope of this book. In

fact, this is the Support Vector Machine (SVM) algorithm. Specically, SVMs solve the

following convex optimization problem over a sample S = {x1, x2, . . . xn} where c is a

constant that is determined empirically.

minimize

subject to

c|w|2 +

(cid:88)

si

i

w  xi  1  si for all positive examples xi

w  xi  1 + si for all negative examples xi

si  0 for all i.

The variables si are called slack variables, and notice that the sum of the slack variables

is the total hinge loss of w. So, this convex optimization is minimizing a weighted sum

of 1/2, where  is the margin, and the total hinge loss. If we were to add the constraint

that all si = 0 then this would be solving for the maximum margin linear separator for the

data. However, in practice, optimizing a weighted combination generally performs better.

SVMs can also be kernelized, by using the dual of the above optimization problem (the

147

key idea is that the optimal w will be a weighted combination of data points, just as in the

Perceptron algorithm, and these weights can be variables in the optimization problem);

details are beyond the scope of this book.

5.11 VC-Dimension

In Section 5.5 we presented several theorems showing that so long as the training set

S is large compared to 1

(cid:15) log(|H|), we can be condent that every h  H with errD(h)  (cid:15)

will have errS(h) > 0, and if S is large compared to 1

(cid:15)2 log(|H|), then we can be condent

that every h  H will have |errD(h)  errS(h)|  (cid:15). In essence, these results used log(|H|)

as a measure of complexity of class H. VC-dimension is a dierent, tighter measure of

complexity for a concept class, and as we will see, is also sucient to yield condence

bounds. For any class H, VCdim(H)  log2(|H|) but it can also be quite a bit smaller.

Lets introduce and motivate it through an example.

Consider a database consisting of the salary and age for a random sample of the adult

population in the United States. Suppose we are interested in using the database to an-

swer questions of the form: what fraction of the adult population in the United States

has age between 35 and 45 and salary between $50,000 and $70,000? That is, we are

interested in queries that ask about the fraction of the adult population within some axis-

parallel rectangle. What we can do is calculate the fraction of the database satisfying

this condition and return this as our answer. This brings up the following question: How

large does our database need to be so that with probability greater than or equal to 1  ,

our answer will be within (cid:15) of the truth for every possible rectangle query of this form?

If we assume our values are discretized such as 100 possible ages and 1,000 possible

salaries, then there are at most (100  1, 000)2 = 1010 possible rectangles. This means we

can apply Theorem 5.5 with |H|  1010. Specically, we can think of the target concept

c as the empty set so that errS(h) is exactly the fraction of the sample inside rectangle

h and errD(h) is exactly the fraction of the whole population inside h.22 This would tell

1

2(cid:15)2 (10 ln 10 + ln(2/)) would be sucient.

us that a sample size of

However, what if we do not wish to discretize our concept class? Another approach

would be to say that if there are only N adults total in the United States, then there

are at most N 4 rectangles that are truly dierent with respect to D and so we could use

|H|  N 4. Still, this suggests that S needs to grow with N , albeit logarithmically, and

one might wonder if that is really necessary. VC-dimension, and the notion of the growth

function of concept class H, will give us a way to avoid such discretization and avoid any

dependence on the size of the support of the underlying distribution D.

22Technically D is the uniform distribution over the adult population of the United States, and we

want to think of S as an independent identically distributed sample from this D.

148

B

A

D

(b)

C

(a)

Figure 5.5: (a) shows a set of four points that can be shattered by rectangles along with

some of the rectangles that shatter the set. Not every set of four points can be shattered

as seen in (b). Any rectangle containing points A, B, and C must contain D. No set of ve

points can be shattered by rectangles with axis-parallel edges. No set of three collinear

points can be shattered, since any rectangle that contains the two end points must also

contain the middle point. More generally, since rectangles are convex, a set with one point

inside the convex hull of the others cannot be shattered.

5.11.1 Denitions and Key Theorems

Denition 5.1 Given a set S of examples and a concept class H, we say that S is

shattered by H if for every A  S there exists some h  H that labels all examples in A

as positive and all examples in S \ A as negative.

Denition 5.2 The VC-dimension of H is the size of the largest set shattered by H.

For example, there exist sets of four points in the plane that can be shattered by rect-

angles with axis-parallel edges, e.g., four points at the vertices of a diamond (see Figure

5.5). Given such a set S, for any A  S, there exists a rectangle with the points in A

inside the rectangle and the points in S \ A outside the rectangle. However, rectangles

with axis-parallel edges cannot shatter any set of ve points. To see this, assume for

contradiction that there is a set of ve points shattered by the family of axis-parallel

rectangles. Find the minimum enclosing rectangle for the ve points. For each edge there

is at least one point that has stopped its movement. Identify one such point for each edge.

The same point may be identied as stopping two edges if it is at a corner of the minimum

enclosing rectangle. If two or more points have stopped an edge, designate only one as

having stopped the edge. Now, at most four points have been designated. Any rectangle

enclosing the designated points must include the undesignated points. Thus, the subset

of designated points cannot be expressed as the intersection of a rectangle with the ve

points. Therefore, the VC-dimension of axis-parallel rectangles is four.

We now need one more denition, which is the growth function of a concept class H.

Denition 5.3 Given a set S of examples and a concept class H, let H[S] = {h  S :

149

h  H}. That is, H[S] is the concept class H restricted to the set of points S. For integer

n and class H, let H[n] = max|S|=n |H[S]|; this is called the growth function of H.

For example, we could have dened shattering by saying that S is shattered by H

if |H[S]| = 2|S|, and then the VC-dimension of H is the largest n such that H[n] = 2n.

Notice also that for axis-parallel rectangles, H[n] = O(n4). The growth function of a class

is sometimes called the shatter function or shatter coecient.

What connects these to learnability are the following three remarkable theorems. The

rst two are analogs of Theorem 5.3 and Theorem 5.5 respectively, showing that one can

replace |H| with its growth function. This is like replacing the number of concepts in H

with the number of concepts after the fact, i.e., after S is drawn, and is subtle because

we cannot just use a union bound after we have already drawn our set S. The third

theorem relates the growth function of a class to its VC-dimension. We now present the

theorems, give examples of VC-dimension and growth function of various concept classes,

and then prove the theorems.

Theorem 5.14 (Growth function sample bound) For any class H and distribution

D, if a training sample S is drawn from D of size

n 

2

(cid:15)

[log2(2H[2n]) + log2(1/)]

then with probability  1, every h  H with errD(h)  (cid:15) has errS(h) > 0 (equivalently,

every h  H with errS(h) = 0 has errD(h) < (cid:15)).

Theorem 5.15 (Growth function uniform convergence) For any class H and dis-

tribution D, if a training sample S is drawn from D of size

n 

8

(cid:15)2 [ln(2H[2n]) + ln(1/)]

then with probability  1  , every h  H will have |errS(h)  errD(h)|  (cid:15).

Theorem 5.16 (Sauers lemma) If VCdim(H) = d then H[n]  (cid:80)d

i=0

(cid:0)n

i

(cid:1)  ( en

d )d.

Notice that Sauers lemma was fairly tight in the case of axis-parallel rectangles,

though in some cases it can be a bit loose. E.g., we will see that for linear separators

in the plane, their VC-dimension is 3 but H[n] = O(n2). An interesting feature about

Sauers lemma is that it implies the growth function switches from taking the form 2n to

taking the form of roughly nVCdim(H) when n exceeds the VC-dimension of the class H.

Putting Theorems 5.14 and 5.16 together, with a little algebra we get the following

corollary (a similar corollary results by combining Theorems 5.15 and 5.16):

150

Corollary 5.17 (VC-dimension sample bound) For any class H and distribution D,

a training sample S of size

(cid:18) 1

(cid:15)

O

[VCdim(H) log(1/(cid:15)) + log(1/)]

(cid:19)

is sucient to ensure that with probability  1  , every h  H with errD(h)  (cid:15) has

errS(h) > 0 (equivalently, every h  H with errS(h) = 0 has errD(h) < (cid:15)).

For any class H, VCdim(H)  log2(|H|) since H must have at least 2k concepts in

order to shatter k points. Thus Corollary 5.17 is never too much worse than Theorem 5.3

and can be much better.

5.11.2 Examples: VC-Dimension and Growth Function

Rectangles with axis-parallel edges

As we saw above, the class of axis-parallel rectangles in the plane has VC-dimension

4 and growth function H[n] = O(n4).

Intervals of the reals

Intervals on the real line can shatter any set of two points but no set of three points

since the subset of the rst and last points cannot be isolated. Thus, the VC-dimension

of intervals is two. Also, H[n] = O(n2) since we have O(n2) choices for the left and right

endpoints.

Pairs of intervals of the reals

Consider the family of pairs of intervals, where a pair of intervals is viewed as the set

of points that are in at least one of the intervals, in other words, their set union. There

exists a set of size four that can be shattered but no set of size ve since the subset of rst,

third, and last point cannot be isolated. Thus, the VC-dimension of pairs of intervals is

four. Also we have H[n] = O(n4).

Convex polygons

Consider the set system of all convex polygons in the plane. For any positive integer

n, place n points on the unit circle. Any subset of the points are the vertices of a convex

polygon. Clearly that polygon will not contain any of the points not in the subset. This

shows that convex polygons can shatter arbitrarily large sets, so the VC-dimension is

innite. Notice that this also implies that H[n] = 2n.

Halfspaces in d-dimensions

151

Dene a halfspace to be the set of all points on one side of a linear separator, i.e.,

a set of the form {x|wT x  w0}. The VC-dimension of halfspaces in d-dimensions is d+1.

There exists a set of size d + 1 that can be shattered by halfspaces. Select the d unit-

coordinate vectors plus the origin to be the d + 1 points. Suppose A is any subset of these

d + 1 points. Without loss of generality assume that the origin is in A. Take a 0-1 vector

w which has 1s precisely in the coordinates corresponding to vectors not in A. Clearly

A lies in the half-space wT x  0 and the complement of A lies in the complementary

halfspace.

We now show that no set of d + 2 points in d-dimensions can be shattered by halfs-

paces. This is done by proving that any set of d + 2 points can be partitioned into two

disjoint subsets A and B of points whose convex hulls intersect. This establishes the claim

since any linear separator with A on one side must have its entire convex hull on that

side,23 so it is not possible to have a linear separator with A on one side and B on the other.

Let convex(S) denote the convex hull of point set S.

Theorem 5.18 (Radon): Any set S  Rd with |S|  d + 2, can be partitioned into two

disjoint subsets A and B such that convex(A)  convex(B) (cid:54)= .

Proof: Without loss of generality, assume |S| = d + 2. Form a d  (d + 2) matrix with one

column for each point of S. Call the matrix A. Add an extra row of all 1s to construct a

(d+1)(d+2) matrix B. Clearly the rank of this matrix is at most d+1 and the columns

are linearly dependent. Say x = (x1, x2, . . . , xd+2) is a nonzero vector with Bx = 0.

Reorder the columns so that x1, x2, . . . , xs  0 and xs+1, xs+2, . . . , xd+2 < 0. Normalize

|xi| = 1. Let bi (respectively ai) be the ith column of B (respectively A). Then,

x so

s

(cid:80)

i=1

|xi|bi =

|xi|. Since

d+2

(cid:80)

i=s+1

s

(cid:80)

i=1

|xi|bi from which it follows that

s

(cid:80)

i=1

d+2

(cid:80)

i=s+1

combination of columns of A which proves the theorem. Thus, S can be partitioned into

two sets, the rst consisting of the rst s points after the rearrangement and the second

consisting of points s + 1 through d + 2 . Their convex hulls intersect as required.

|xi| = 1 each side of

|xi|ai is a convex

|xi| = 1 and

d+2

(cid:80)

i=s+1

d+2

(cid:80)

i=s+1

d+2

(cid:80)

i=s+1

|xi|ai and

|xi|ai =

|xi|ai =

s

(cid:80)

i=1

s

(cid:80)

i=1

s

(cid:80)

i=1

|xi| =

Radons theorem immediately implies that half-spaces in d-dimensions do not shatter

any set of d + 2 points.

Spheres in d-dimensions

23If any two points x1 and x2 lie on the same side of a linear separator, so must any convex combination:

if w  x1  b and w  x2  b then w  (ax1 + (1  a)x2)  b.

152

A sphere in d-dimensions is a set of points of the form {x| |x  x0|  r}. The VC-

dimension of spheres is d + 1. It is the same as that of halfspaces. First, we prove that no

set of d + 2 points can be shattered by spheres. Suppose some set S with d + 2 points can

be shattered. Then for any partition A1 and A2 of S, there are spheres B1 and B2 such

that B1  S = A1 and B2  S = A2. Now B1 and B2 may intersect, but there is no point

of S in their intersection. It is easy to see that there is a hyperplane perpendicular to

the line joining the centers of the two spheres with all of A1 on one side and all of A2 on

the other and this implies that halfspaces shatter S, a contradiction. Therefore no d + 2

points can be shattered by hyperspheres.

It is also not dicult to see that the set of d+1 points consisting of the unit-coordinate

vectors and the origin can be shattered by spheres. Suppose A is a subset of the d + 1

points. Let a be the number of unit vectors in A. The center a0 of our sphere will be

the sum of the vectors in A. For every unit vector in A, its distance to this center will

a + 1.

be

a. Thus, we can choose the radius so that

The distance of the origin to the center is

precisely the points in A are in the hypersphere.

a  1 and for every unit vector outside A, its distance to this center will be







Finite sets

The system of nite sets of real numbers can shatter any nite set of real numbers

and thus the VC-dimension of nite sets is innite.

5.11.3 Proof of Main Theorems

We begin with a technical lemma. Consider drawing a set S of n examples from D and

let A denote the event that there exists h  H with zero training error on S but true

error greater than or equal to (cid:15). Now draw a second set S(cid:48) of n examples from D and let

B denote the event that there exists h  H with zero error on S but error greater than

or equal to (cid:15)/2 on S(cid:48). We claim that Prob(B)  Prob(A)/2.

Lemma 5.19 Let H be a concept class over some domain X and let S and S(cid:48) be sets of

n elements drawn from some distribution D on X , where n  8/(cid:15). Let A be the event that

there exists h  H with zero error on S but true error greater than or equal to (cid:15). Let B

be the event that there exists h  H with zero error on S but error greater than or equal

to (cid:15)

2 on S(cid:48). Then Prob(B)  Prob(A)/2.

Proof: Clearly, Prob(B)  Prob(A, B) = Prob(A)Prob(B|A). Consider drawing set S

and suppose event A occurs. Let h be in H with errD(h)  (cid:15) but errS(h) = 0. Now,

draw set S(cid:48). E(error of h on S(cid:48)) = errD(h)  (cid:15). So, by Cherno bounds, since n  8/(cid:15),

Prob(errS(cid:48)(h)  (cid:15)/2)  1/2. Thus, Prob(B|A)  1/2 and Prob(B)  Prob(A)/2 as

desired.

We now prove Theorem 5.14, restated here for convenience.

153

Theorem 5.14 (Growth function sample bound) For any class H and distribution

D, if a training sample S is drawn from D of size

n 

2

(cid:15)

[log2(2H[2n]) + log2(1/)]

then with probability  1, every h  H with errD(h)  (cid:15) has errS(h) > 0 (equivalently,

every h  H with errS(h) = 0 has errD(h) < (cid:15)).

Proof: Consider drawing a set S of n examples from D and let A denote the event that

there exists h  H with true error greater than (cid:15) but training error zero. Our goal is to

prove that Prob(A)  .

By Lemma 5.19 it suces to prove that Prob(B)  /2. Consider a third experiment.

Draw a set S(cid:48)(cid:48) of 2n points from D and then randomly partition S(cid:48)(cid:48) into two sets S and

S(cid:48) of n points each. Let B denote the event that there exists h  H with errS(h) = 0

but errS(cid:48)(h)  (cid:15)/2. Prob(B) = Prob(B) since drawing 2n points from D and randomly

partitioning them into two sets of size n produces the same distribution on (S, S(cid:48)) as does

drawing S and S(cid:48) directly. The advantage of this new experiment is that we can now

argue that Prob(B) is low by arguing that for any set S(cid:48)(cid:48) of size 2n, Prob(B|S(cid:48)(cid:48)) is low,

with probability now taken over just the random partition of S(cid:48)(cid:48) into S and S(cid:48). The key

point is that since S(cid:48)(cid:48) is xed, there are at most |H[S(cid:48)(cid:48)]|  H[2n] events to worry about.

Specically, it suces to prove that for any xed h  H[S(cid:48)(cid:48)], the probability over the

partition of S(cid:48)(cid:48) that h makes zero mistakes on S but more than (cid:15)n/2 mistakes on S(cid:48) is at

most /(2H[2n]). We can then apply the union bound over H[S(cid:48)(cid:48)] = {h  S(cid:48)(cid:48)|h  H}.

To make the calculations easier, consider the following specic method for partitioning

S(cid:48)(cid:48) into S and S(cid:48). Randomly put the points in S(cid:48)(cid:48) into pairs: (a1, b1), (a2, b2), . . ., (an, bn).

For each index i, ip a fair coin. If heads put ai into S and bi into S(cid:48), else if tails put ai

into S(cid:48) and bi into S. Now, x some partition h  H[S(cid:48)(cid:48)] and consider the probability over

these n fair coin ips that h makes zero mistakes on S but more than (cid:15)n/2 mistakes on S(cid:48).

First of all, if for any index i, h makes a mistake on both ai and bi then the probability is

zero (because it cannot possibly make zero mistakes on S). Second, if there are fewer than

(cid:15)n/2 indices i such that h makes a mistake on either ai or bi then again the probability is

zero because it cannot possibly make more than (cid:15)n/2 mistakes on S(cid:48). So, assume there

are r  (cid:15)n/2 indices i such that h makes a mistake on exactly one of ai or bi. In this case,

the chance that all of those mistakes land in S(cid:48) is exactly 1/2r. This quantity is at most

1/2(cid:15)n/2  /(2H[2n]) as desired for n as given in the theorem statement.

We now prove Theorem 5.15, restated here for convenience.

Theorem 5.15 (Growth function uniform convergence) For any class H and dis-

tribution D, if a training sample S is drawn from D of size

n 

8

(cid:15)2 [ln(2H[2n]) + ln(1/)]

154

then with probability  1  , every h  H will have |errS(h)  errD(h)|  (cid:15).

Proof: This proof is identical to the proof of Theorem 5.14 except B is now the event

that there exists a set h  H[S(cid:48)(cid:48)] such that the error of h on S diers from the error of h on

S(cid:48) by more than (cid:15)/2. We again consider the experiment where we randomly put the points

in S(cid:48)(cid:48) into pairs (ai, bi) and then ip a fair coin for each index i, if heads placing ai into S

and bi into S(cid:48), else placing ai into S(cid:48) and bi into S. Consider the dierence between the

number of mistakes h makes on S and the number of mistakes h makes on S(cid:48) and observe

how this dierence changes as we ip coins for i = 1, 2, . . . , n. Initially, the dierence

is zero. If h makes a mistake on both or neither of (ai, bi) then the dierence does not

change. Else, if h makes a mistake on exactly one of ai or bi, then with probability 1/2

the dierence increases by one and with probability 1/2 the dierence decreases by one.

If there are r  n such pairs, then if we take a random walk of r  n steps, what is the

probability that we end up more than (cid:15)n/2 steps away from the origin? This is equivalent

to asking: if we ip r  n fair coins, what is the probability the number of heads diers

from its expectation by more than (cid:15)n/4. By Hoeding bounds, this is at most 2e(cid:15)2n/8.

This quantity is at most /(2H[2n]) as desired for n as given in the theorem statement.

Finally, we prove Sauers lemma, relating the growth function to the VC-dimension.

Theorem 5.16 (Sauers lemma) If VCdim(H) = d then H[n]  (cid:80)d

i=0

(cid:0)n

i

(cid:1)  ( en

d )d.

Proof: Let d = VCdim(H). Our goal is to prove for any set S of n points that

|H[S]|  (cid:0) n

(cid:1); this is the number of distinct

ways of choosing d or fewer elements out of n. We will do so by induction on n. As a

base case, our theorem is trivially true if n  d.

(cid:1), where we are dening (cid:0) n

(cid:1) = (cid:80)d

(cid:0)n

i

i=0

d

d

As a rst step in the proof, notice that:

(cid:19)

(cid:18) n

 d

=

(cid:19)

(cid:18)n  1

 d

+

(cid:19)

(cid:18) n  1

 d  1

(5.2)

because we can partition the ways of choosing d or fewer items into those that do not

include the rst item (leaving  d to be chosen from the remainder) and those that do

include the rst item (leaving  d  1 to be chosen from the remainder).

Now, consider any set S of n points and pick some arbitrary point x  S. By induc-

(cid:1). So, by equation (5.2) all we need to show

(cid:1). Thus, our problem has reduced to analyzing how

tion, we may assume that |H[S \ {x}]|  (cid:0)n1

is that |H[S]|  |H[S \ {x}]|  (cid:0) n1

many more partitions there are of S than there are of S \ {x} using sets in H.

d1

d

If H[S] is larger than H[S \ {x}], it is because of pairs of sets in H[S] that dier only

on point x and therefore collapse to the same set when x is removed. For set h  H[S]

155

containing point x, dene twin(h) = h \ {x}; this may or may not belong to H[S]. Let

T = {h  H[S] : x  h and twin(h)  H[S]}. Notice |H[S]|  |H[S \ {x}]| = |T |.

Now, what is the VC-dimension of T ? If d(cid:48) = VCdim(T ), this means there is some set

R of d(cid:48) points in S \ {x} that are shattered by T . By denition of T , all 2d(cid:48) subsets of R

can be extended to either include x, or not include x and still be a set in H[S]. In other

words, R  {x} is shattered by H. This means, d(cid:48) + 1  d. Since VCdim(T )  d  1, by

induction we have |T |  (cid:0) n1

(cid:1) as desired.

d1

5.11.4 VC-Dimension of Combinations of Concepts

Often one wants to create concepts out of other concepts. For example, given several

linear separators, one could take their intersection to create a convex polytope. Or given

several disjunctions, one might want to take their majority vote. We can use Sauers

lemma to show that such combinations do not increase the VC-dimension of the class by

too much.

Specically, given k concepts h1, h2, . . . , hk and a Booelan function f dene the set

combf (h1, . . . , hk) = {x  X : f (h1(x), . . . , hk(x)) = 1}, where here we are using hi(x) to

denote the indicator for whether or not x  hi. For example, f might be the AND function

to take the intersection of the sets hi, or f might be the majority-vote function. This can

be viewed as a depth-two neural network. Given a concept class H, a Boolean function f ,

and an integer k, dene the new concept class COM Bf,k(H) = {combf (h1, . . . , hk) : hi 

H}. We can now use Sauers lemma to produce the following corollary.

Corollary 5.20 If the concept class H has VC-dimension d, then for any combination

function f , the class COMBf,k(H) has VC-dimension O(cid:0)kd log(kd)(cid:1).

Proof: Let n be the VC-dimension of COMBf,k(H), so by denition, there must exist

a set S of n points shattered by COMBf,k(H). We know by Sauers lemma that there

are at most nd ways of partitioning the points in S using sets in H. Since each set in

COMBf,k(H) is determined by k sets in H, and there are at most (nd)k = nkd dierent

k-tuples of such sets, this means there are at most nkd ways of partitioning the points

using sets in COMBf,k(H). Since S is shattered, we must have 2n  nkd, or equivalently

n so

n  kd log2(n). We solve this as follows. First, assuming n  16 we have log2(n) 

n which implies that n  (kd)2. To get the better bound, plug back into

kd log2(n)  kd

the original inequality. Since n  (kd)2, it must be that log2(n)  2 log2(kd). Substituting

log n  2 log2(kd) into n  kd log2 n gives n  2kd log2(kd).





This result will be useful for our discussion of Boosting in Section 5.12.

5.11.5 Other Measures of Complexity

VC-dimension and number of bits needed to describe a set are not the only measures

of complexity one can use to derive generalization guarantees. There has been signicant

156

(cid:80)n

work on a variety of measures. One measure called Rademacher complexity measures

the extent to which a given concept class H can t random noise. Given a set of n

examples S = {x1, . . . , xn}, the empirical Rademacher complexity of H is dened as

1

i=1 ih(xi), where i  {1, 1} are independent random labels

RS(H) = E1,...,n max

n

hH

with Prob[i = 1] = 1

2. E.g., if you assign random 1 labels to the points in S and the

best classier in H on average gets error 0.45 then RS(H) = 0.55  0.45 = 0.1. One can

prove that with probability greater than or equal to 1  , every h  H satises true error

2n . For more on results such as

less than or equal to training error plus RS(H) + 3

this, see, e.g., [BM02].

(cid:113) ln(2/)

5.12 Strong and Weak Learning - Boosting

We now describe boosting, which is important both as a theoretical result and as a

practical and easy-to-use learning method.

2   for some 0 <   1

A strong learner for a problem is an algorithm that with high probability is able to

achieve any desired error rate (cid:15) using a number of samples that may depend polynomially

on 1/(cid:15). A weak learner for a problem is an algorithm that does just a little bit better than

random guessing. It is only required to get with high probability an error rate less than

or equal to 1

2. We show here that a weak-learner for a problem

that achieves the weak-learning guarantee for any distribution of data can be boosted to a

strong learner, using the technique of boosting. At the high level, the idea will be to take

our training sample S, and then to run the weak-learner on dierent data distributions

produced by weighting the points in the training sample in dierent ways. Running the

weak learner on these dierent weightings of the training sample will produce a series of

hypotheses h1, h2, . . ., and the idea of our reweighting procedure will be to focus attention

on the parts of the sample that previous hypotheses have performed poorly on. At the

end we will combine the hypotheses together by a majority vote.

Assume the weak learning algorithm A outputs hypotheses from some class H. Our

boosting algorithm will produce hypotheses that will be majority votes over t0 hypotheses

from H, for t0 dened below. This means that we can apply Corollary 5.20 to bound the

VC-dimension of the class of hypotheses our boosting algorithm can produce in terms of

the VC-dimension of H. In particular, the class of rules that can be produced by the

booster running for t0 rounds has VC-dimension O(t0VCdim(H) log(t0VCdim(H))). This

in turn gives a bound on the number of samples needed, via Corollary 5.17, to ensure that

high accuracy on the sample will translate to high accuracy on new data.

To make the discussion simpler, we will assume that the weak learning algorithm A,

when presented with a weighting of the points in our training sample, always (rather than

with high probability) produces a hypothesis that performs slightly better than random

guessing with respect to the distribution induced by weighting. Specically:

157

Boosting Algorithm

Given a sample S of n labeled examples x1, . . . , xn, initialize each

example xi to have a weight wi = 1. Let w = (w1, . . . , wn).

For t = 1, 2, . . . , t0 do

Call the weak learner on the weighted sample (S, w), receiving

hypothesis ht.

Multiply the weight of each example that was misclassied by

ht by  =

. Leave the other weights as they are.

1

2 +

1

2 

End

Output the classier MAJ(h1, . . . , ht0) which takes the majority vote

of the hypotheses returned by the weak learner. Assume t0 is odd so

there is no tie.

Figure 5.6: The boosting algorithm

Denition 5.4 (-Weak learner on sample) A -weak learner is an algorithm that

given examples, their labels, and a nonnegative real weight wi on each example xi, produces

a classier that correctly labels a subset of examples with total weight at least ( 1

wi.

2 +)

n

(cid:80)

i=1

At the high level, boosting makes use of the intuitive notion that if an example was

misclassied, one needs to pay more attention to it. The boosting procedure is in Figure

5.6.

Theorem 5.21 Let A be a -weak learner for sample S. Then t0 = O( 1

2 log n) is su-

cient so that the classier MAJ(h1, . . . , ht0) produced by the boosting procedure has training

error zero.

Proof: Suppose m is the number of examples the nal classier gets wrong. Each of

these m examples was misclassied at least t0/2 times so each has weight at least t0/2.

Thus the total weight is at least mt0/2. On the other hand, at time t+1, only the weights

of examples misclassied at time t were increased. By the property of weak learning, the

total weight of misclassied examples is at most ( 1

2  ) of the total weight at time t. Let

weight(t) be the total weight at time t. Then

weight(t + 1) 

(cid:16)

 (cid:0) 1

2 + (cid:1) (cid:17)

2  (cid:1) + (cid:0) 1

= (1 + 2)  weight(t).

 weight(t)

158

Since weight(0) = n, the total weight at the end is at most n(1 + 2)t0. Thus

mt0/2  total weight at end  n(1 + 2)t0.

Substituting  = 1/2+

1/2 = 1+2

12 and rearranging terms

m  n(1  2)t0/2(1 + 2)t0/2 = n[1  42]t0/2.

Using 1  x  ex, m  ne2t02. For t0 > ln n

items must be zero.

22 , m < 1, so the number of misclassied

Having completed the proof of the boosting result, here are two interesting observa-

tions:

Connection to Hoeding bounds: The boosting result applies even if our weak learn-

ing algorithm is adversarial, giving us the least helpful classier possible subject

to Denition 5.4. This is why we dont want the  in the boosting algorithm to be

too large, otherwise the weak learner could return the negation of the classier it

gave the last time. Suppose that the weak learning algorithm gave a classier each

time that for each example, ipped a coin and produced the correct answer with

probability 1

2  , so it is a -weak

learner in expectation. In that case, if we called the weak learner t0 times, for any

xed xi, Hoeding bounds imply the chance the majority vote of those classiers is

incorrect on xi is at most e2t02. So, the expected total number of mistakes m is

at most ne2t02. What is interesting is that this is the exact bound we get from

boosting without the expectation for an adversarial weak-learner.

2 +  and the wrong answer with probability 1

A minimax view: Consider a 2-player zero-sum game 24 with one row for each example

xi and one column for each hypothesis hj that the weak-learning algorithm might

output. If the row player chooses row i and the column player chooses column j,

then the column player gets a payo of one if hj(xi) is correct and gets a payo

of zero if hj(xi) is incorrect. The -weak learning assumption implies that for any

randomized strategy for the row player (any mixed strategy in the language of

game theory), there exists a response hj that gives the column player an expected

payo of at least 1

2 + . The von Neumann minimax theorem 25 states that this

implies there exists a probability distribution on the columns (a mixed strategy for

the column player) such that for any xi, at least a 1

2 +  probability mass of the

24A two person zero sum game consists of a matrix whose columns correspond to moves for Player 1

and whose rows correspond to moves for Player 2. The ijth entry of the matrix is the payo for Player

1 if Player 1 choose the jth column and Player 2 choose the ith row. Player 2s payo is the negative of

Player1s.

25The von Neumann minimax theorem states that there exists a mixed strategy for each player so that

given Player 2s strategy the best payo possible for Player 1 is the negative of given Player 1s strategy

the best possible payo for Player 2. A mixed strategy is one in which a probability is assigned to every

possible move for each situation a player could be in.

159

columns under this distribution is correct on xi. We can think of boosting as a

fast way of nding a very simple probability distribution on the columns (just an

average over O(log n) columns, possibly with repetitions) that is nearly as good (for

any xi, more than half are correct) that moreover works even if our only access to

the columns is by running the weak learner and observing its outputs.

We argued above that t0 = O( 1

2 log n) rounds of boosting are sucient to produce a

majority-vote rule h that will classify all of S correctly. Using our VC-dimension bounds,

this implies that if the weak learner is choosing its hypotheses from concept class H, then

a sample size

n = O

(cid:18) 1

(cid:15)

(cid:18) VCdim(H)

2

(cid:19)(cid:19)

is sucient to conclude that with probability 1   the error is less than or equal to (cid:15),

where we are using the O notation to hide logarithmic factors. It turns out that running

the boosting procedure for larger values of t0 i.e., continuing past the point where S is

classied correctly by the nal majority vote, does not actually lead to greater overtting.

The reason is that using the same type of analysis used to prove Theorem 5.21, one can

show that as t0 increases, not only will the majority vote be correct on each x  S, but

in fact each example will be correctly classied by a 1

2 + (cid:48) fraction of the classiers,

where (cid:48)   as t0  . I.e., the vote is approaching the minimax optimal strategy for

the column player in the minimax view given above. This in turn implies that h can be

well-approximated over S by a vote of a random sample of O(1/2) of its component weak

hypotheses hj. Since these small random majority votes are not overtting by much, our

generalization theorems imply that h cannot be overtting by much either.

5.13 Stochastic Gradient Descent

We now describe a widely-used algorithm in machine learning, called stochastic gradi-

ent descent (SGD). The Perceptron algorithm we examined in Section 5.8.3 can be viewed

as a special case of this algorithm, as can methods for deep learning.

Let F be a class of real-valued functions fw : Rd  R where w = (w1, w2, . . . , wn) is a

vector of parameters. For example, we could think of the class of linear functions where

n = d and fw(x) = wT x, or we could have more complicated functions where n > d. For

each such function fw we can dene an associated set hw = {x : fw(x)  0}, and let

HF = {hw : fw  F}. For example, if F is the class of linear functions then HF is the

class of linear separators.

To apply stochastic gradient descent, we also need a loss function L(fw(x), c(x)) that

describes the real-valued penalty we will associate with function fw for its prediction on

an example x whose true label is c(x). The algorithm is then the following:

160

Stochastic Gradient Descent:

Given: starting point w = winit and learning rates 1, 2, 3, . . .

(e.g., winit = 0 and t = 1 for all t, or t = 1/

t).



Consider a sequence of random examples (x1, c(x1)), (x2, c(x2)), . . ..

1. Given example (xt, c(xt)), compute the gradient L(fw(xt), c(xt)) of the loss of

fw(xt) with respect to the weights w. This is a vector in Rn whose ith component is

L(fw(xt),c(xt))

wi

.

2. Update: w  w  tL(fw(xt), c(xt)).

Lets now try to understand the algorithm better by seeing a few examples of instan-

tiating the class of functions F and loss function L.

First, consider n = d and fw(x) = wT x, so F is the class of linear predictors. Consider

the loss function L(fw(x), c(x)) = max(0, c(x)fw(x)), and recall that c(x)  {1, 1}.

In other words, if fw(x) has the correct sign, then we have a loss of 0, otherwise we have

a loss equal to the magnitude of fw(x). In this case, if fw(x) has the correct sign and is

non-zero, then the gradient will be zero since an innitesimal change in any of the weights

will not change the sign. So, when hw(x) is correct, the algorithm will leave w alone.

On the other hand, if fw(x) has the wrong sign, then L

= c(x)xi. So,

wi

using t = 1, the algorithm will update w  w + c(x)x. Note that this is exactly the

Perceptron algorithm. (Technically we must address the case that fw(x) = 0; in this case,

we should view fw as having the wrong sign just barely.)

= c(x) wx

wi

As a small modication to the above example, consider the same class of linear predic-

tors F but now modify the loss function to the hinge-loss L(fw(x), c(x)) = max(0, 1 

c(x)fw(x)). This loss function now requires fw(x) to have the correct sign and have mag-

nitude at least 1 in order to be zero. Hinge loss has the useful property that it is an upper

bound on error rate: for any sample S, the training error is at most (cid:80)

xS L(fw(x), c(x)).

With this loss function, stochastic gradient descent is called the margin perceptron algo-

rithm.

More generally, we could have a much more complex class F. For example, consider

a layered circuit of soft threshold gates. Each node in the circuit computes a linear func-

tion of its inputs and then passes this value through an activation function such as

a(z) = tanh(z) = (ez  ez)/(ez + ez). This circuit could have multiple layers with

the output of layer i being used as the input to layer i + 1. The vector w would be the

concatenation of all the weight vectors in the network. This is the idea of deep neural

networks discussed further in Section 5.15.

While it is dicult to give general guarantees on when stochastic gradient descent will

succeed in nding a hypothesis of low error on its training set S, Theorems 5.7 and 5.5

161

imply that if it does and if S is suciently large, we can be condent that its true error

will be low as well. Suppose that stochastic gradient descent is run on a machine where

each weight is a 64-bit oating point number. This means that its hypotheses can each

be described using 64n bits. If S has size at least 1

(cid:15) [64n ln(2) + ln(1/)], by Theorem 5.7

it is unlikely any such hypothesis of true error greater than (cid:15) will be consistent with the

sample, and so if it nds a hypothesis consistent with S, we can be condent its true error

(cid:0)64n ln(2) + ln(2/)(cid:1) then almost surely the

is at most (cid:15). Or, by Theorem 5.5, if |S|  1

2(cid:15)2

nal hypothesis h produced by stochastic gradient descent satises true error leas than

or equal to training error plus (cid:15).

5.14 Combining (Sleeping) Expert Advice

Imagine you have access to a large collection of rules-of-thumb that specify what to

predict in dierent situations. For example, in classifying news articles, you might have

one that says if the article has the word football, then classify it as sports and another

that says if the article contains a dollar gure, then classify it as business. In predicting

the stock market, these could be dierent economic indicators. These predictors might

at times contradict each other, e.g., a news article that has both the word football and

a dollar gure, or a day in which two economic indicators are pointing in dierent direc-

tions. It also may be that no predictor is perfectly accurate with some much better than

others. We present here an algorithm for combining a large number of such predictors

with the guarantee that if any of them are good, the algorithm will perform nearly as well

as each good predictor on the examples on which that predictor res.

Formally, dene a sleeping expert to be a predictor h that on any given example x

either makes a prediction on its label or chooses to stay silent (asleep). We will think of

them as black boxes. Now, suppose we have access to n such sleeping experts h1, . . . , hn,

and let Si denote the subset of examples on which hi makes a prediction (e.g., this could

be articles with the word football in them). We consider the online learning model,

and let mistakes(A, S) denote the number of mistakes of an algorithm A on a sequence

of examples S. Then the guarantee of our algorithm A will be that for all i

E(cid:0)mistakes(A, Si)(cid:1)  (1 + (cid:15))  mistakes(hi, Si) + O (cid:0) log n

(cid:15)

(cid:1)

where (cid:15) is a parameter of the algorithm and the expectation is over internal randomness

in the randomized algorithm A.

As a special case, if h1, . . . , hn are concepts from a concept class H, and so they all

make predictions on every example, then A performs nearly as well as the best concept

in H. This can be viewed as a noise-tolerant version of the Halving Algorithm of Section

5.8.2 for the case that no concept in H is perfect. The case of predictors that make

predictions on every example is called the problem of combining expert advice, and the

more general case of predictors that sometimes re and sometimes are silent is called the

162

sleeping experts problem.

Combining Sleeping Experts Algorithm:

Initialize each expert hi with a weight wi = 1. Let (cid:15)  (0, 1). For each example x, do the

following:

1. [Make prediction] Let Hx denote the set of experts hi that make a prediction on x, and

wj. Choose hi  Hx with probability pix = wi/wx and predict hi(x).

let wx = (cid:80)

hj Hx

2. [Receive feedback] Given the correct label, for each hi  Hx let mix = 1 if hi(x) was

incorrect, else let mix = 0.

3. [Update weights] For each hi  Hx, update its weight as follows:

Let rix =

(cid:16)(cid:80)

hj Hx

(cid:17)

pjxmjx

/(1 + (cid:15))  mix.

Update wi  wi(1 + (cid:15))rix.

hj Hx

Note that (cid:80)

pjxmjx represents the algorithms probability of making a mis-

take on example x. So, hi is rewarded for predicting correctly (mix = 0) especially

when the algorithm had a high probability of making a mistake, and hi is penal-

ized for predicting incorrectly (mix = 1) especially when the algorithm had a low

probability of making a mistake.

For each hi (cid:54) Hx, leave wi alone.

Theorem 5.22 For any set of n sleeping experts h1, . . . , hn, and for any sequence of

examples S, the Combining Sleeping Experts Algorithm A satises for all i:

E(cid:0)mistakes(A, Si)(cid:1)  (1 + (cid:15))  mistakes(hi, Si) + O (cid:0) log n

(cid:15)

(cid:1)

where Si = {x  S : hi  Hx}.

Proof: Consider sleeping expert hi. The weight of hi after the sequence of examples S

is exactly:

wi = (1 + (cid:15))

(cid:104)(cid:16)(cid:80)

(cid:80)

xSi

hj Hx

pjxmjx

(cid:17)

/(1+(cid:15))mix

(cid:105)

= (1 + (cid:15))E[mistakes(A,Si)]/(1+(cid:15))mistakes(hi,Si).

Let w = (cid:80)

j wj. Clearly wi  w. Therefore, taking logs, we have:

E(cid:0)mistakes(A, Si)(cid:1)/(1 + (cid:15))  mistakes(hi, Si)  log1+(cid:15) w.

So, using the fact that log1+(cid:15) w = O( log W

(cid:15)

),

E(cid:0)mistakes(A, Si)(cid:1)  (1 + (cid:15))  mistakes(hi, Si) + O (cid:0) log w

(cid:15)

(cid:1) .

163

Initially, w = n. To prove the theorem, it is enough to prove that w never increases. To

do so, we need to show that for each x, (cid:80)

wi, or equivalently

hiHx

hiHx

dividing both sides by (cid:80)

wj that (cid:80)

i pix(1 + (cid:15))rix  1, where for convenience we

dene pix = 0 for hi (cid:54) Hx.

wi(1 + (cid:15))rix  (cid:80)

hj Hx

For this we will use the inequalities that for , z  [0, 1], z  1  (1  )z and

z  1 + (1  )z/. Specically, we will use  = (1 + (cid:15))1. We now have:

pix(1 + (cid:15))rix =

(cid:88)

pixmix((cid:80)

j pjxmjx)

(cid:88)

i

i

(cid:88)

(cid:16)

pix

1  (1  )mix





i

(cid:32)

(cid:88)

i

(cid:32)

(cid:17)

1 + (1  )

(cid:33)(cid:33)

pjxmjx

(cid:32)

(cid:88)

j

(cid:33)

pix

 (1  )

(cid:88)

i

pixmix + (1  )

(cid:88)

pix

(cid:88)

pjxmjx

i

j

= 1  (1  )

(cid:88)

i

= 1,

pixmix + (1  )

(cid:88)

j

pjxmjx

where the second-to-last line follows from using (cid:80)

increases and the bound follows as desired.

i pix = 1 in two places. So w never

5.15 Deep Learning

Deep learning, or deep neural networks, refers to training many-layered networks of

nonlinear computational units.

Each computational unit or gate works as follows: there are a set of wires bringing

inputs to the gate. Each wire has a weight; the gates output is a real number obtained

by applying a non-linear activation function g : R  R to the the weighted sum of the

input values. The activation function g is generally the same for all gates in the network,

though, the number of inputs to individual gates may dier.

The input to the network is an example x  Rd. The rst layer of the network

transforms the example into a new vector f1(x). Then the second layer transforms f1(x)

into a new vector f2(f1(x)), and so on. Finally, the kth layer outputs the nal prediction

f (x) = fk(fk1(. . . (f1(x)))).

In supervised learning, we are given training examples x1, x2, . . . , and corresponding

labels c(x1), c(x2), . . .. The training process nds a set of weights of all wires so as to

minimize the error: (f0(x1  c(x1))2 + (f0(x2)  c(x2))2 +    . (One could alternatively

aim to minimize other quantities besides the sum of squared errors of training examples.)

Often training is carried out by running stochastic gradient descient, i.e., doing stochastic

gradient descent in the weights space.

164

The motivation for deep learning is that often we are interested in data, such as images,

that are given to us in terms of very low-level features, such as pixel intensity values. Our

goal is to achieve some higher-level understanding of each image, such as what objects

are in the image and what they are doing. To do so, it is natural to rst convert the given

low-level representation into one of higher-level features. That is what the layers of the

network aim to do. Deep learning is also motivated by multi-task learning, with the idea

that a good higher-level representation of data should be useful for a wide range of tasks.

Indeed, a common use of deep learning for multi-task learning is to share initial levels of

the network across tasks.

A typical architecture of a deep neural network consists of layers of logic units. In a

fully connected layer, the output of each gate in the layer is connected to the input of

every gate in the next layer. However, if the input is an image one might like to recognize

features independent of where they are located in the image. To achieve this one often

uses a number of convolution layers. In a convolution layer, each gate gets inputs from a

small k  k grid where k may be 5 to 10. There is a gate for each k  k square array of

the image. The weights on each gate are tied together so that each gate recognizes the

same feature. There will be several such collections of gates, so several dierent features

can be learned. Such a level is called a convolution level and the fully connected layers

are called autoencoder levels. A technique called pooling is used to keep the number of

gates reasonable. A small k  k grid with k typically set to two is used to scan a layer.

The stride is set so the grid will provide a non overlapping cover of the layer. Each k  k

input grid will be reduced to a single cell by selecting the maximum input value or the

average of the inputs. For k = 2 this reduces the number of cells by a factor of four.

Deep learning networks are trained by stochastic gradient descent (Section 5.13), some-

times called back propagation in the network context. An error function is constructed

and the weights are adjusted using the derivative of the error function. This requires that

the error function be dierentiable. A smooth threshold is used such as

tanh(x) =

ex  ex

ex + ex

where



x

ee  ee

ex + ex = 1 

(cid:19)2

(cid:18)ex  ex

ex + ex

or sigmod(x) = 1

1+ex where

 sigmod(x)

x

=

ex

(1 + ex)2 = sigmod(x)

ex

1 + ex = sigmoid(x)(cid:0)1  sigmoid(x)(cid:1).

In fact the function

ReLU (x) =

(cid:26) x x  0

0 otherwise

where

ReLU(x)

x

=

(cid:26) 1 x  0

0 otherwise

seems to work well even though its derivative at x = 0 is undened. An advantage of

ReLU over sigmoid is that ReLU does not saturate far from the origin.

165













Each gate is connected to a

k  k grid. Weights are tied

together.

Second set of gates each

connected to a k  k grid.

Weights are tied together.

Figure 5.7: Convolution layers

W1

W2

W3

W4

W5

W6

Figure 5.8: A deep learning fully connected network.

166

W1

W2

W1

W2

W3

(a)

(b)

Figure 5.9: Autoencoder technique used to train one level at a time. In the Figure 5.9

(a) train W1 and W2. Then in Figure 5.9 (b), freeze W1 and train W2 and W3. In this

way one trains one set of weights at a time.

Training a deep learning network of 7 or 8 levels using gradient descent can be compu-

tationally expensive.26 To address this issue one can train one level at a time on unlabeled

data using an idea called autoencoding. There are three levels, the input, a middle level

called the hidden level, and an output level as shown in Figure 5.9a. There are two sets

of weights. W1 is the weights of the hidden level gates and W2 is W T

1 . Let x be the input

pattern and y be the output. The error is |x  y|2. One uses gradient descent to reduce

the error. Once the weights W1 are determined they are frozen and a second hidden level

of gates is added as in Figure 5.9 b. In this network W3 = W T

2 and stochastic gradient

descent is again used this time to determine W2. In this way one level of weights is trained

at a time.

The output of the hidden gates is an encoding of the input. An image might be a

108 dimensional input and there may only be 105 hidden gates. However, the number of

images might be 107 so even though the dimension of the hidden layer is smaller than the

dimension of the input, the number of possible codes far exceeds the number of inputs

and thus the hidden layer is a compressed representation of the input. If the hidden layer

were the same dimension as the input layer one might get the identity mapping. This

does not happen for gradient descent starting with random weights.

The output layer of a deep network typically uses a softmax procedure. Softmax is

a generalization of logistic regression where given a set of vectors {x1, x2, . . . xn} with

labels l1, l2, . . . ln, li  {0, 1} and with a weight vector w we dene the probability that

26In the image recognition community, researchers work with networks of 150 levels. The levels tend

to be convolution rather than fully connected.

167

the label l given x equals 0 or 1 by

and

Prob(l = 1|x) =

1

1 + ewTx

= (wTx)

Prob(l = 0|x) = 1  Prob(l = 1/x)

where  is the sigmoid function.

Dene a cost function

J(w) =

(cid:88)

(cid:16)

i

li log(Prob(l = 1|x)) + (1  li) log(1  Prob(l = 1|x))

(cid:17)

and compute w to minimize J(x). Then

J(w) =

(cid:88)

(cid:16)

(cid:17)

li log((wTx)) + (1  li) log(1  (wTx))

i

Since (wTx)

Thus

wj

= (wTx)(1  (wTx))xj, it follows that  log((wTx))

wj

= (wTx)(1(wTx))xj

(wTx)

,

(cid:88)

(cid:18)

li

(wTx)(1  (wTx))

(wTx)

xj  (1  li)

(1  (wTx))(wTx)

1  (wTx)

xj

(cid:19)

J

wj

=

=

=

=

i

(cid:88)

(cid:16)

i

(cid:88)

(cid:16)

i

(cid:88)

(cid:16)

i

li(1  (wTx))xj  (1  li)(wTx)xj

(cid:17)

(lixj  li(wTx)xj  (wTx)xj + li(wTx)xj

(cid:17)

li  (wTx)

(cid:17)

xj.

Softmax is a generalization of logistic regression to multiple classes. Thus, the labels

li take on values {1, 2, . . . , k}. For an input x, softmax estimates the probability of each

label. The hypothesis is of the form

hw(x) =











Prob(l = 1|x, w1)

Prob(l = 2|x, w2)

...

Prob(l = k|x, wk)











=

1

i=1 ewT

i x

(cid:80)k











ewT

1 x

ewT

2 x

...

ewT

k x











where the matrix formed by the weight vectors is

W = (w1, w2, . . . , wk)T

168

convolution

pooling

Image

Convolution levels

Fully connected levels

Softmax

Figure 5.10: A convolution network

169

W is a matrix since for each label li, there is a vector wi of weights.

Consider a set of n inputs {x1, x2, . . . , xn}. Dene

(cid:26) 1 if l = k

(l = k) =

0 otherwise

and

J(W ) =

n

(cid:88)

k

(cid:88)

i=1

j=1

(li = j) log

ewT

j xi

h=1 ewT

h xi

(cid:80)k

.

The derivative of the cost function with respect to the weights is

wiJ(W ) = 

n

(cid:88)

j=1

(cid:0)(lj = k)  Prob(lj = k)|xj, W (cid:1).

xj

Note wiJ(W ) is a vector. Since wi is a vector, each component of wiJ(W ) is the

derivative with respect to one component of the vector wi.

Over tting is a major concern in deep learning since large networks can have hun-

In image recognition, the number of training images can

dreds of millions of weights.

be signicantly increased by random jittering of the images. Another technique called

dropout randomly deletes a fraction of the weights at each training iteration. Regulariza-

tion is used to assign a cost to the size of weights and many other ideas are being explored.

Deep learning is an active research area. Some of the ideas being explored are what

do individual gates or sets of gates learn. If one trains a network twice from starting with

random sets of weights, do gates learn the same features? In image recognition, the early

convolution layers seem to learn features of images rather than features of the specic set

of images they are being trained with. Once a network is trained on say a set of images

one of which is a cat one can freeze the weights and then nd images that will map to

the activation vector generated by the cat image. One can take an artwork image and

separate the style from the content and then create an image using the content but a

dierent style [GEB15]. This is done by taking the activation of the original image and

moving it to the manifold of activation vectors of images of a given style. One can do

many things of this type. For example one can change the age of a child in an image

or change some other feature [GKL+15]. For more information about deep learning, see

[Ben09].27

5.15.1 Generative Adversarial Networks (GANs)

A method that is promising in trying to generate images that look real is to create code

that tries to discern between real images and synthetic images.

27See

also

the

tutorials:

http://deeplearning.net/tutorial/deeplearning.pdf

and

http://deeplearning.stanford.edu/tutorial/.

170

image

generator

real

image

synthetic

image

discriminator

One rst trains the synthetic image discriminator to distinguish between real images and

synthetic ones. Then one trains the image generator to generate images that the discrim-

inator believes are real images. Alternating the training between the two units ends up

forcing the image generator to produce real looking images. This is the idea of Generative

Adversarial Networks.

There are many possible applications for this technique. Suppose you wanted to train

a network to translate from English to German. First train a discriminator to determine

if a sentence is a real sentence in German as opposed to a synthetic sentence. Then train

a translator for English to German and a translator from German to English.

translate

to German

translate

to English

discriminator

5.16 Further Current Directions

We now briey discuss a few additional current directions in machine learning, focusing

on semi-supervised learning, active learning, and multi-task learning.

5.16.1 Semi-Supervised Learning

Semi-supervised learning refers to the idea of trying to use a large unlabeled data set U to

augment a given labeled data set L in order to produce more accurate rules than would

have been achieved using just L alone. The motivation is that in many settings (e.g.,

document classication, image classication, speech recognition), unlabeled data is much

more plentiful than labeled data, so one would like to make use of it if possible. Of course,

unlabeled data is missing the labels! Nonetheless it often contains information that an

171

algorithm can take advantage of.

As an example, suppose one believes the target function is a linear separator that

separates most of the data by a large margin. By observing enough unlabeled data to es-

timate the probability mass near to any given linear separator, one could in principle then

discard separators in advance that slice through dense regions and instead focus attention

on just those that indeed separate most of the distribution by a large margin. This is the

high level idea behind a technique known as Semi-Supervised SVMs. Alternatively, sup-

pose data objects can be described by two dierent kinds of features (e.g., a webpage

could be described using words on the page itself or using words on links pointing to the

page), and one believes that each kind should be sucient to produce an accurate classi-

er. Then one might want to train a pair of classiers (one on each type of feature) and

use unlabeled data for which one is condent but the other is not to bootstrap, labeling

such examples with the condent classier and then feeding them as training data to the

less-condent one. This is the high-level idea behind a technique known as Co-Training.

Or, if one believes similar examples should generally have the same label, one might

construct a graph with an edge between examples that are suciently similar, and aim for

a classier that is correct on the labeled data and has a small cut value on the unlabeled

data; this is the high-level idea behind graph-based methods.

A formal model: The batch learning model introduced in Sections 5.1 and 5.6 in essence

assumes that ones prior beliefs about the target function be described in terms of a class

of functions H. In order to capture the reasoning used in semi-supervised learning, we

need to also describe beliefs about the relation between the target function and the data

distribution. A clean way to do this is via a notion of compatibility  between a hypoth-

esis h and a distribution D. Formally,  maps pairs (h, D) to [0, 1] with (h, D) = 1

meaning that h is highly compatible with D and (h, D) = 0 meaning that h is very

incompatible with D. The quantity 1  (h, D) is called the unlabeled error rate of h, and

denoted errunl(h). Note that for  to be useful, it must be estimatable from a nite sam-

ple; to this end, let us further require that  is an expectation over individual examples.

That is, overloading notation for convenience, we require (h, D) = ExD[(h, x)], where

 : H  X  [0, 1].

For instance, suppose we believe the target should separate most data by margin .

We can represent this belief by dening (h, x) = 0 if x is within distance  of the de-

cision boundary of h, and (h, x) = 1 otherwise. In this case, errunl(h) will denote the

probability mass of D within distance  of hs decision boundary. As a dierent exam-

ple, in co-training, we assume each example can be described using two views that

each are sucient for classication; that is, there exist c

1, c

2 such that for each example

x = (cid:104)x1, x2(cid:105) we have c

2(x2). We can represent this belief by dening a hypothesis

h = (cid:104)h1, h2(cid:105) to be compatible with an example (cid:104)x1, x2(cid:105) if h1(x1) = h2(x2) and incompatible

otherwise; errunl(h) is then the probability mass of examples on which h1 and h2 disagree.

1(x1) = c

172

As with the class H, one can either assume that the target is fully compatible (i.e.,

errunl(c) = 0) or instead aim to do well as a function of how compatible the target is.

The case that we assume c  H and errunl(c) = 0 is termed the doubly realizable

case. The concept class H and compatibility notion  are both viewed as known.

Intuition: In this framework, the way that unlabeled data helps in learning can be in-

tuitively described as follows. Suppose one is given a concept class H (such as linear

separators) and a compatibility notion  (such as penalizing h for points within distance

 of the decision boundary). Suppose also that one believes c  H (or at least is close)

and that errunl(c) = 0 (or at least is small). Then, unlabeled data can help by allowing

one to estimate the unlabeled error rate of all h  H, thereby in principle reducing the

search space from H (all linear separators) down to just the subset of H that is highly

compatible with D. The key challenge is how this can be done eciently (in theory,

in practice, or both) for natural notions of compatibility, as well as identifying types of

compatibility that data in important problems can be expected to satisfy.

A theorem: The following is a semi-supervised analog of our basic sample complexity

theorem, Theorem 5.3. First, x some set of functions H and compatibility notion .

Given a labeled sample L, dene (cid:99)err(h) to be the fraction of mistakes of h on L. Given

an unlabeled sample U , dene (h, U ) = ExU [(h, x)] and dene (cid:99)errunl(h) = 1(h, U ).

That is, (cid:99)err(h) and (cid:99)errunl(h) are the empirical error rate and unlabeled error rate of h,

respectively. Finally, given  > 0, dene HD,() to be the set of functions f  H such

that errunl(f )  .

Theorem 5.23 If c  H then with probability at least 1  , for labeled set L and

unlabeled set U drawn from D, the h  H that optimizes (cid:99)errunl(h) subject to (cid:99)err(h) = 0

will have errD(h)  (cid:15) for

|U | 

2

(cid:15)2

(cid:20)

ln |H| + ln

(cid:21)

4



, and |L| 

1

(cid:15)

(cid:20)

ln |HD,(errunl(c) + 2(cid:15))| + ln

(cid:21)

.

2



Equivalently, for |U | satisfying this bound, for any |L|, whp the h  H that minimizes

(cid:99)errunl(h) subject to (cid:99)err(h) = 0 has

(cid:20)

ln |HD,(errunl(c) + 2(cid:15))| + ln

errD(h) 

(cid:21)

.

1

|L|

2



Proof: By Hoeding bounds, |U | is suciently large so that with probability at least

1  /2, all h  H have | (cid:99)errunl(h)  errunl(h)|  (cid:15). Thus we have:

{f  H : (cid:99)errunl(f )  errunl(c) + (cid:15)}  HD,(errunl(c) + 2(cid:15)).

The given bound on |L| is sucient so that with probability at least 1  , all h  H with

(cid:99)err(h) = 0 and (cid:99)errunl(h)  errunl(c) + (cid:15) have errD(h)  (cid:15); furthermore, (cid:99)errunl(c) 

errunl(c) + (cid:15), so such a function h exists. Therefore, with probability at least 1  , the

h  H that optimizes (cid:99)errunl(h) subject to (cid:99)err(h) = 0 has errD(h)  (cid:15), as desired.

173

One can view Theorem 5.23 as bounding the number of labeled examples needed to learn

well as a function of the helpfulness of the distribution D with respect to . Namely,

a helpful distribution is one in which HD,() is small for  slightly larger than the

compatibility of the true target function, so we do not need much labeled data to identify a

good function among those in HD,(). For more information on semi-supervised learning,

see [BB10, BM98, CSZ06, Joa99, Zhu06, ZGL03].

5.16.2 Active Learning

Active learning refers to algorithms that take an active role in the selection of which ex-

amples are labeled. The algorithm is given an initial unlabeled set U of data points drawn

from distribution D and then interactively requests for the labels of a small number of

these examples. The aim is to reach a desired error rate (cid:15) using much fewer labels than

would be needed by just labeling random examples (i.e., passive learning).

(cid:15) log( 1

As a simple example, suppose that data consists of points on the real line and H =

{fa : fa(x) = 1 i x  a} for a  R. That is, H is the set of all threshold functions on

the line. It is not hard to show (see Exercise 5.2) that a random labeled sample of size

O( 1

 )) is sucient to ensure that with probability  1  , any consistent threshold

a(cid:48) has error at most (cid:15). Moreover, it is not hard to show that ( 1

(cid:15) ) random examples are

necessary for passive learning. However, with active learning we can achieve error (cid:15) using

only O(log( 1

 )) labels. Specically, rst draw an unlabeled sample U of size

(cid:15) log( 1

O( 1

if these are both negative

then output a(cid:48) = , and if these are both positive then output a(cid:48) = . Otherwise (the

leftmost is negative and the rightmost is positive), perform binary search to nd two ad-

jacent examples x, x(cid:48) such that x is negative and x(cid:48) is positive, and output a(cid:48) = (x + x(cid:48))/2.

This threshold a(cid:48) is consistent with the labels on the entire set U , and so by the above

argument, has error  (cid:15) with probability  1  .

 )). Then query the leftmost and rightmost points:

(cid:15) ) + log log( 1

The agnostic case, where the target need not belong in the given class H is quite a bit

more subtle, and addressed in a quite general way in the A2 Agnostic Active learning

algorithm [BBL09]. For more information on active learning, see [Das11, BU14].

5.16.3 Multi-Task Learning

In this chapter we have focused on scenarios where our goal is to learn a single target

function c. However, there are also scenarios where one would like to learn multiple target

functions c

n. If these functions are related in some way, then one could hope to

do so with less data per function than one would need to learn each function separately.

This is the idea of multi-task learning.

2, . . . , c

1, c

One natural example is object recognition. Given an image x, c

1(x) might be 1 if x is

a coee cup and 0 otherwise; c

3(x) might

be 1 if x is a laptop and 0 otherwise. These recognition tasks are related in that image

2(x) might be 1 if x is a pencil and 0 otherwise; c

174

features that are good for one task are likely to be helpful for the others as well. Thus,

one approach to multi-task learning is to try to learn a common representation under

which each of the target functions can be described as a simple function. Another natural

example is personalization. Consider a speech recognition system with n dierent users.

In this case there are n target tasks (recognizing the speech of each user) that are clearly

related to each other. Some good references for multi-task learning are [TM95, Thr96].

5.17 Bibliographic Notes

The basic theory underlying learning in the distributional setting was developed by Vap-

nik [Vap82], Vapnik and Chervonenkis [VC71], and Valiant [Val84]. The connection of

this to the notion of Occams razor is due to [BEHW87]. For more information on uniform

convergence, regularization and complexity penalization, see [Vap98]. The Perceptron al-

gorithm for online learning of linear separators was rst analyzed by Block [Blo62] and

Noviko [Nov62]; the proof given here is from [MP69]. A formal description of the on-

line learning model and its connections to learning in the distributional setting is given

in [Lit87]. Support Vector Machines and their connections to kernel functions were rst

introduced by [BGV92], and extended by [CV95], with analysis in terms of margins given

by [STBWA98]. For further reading on SVMs, learning with kernel functions, and regu-

larization, see [SS01]. VC dimension is due to Vapnik and Chervonenkis [VC71] with the

results presented here given in Blumer, Ehrenfeucht, Haussler and Warmuth [BEHW89].

Boosting was rst introduced by Schapire [Sch90], and Adaboost and its guarantees are

due to Freund and Schapire [FS97] . Analysis of the problem of combining expert advice

was given by Littlestone and Warmuth [LW94] and Cesa-Bianchi et al. [CBFH+97]; the

analysis of the sleeping experts problem given here is from [BM07].

175

5.18 Exercises

Exercise 5.1 (Section 5.5 and 5.6) Consider the instance space X = {0, 1}d and let

H be the class of 3-CNF formulas. That is, H is the set of concepts that can be described

as a conjunction of clauses where each clause is an OR of up to 3 literals. (These are also

called 3-SAT formulas). For example c might be (x1x2x3)(x2x4)(x1x3)(x2x3x4).

Assume we are in the PAC learning setting, so examples are drawn from some underlying

distribution D and labeled by some 3-CNF formula c.

1. Give a number of samples m that would be sucient to ensure that with probability

 1  , all 3-CNF formulas consistent with the sample have error at most (cid:15) with

respect to D.

2. Give a polynomial-time algorithm for PAC-learning the class of 3-CNF formulas.

Exercise 5.2 (Section 5.5) Consider the instance space X = R, and the class of func-

tions H = {fa : fa(x) = 1 i x  a} for a  R. That is, H is the set of all threshold

functions on the line. Prove that for any distribution D, a sample S of size O( 1

 ))

is sucient to ensure that with probability  1  , any fa(cid:48) such that errS(fa(cid:48)) = 0 has

errD(fa(cid:48))  (cid:15). Note that you can answer this question from rst principles, without using

the concept of VC-dimension.

(cid:15) log( 1

Exercise 5.3 (Perceptron; Section 5.8.3) Consider running the Perceptron algorithm

in the online model on some sequence of examples S. Let S(cid:48) be the same set of examples

as S but presented in a dierent order. Does the Perceptron algorithm necessarily make

the same number of mistakes on S as it does on S(cid:48)? If so, why? If not, show such an S

and S(cid:48) (consisting of the same set of examples in a dierent order) where the Perceptron

algorithm makes a dierent number of mistakes on S(cid:48) than it does on S.

Exercise 5.4 (representation and linear separators) Show that any disjunction (see

Section 5.6.1) over {0, 1}d can be represented as a linear separator. Show that moreover

the margin of separation is (1/

d).



Exercise 5.5 (Linear separators; easy) Show that the parity function on d  2

Boolean variables cannot be represented by a linear threshold function. The parity function

is 1 if and only if an odd number of inputs is 1.

Exercise 5.6 (Perceptron; Section 5.8.3) We know the Perceptron algorithm makes

at most 1/2 mistakes on any sequence of examples that is separable by margin  (we

assume all examples are normalized to have length 1). However, it need not nd a sep-

arator of large margin. If we also want to nd a separator of large margin, a natural

alternative is to update on any example x such that f (x)(w  x) < 1; this is called the

margin perceptron algorithm.

1. Argue why margin perceptron is equivalent to running stochastic gradient descent on

the class of linear predictors (fw(x) = w  x) using hinge loss as the loss function

and using t = 1.

176

2. Prove that on any sequence of examples that are separable by margin , this algorithm

will make at most 3/2 updates.

3. In part 2 you probably proved that each update increases |w|2 by at most 3. Use

this (and your result from part 2) to conclude that if you have a dataset S that

is separable by margin , and cycle through the data until the margin perceptron

algorithm makes no more updates, that it will nd a separator of margin at least

/3.

Exercise 5.7 (Decision trees, regularization; Section 5.6) Pruning a decision tree:

Let S be a labeled sample drawn iid from some distribution D over {0, 1}n, and suppose

we have used S to create some decision tree T . However, the tree T is large, and we are

concerned we might be overtting. Give a polynomial-time algorithm for pruning T that

nds the pruning h of T that optimizes the right-hand-side of Corollary 5.8, i.e., that for

a given  > 0 minimizes:

(cid:115)

errS(h) +

size(h) ln(4) + ln(2/)

2|S|

.

To discuss this, we need to dene what we mean by a pruning of T and what we mean

by the size of h. A pruning h of T is a tree in which some internal nodes of T have been

turned into leaves, labeled + or  depending on whether the majority of examples in

S that reach that node are positive or negative. Let size(h) = L(h) log(n) where L(h) is

the number of leaves in h.

Hint #1: it is sucient, for each integer L = 1, 2, . . . , L(T ), to nd the pruning of T

with L leaves of lowest empirical error on S, that is, hL = argminh:L(h)=LerrS(h). Then

you can just plug them all into the displayed formula above and pick the best one.

Hint #2: use dynamic programming.

Exercise 5.8 (Decision trees, sleeping experts; Sections 5.6, 5.14) Pruning a

Decision Tree Online via Sleeping Experts: Suppose that, as in the above problem, we are

given a decision tree T , but now we are faced with a sequence of examples that arrive

online. One interesting way we can make predictions is as follows. For each node v of

T (internal node or leaf ) create two sleeping experts: one that predicts positive on any

example that reaches v and one that predicts negative on any example that reaches v. So,

the total number of sleeping experts is O(L(T )).

1. Say why any pruning h of T , and any assignment of {+, } labels to the leaves of h,

corresponds to a subset of sleeping experts with the property that exactly one sleeping

expert in the subset makes a prediction on any given example.

2. Prove that for any sequence S of examples, and any given number of leaves L, if

, then the expected error

we run the sleeping-experts algorithm using (cid:15) =

rate of the algorithm on S (the total number of mistakes of the algorithm divided by

(cid:113) L log(L(T ))

|S|

177

|S|) will be at most errS(hL) + O(

is the pruning of T with L leaves of lowest error on S.

(cid:113) L log(L(T ))

|S|

), where hL = argminh:L(h)=LerrS(h)

3. In the above question, we assumed L was given. Explain how we can remove this as-

(cid:104)

by instantiating

sumption and achieve a bound of minL

L(T ) copies of the above algorithm (one for each value of L) and then combining

these algorithms using the experts algorithm (in this case, none of them will be

sleeping).

(cid:113) L log(L(T ))

|S|

errS(hL) + O(

(cid:105)

)

Exercise 5.9 Kernels; (Section 5.3) Prove Theorem 5.2.

Exercise 5.10 What is the VC-dimension of right corners with axis aligned edges that

are oriented with one edge going to the right and the other edge going up?

Exercise 5.11 (VC-dimension; Section 5.11) What is the VC-dimension V of the

class H of axis-parallel boxes in Rd? That is, H = {ha,b : a, b  Rd} where ha,b(x) = 1

if ai  xi  bi for all i = 1, . . . , d and ha,b(x) = 1 otherwise.

1. Prove that the VC-dimension is at least your chosen V by giving a set of V points

that is shattered by the class (and explaining why it is shattered).

2. Prove that the VC-dimension is at most your chosen V by proving that no set of

V + 1 points can be shattered.

Exercise 5.12 (VC-dimension, Perceptron, and Margins; Sections 5.8.3, 5.11)

Say that a set of points S is shattered by linear separators of margin  if every labeling

of the points in S is achievable by a linear separator of margin at least . Prove that no

set of 1/2 + 1 points in the unit ball is shattered by linear separators of margin .

Hint: think about the Perceptron algorithm and try a proof by contradiction.

Exercise 5.13 (Linear separators) Suppose the instance space X is {0, 1}d and con-

sider the target function c that labels an example x as positive if the least index i for

which xi = 1 is odd, else labels x as negative. In other words, c(x) = if x1 = 1 then

positive else if x2 = 1 then negative else if x3 = 1 then positive else ... else negative.

Show that the rule can be represented by a linear threshold function.

Exercise 5.14 (Linear separators; harder) Prove that for the problem of Exercise

5.13, we cannot have a linear separator with margin at least 1/f (d) where f (d) is bounded

above by a polynomial function of d.

Exercise 5.15 VC-dimension Prove that the VC-dimension of circles in the plane is

three.

Exercise 5.16 VC-dimension Show that the VC-dimension of arbitrary right triangles

in the plane is seven.

178

Exercise 5.17 VC-dimension Prove that the VC-dimension of triangles in the plane

is seven.

Exercise 5.18 VC-dimension Prove that the VC dimension of convex polygons in the

plane is innite.

Exercise 5.19 At present there are many interesting research directions in deep learning

that are being explored. This exercise focuses on whether gates in networks learn the same

thing independent of the architecture or how the network is trained. On the web there

are several copies of Alexnet that have been trained starting from dierent random initial

weights. Select two copies and form a matrix where the columns of the matrix correspond

to gates in the rst copy of Alexnet and the rows of the matrix correspond to gates of

the same level in the second copy. The ijth entry of the matrix is the covariance of the

activation of the jth gate in the rst copy of Alexnet with the ith gate in the second copy.

The covariance is the expected value over all images in the data set.

1. Match the gates in the two copies of the network using a bipartite graph matching

algorithm. What is the fraction of matches that have a high covariance?

2. It is possible that there is no good one to one matching of gates but that some small

set of gates in the rst copy of the network learn what some small set of gates in the

second copy learn. Explore a clustering technique to match sets of gates and carry

out an experiment to do this.

Exercise 5.20

1. Input an image to a deep learning network. Reproduce the image from the activation

vector, aimage, it produced by inputting a random image and producing an activation

vector arandom. Then by gradient descent modify the pixels in the random image to

minimize the error function |aimage  arandom|2.

2. Train a deep learning network to produce an image from an activation network.

Exercise 5.21

1. Create and train a simple deep learning network consisting of a convolution level with

pooling, a fully connected level, and then softmax. Keep the network small. For input

data use the MNIST data set http://yann.lecun.com/exdb/mnist/ with 2828

images of digits. Use maybe 20 channels for the convolution level and 100 gates for

the fully connected level.

2. Create and train a second network with two fully connected levels, the rst level with

200 gates and the second level with 100 gates. How does the accuracy of the second

network compare to the rst?

179

3. Train the second network again but this time use the activation vector of the 100

gate level and train the second network to produce that activation vector and only

then train the softmax. How does the accuracy compare to direct training of the

second network and the rst network?

convolution

rst network

second network

180

6 Algorithms for Massive Data Problems: Stream-

ing, Sketching, and Sampling

6.1 Introduction

This chapter deals with massive data problems where the input data is too large to be

stored in random access memory. One model for such problems is the streaming model,

where n data items a1, a2, . . . , an arrive one at a time. For example, the ai might be

IP addresses being observed by a router on the internet. The goal is for our algorithm

to compute some statistics, property, or summary of these data items without using too

much memory, much less than n. More specically, we assume each ai itself is a b-bit

quantity where b is not too large. For example, each ai might be an integer in {1, . . . , m}

where m = 2b. Our goal will be to produce some desired output using space polynomial

in b and log n; see Figure 6.1.

For example, a very easy problem to solve in the streaming model is to compute the

sum of all the ai. If each ai is an integer between 1 and m = 2b, then the sum of all the ai

is an integer between 1 and mn and so the number of bits of memory needed to maintain

the sum is O(b + log n). A harder problem, which we discuss shortly, is computing the

number of distinct numbers in the input sequence.

One natural approach for tackling a range of problems in the streaming model is to

perform random sampling of the input on the y. To introduce the basic avor of

sampling on the y, consider a stream a1, a2, . . . , an from which we are to select an index

i with probability proportional to the value of ai. When we see an element, we do not

know the probability with which to select it since the normalizing constant depends on

all of the elements including those we have not yet seen. However, the following method

works. Let s be the sum of the ais seen so far. Maintain s and an index i selected

with probability ai

s . Initially i = 1 and s = a1. Having seen symbols a1, a2, . . . , aj, s will

equal a1 + a2 +    + aj and for each i in {1, . . . , j}, the selected index will be i with

s . On seeing aj+1, change the selected index to j + 1 with probability aj+1

probability ai

s+aj+1

and otherwise keep the same index as before with probability 1  aj+1

. If we change

s+aj+1

the index to j + 1, clearly it was selected with the correct probability. If we keep i as our

selection, then it will have been selected with probability

(cid:18)

1 

aj+1

s + aj+1

(cid:19) ai

s

=

s

s + aj+1

ai

s

=

ai

s + aj+1

which is the correct probability for selecting index i. Finally s is updated by adding aj+1

to s. This problem comes up in many areas such as sleeping experts where there is a

sequence of weights and we want to pick an expert with probability proportional to its

weight. The ais are the weights and the subscript i denotes the expert.

181

stream a1, a2, . . . , an

Algorithm

(low space)

some output

Figure 6.1: High-level representation of the streaming model

6.2 Frequency Moments of Data Streams

An important class of problems concerns the frequency moments of data streams. As

mentioned above, a data stream a1, a2, . . . , an of length n consists of symbols ai from

an alphabet of m possible symbols which for convenience we denote as {1, 2, . . . , m}.

Throughout this section, n, m, and ai will have these meanings and s (for symbol) will

denote a generic element of {1, 2, . . . , m}. The frequency fs of the symbol s is the number

of occurrences of s in the stream. For a nonnegative integer p, the pth frequency moment

of the stream is

m

(cid:88)

f p

s .

s=1

Note that the p = 0 frequency moment corresponds to the number of distinct symbols

occurring in the stream using the convention 00 = 0. The rst frequency moment is just

n, the length of the string. The second frequency moment, (cid:80)

s , is useful in computing

the variance of the stream, i.e., the average squared dierence from the average frequency.

s f 2

1

m

m

(cid:88)

(cid:16)

s=1

fs 

(cid:17)2

n

m

=

1

m

m

(cid:88)

(cid:18)

s=1

f 2

s  2

n

m

fs +

(cid:16) n

m

(cid:32)

(cid:17)2(cid:19)

=

1

m

m

(cid:88)

s=1

(cid:33)

f 2

s



n2

m2

In the limit as p becomes large,

ment(s).

(cid:19)1/p

(cid:18) m

(cid:80)

s=1

f p

s

is the frequency of the most frequent ele-

We will describe sampling based algorithms to compute these quantities for streaming

data shortly. First a note on the motivation for these problems. The identity and fre-

quency of the the most frequent item, or more generally, items whose frequency exceeds a

given fraction of n, is clearly important in many applications. If the items are packets on

a network with source and/or destination addresses, the high frequency items identify the

heavy bandwidth users. If the data consists of purchase records in a supermarket, the high

frequency items are the best-selling items. Determining the number of distinct symbols

is the abstract version of determining such things as the number of accounts, web users,

or credit card holders. The second moment and variance are useful in networking as well

as in database and other applications. Large amounts of network log data are generated

by routers that can record the source address, destination address, and the number of

packets for all the messages passing through them. This massive data cannot be easily

sorted or aggregated into totals for each source/destination. But it is important to know

182

if some popular source-destination pairs have a lot of trac for which the variance is one

natural measure.

6.2.1 Number of Distinct Elements in a Data Stream

Consider a sequence a1, a2, . . . , an of n elements, each ai an integer in the range 1 to m

where n and m are very large. Suppose we wish to determine the number of distinct ai in

the sequence. Each ai might represent a credit card number extracted from a sequence of

credit card transactions and we wish to determine how many distinct credit card accounts

there are. Note that this is easy to do in O(m) space by just storing a bit-vector that

records which elements have been seen so far and which have not. It is also easy to do in

O(n log m) space by storing a list of all distinct elements that have been seen. However,

our goal is to use space logarithmic in m and n. We rst show that this is impossible

using an exact deterministic algorithm. Any deterministic algorithm that determines the

number of distinct elements exactly must use at least m bits of memory on some input

sequence of length O(m). We then will show how to get around this problem using ran-

domization and approximation.

Lower bound on memory for exact deterministic algorithm

We show that any exact deterministic algorithm must use at least m bits of memory

on some sequence of length m + 1. Suppose we have seen a1, . . . , am, and suppose for sake

of contradiction that our algorithm uses less than m bits of memory on all such sequences.

There are 2m  1 possible subsets of {1, 2, . . . , m} that the sequence could contain and

yet only 2m1 possible states of our algorithms memory. Therefore there must be two

dierent subsets S1 and S2 that lead to the same memory state.

If S1 and S2 are of

dierent sizes, then clearly this implies an error for one of the input sequences. On the

other hand, if they are the same size, then if the next element is in S1 but not S2, the

algorithm will give the same answer in both cases and therefore must give an incorrect

answer on at least one of them.

Algorithm for the Number of distinct elements

To beat the above lower bound, consider approximating the number of distinct el-

ements. Our algorithm will produce a number that is within a constant factor of the

correct answer using randomization and thus a small probability of failure. First, the

idea: suppose the set S of distinct elements was itself chosen uniformly at random from

{1, . . . , m}. Let min denote the minimum element in S. What is the expected value of

min? If there was one distinct element, then its expected value would be roughly m

2 . If

there were two distinct elements, the expected value of the minimum would be roughly

m

3 . More generally, for a random set S, the expected value of the minimum is approxi-

mately m

min  1. This suggests

keeping track of the minimum element in O(log m) space and using this equation to give

|S|+1. See Figure 6.2. Solving min = m

|S|+1 yields |S| = m

183

|S| + 1 subsets

(cid:125)(cid:124)

(cid:123)

(cid:122)

m

|S|+1

Figure 6.2: Estimating the size of S from the minimum element in S which has value

approximately m

|S|+1. The elements of S partition the set {1, 2, . . . , m} into |S| + 1 subsets

each of size approximately m

|S|+1.

an estimate of |S|.

Converting the intuition into an algorithm via hashing

In general, the set S might not have been chosen uniformly at random.

If the el-

ements of S were obtained by selecting the |S| smallest elements of {1, 2, . . . , m}, the

above technique would give a very bad answer. However, we can convert our intuition

into an algorithm that works well with high probability on every sequence via hashing.

Specically, we will use a hash function h where

h : {1, 2, . . . , m}  {0, 1, 2, . . . , M  1} ,

and then instead of keeping track of the minimum element ai  S, we will keep track of

the minimum hash value. The question now is: what properties of a hash function do

we need? Since we need to store h, we cannot use a totally random mapping since that

would take too many bits. Luckily, a pairwise independent hash function, which can be

stored compactly is sucient.

We recall the formal denition of pairwise independence below. But rst recall that

a hash function is always chosen at random from a family of hash functions and phrases

like probability of collision refer to the probability in the choice of hash function.

2-Universal (Pairwise Independent) Hash Functions

A set of hash functions

H = (cid:8)h | h : {1, 2, . . . , m}  {0, 1, 2, . . . , M  1}(cid:9)

is 2-universal or pairwise independent if for all x and y in {1, 2, . . . , m} with x (cid:54)= y,

h(x) and h(y) are each equally likely to be any element of {0, 1, 2, . . . , M  1} and are

statistically independent. It follows that a set of hash functions H is 2-universal if and

only if for all x and y in {1, 2, . . . , m}, x (cid:54)= y, h(x) and h(y) are each equally likely to be

any element of {0, 1, 2, . . . , M  1}, and for all w, z we have:

(cid:0)h (x) = w and h (y) = z(cid:1) = 1

M 2 .

Prob

hH

184

We now give an example of a 2-universal family of hash functions. Let M be a prime

greater than m. For each pair of integers a and b in the range [0, M  1], dene a hash

function

hab (x) = ax + b

(mod M )

To store the hash function hab, store the two integers a and b. This requires only O(log M )

space. To see that the family is 2-universal note that h(x) = w and h(y) = z if and only

if

(cid:19) (cid:18) a

b

(cid:19)

(cid:18) w

z

=

(cid:19)

(mod M )

(cid:18) x 1

y 1

(cid:19)

(cid:18) x 1

y 1

If x (cid:54)= y, the matrix

is invertible modulo M .28 Thus

(cid:19)

(cid:18)a

b

=

(cid:18)x 1

y 1

(cid:19)1 (cid:18)w

z

(cid:19)

(mod M )

and for each (cid:0)w

z

(cid:1) there is a unique (cid:0)a

(cid:1). Hence

b

Prob(cid:0)h(x) = w and h(y) = z(cid:1) =

1

M 2

and H is 2-universal.

Analysis of distinct element counting algorithm

Let b1, b2, . . . , bd be the distinct values that appear in the input. Select h from the

2-universal family of hash functions H. Then the set S = {h(b1), h(b2), . . . , h(bd)} is a

set of d random and pairwise independent values from the set {0, 1, 2, . . . , M  1}. We

now show that M

min is a good estimate for d, the number of distinct elements in the input,

where min = min(S).

Lemma 6.1 With probability at least 2

smallest element of S.

3  d

M , we have d

6  M

min  6d, where min is the

Proof: First, we show that Prob (cid:0) M

independence.

min > 6d(cid:1) < 1

6 + d

M . This part does not require pairwise

Prob

(cid:18) M

min

(cid:19)

(cid:18)

> 6d

= Prob

min <

(cid:19)

M

6d

(cid:18)

= Prob

k, h (bk) <



d

(cid:88)

i=1

(cid:18)

Prob

h(bi) <

(cid:19)

M

6d

 d

(cid:33)

(cid:32)

(cid:100) M

6d (cid:101)

M

 d

(cid:19)

M

6d

(cid:18) 1

6d

(cid:19)

+

1

M



1

6

+

d

M

.

28The primality of M ensures that inverses of elements exist in Z 

M and M > m ensures that if x (cid:54)= y,

then x and y are not equal mod M .

185

t

M

6d

M

d

(cid:124)

(cid:125)

(cid:123)(cid:122)

Min here if

there exists

i, h(bi)  M

6d

6M

d

(cid:124)

(cid:125)

(cid:123)(cid:122)

Min here

if for all i,

h(bi)  6M

d

Figure 6.3: Location of the minimum in the distinct counting algorithm.

Next, we show that Prob (cid:0) M

min < d

6

(cid:1) < 1

First, Prob (cid:0) M

dene the indicator variable

min < d

6

(cid:1) = Prob (cid:0)min > 6M

d

6. This part will use pairwise independence.

(cid:1). For i = 1, 2, . . . , d,

d

(cid:1) = Prob (cid:0)k, h (bk) > 6M

and let

yi =

(cid:26) 0 if h (bi) > 6M

1 otherwise

d

y =

d

(cid:88)

i=1

yi.

We want to show that with good probability, we will see a hash value in [0, 6M

d ], i.e.,

that Prob(y = 0) is small. Now Prob (yi = 1)  6

d , and E (y)  6. For 2-way

independent random variables, the variance of their sum is the sum of their variances. So

Var (y) = dVar (y1). Further, since y1 is 0 or 1, Var(y1) = E (cid:2)(y1  E(y1))2(cid:3) = E(y2

1) 

E2(y1) = E(y1)  E2(y1)  E (y1) . Thus Var(y)  E (y). By the Chebyshev inequality,

d , E (yi)  6

Prob

(cid:18) M

min

<

(cid:19)

d

6

= Prob (cid:0)min > 6M

d

(cid:1) = Prob

(cid:18)

k h (bk) >

(cid:19)

6M

d

= Prob (y = 0)

 Prob (|y  E (y)|  E (y))



Var(y)

E2 (y)



1

E (y)



1

6

Since M

6  M

d

min > 6d with probability at most 1

min  6d with probability at least 2

6 + d

3  d

M .

M and M

min < d

6 with probability at most 1

6,

6.2.2 Number of Occurrences of a Given Element.

To count the number of occurrences of a given element in a stream requires at most

log n space where n is the length of the stream. Clearly, for any length stream that occurs

in practice, one can aord log n space. For this reason, the following material may never

be used in practice, but the technique is interesting and may give insight into how to solve

186

some other problem.

Consider a string of 0s and 1s of length n in which we wish to count the number of

occurrences of 1s. Clearly with log n bits of memory we could keep track of the exact

number of 1s. However, the number can be approximated with only log log n bits.

Let m be the number of 1s that occur in the sequence. Keep a value k such that 2k

is approximately the number m of occurrences. Storing k requires only log log n bits of

memory. The algorithm works as follows. Start with k=0. For each occurrence of a 1,

add one to k with probability 1/2k. At the end of the string, the quantity 2k  1 is the

estimate of m. To obtain a coin that comes down heads with probability 1/2k, ip a fair

coin, one that comes down heads with probability 1/2, k times and report heads if the fair

coin comes down heads in all k ips.

Given k, on average it will take 2k ones before k is incremented. Thus, the expected

number of 1s to produce the current value of k is 1 + 2 + 4 +    + 2k1 = 2k  1.

6.2.3 Frequent Elements

The Majority and Frequent Algorithms

First consider the very simple problem of n people voting. There are m candidates,

{1, 2, . . . , m}. We want to determine if one candidate gets a majority vote and if so

who. Formally, we are given a stream of integers a1, a2, . . . , an, each ai belonging to

{1, 2, . . . , m}, and want to determine whether there is some s  {1, 2, . . . , m} which oc-

curs more than n/2 times and if so which s. It is easy to see that to solve the problem

exactly on read-once streaming data with a deterministic algorithm, requires (min(n, m))

space. Suppose n is even and the last n/2 items are identical. Suppose also that after

reading the rst n/2 items, there are two dierent sets of elements that result in the same

In that case, a mistake would occur if the second half of the

content of our memory.

stream consists solely of an element that is in one set, but not in the other. If n/2  m

then there are at least 2m  1 possible subsets of the rst n/2 elements.

If n/2  m

(cid:1) subsets. By the above argument, the number of bits of mem-

then there are (cid:80)n/2

ory must be at least the base 2 logarithm of the number of subsets, which is (min(m, n)).

(cid:0)m

i

i=1

Surprisingly, we can bypass the above lower bound by slightly weakening our goal.

Again lets require that if some element appears more than n/2 times, then we must

output it. But now, let us say that if no element appears more than n/2 times, then our

algorithm may output whatever it wants, rather than requiring that it output no. That

is, there may be false positives, but no false negatives.

Majority Algorithm

Store a1 and initialize a counter to one. For each subsequent ai, if ai is the

187

same as the currently stored item, increment the counter by one. If it diers,

decrement the counter by one provided the counter is nonzero. If the counter

is zero, then store ai and set the counter to one.

To analyze the algorithm, it is convenient to view the decrement counter step as elim-

inating two items, the new one and the one that caused the last increment in the counter.

It is easy to see that if there is a majority element s, it must be stored at the end. If

not, each occurrence of s was eliminated; but each such elimination also causes another

item to be eliminated. Thus for a majority item not to be stored at the end, more than

n items must have eliminated, a contradiction.

Next we modify the above algorithm so that not just the majority, but also items

with frequency above some threshold are detected. More specically, the algorithm below

nds the frequency (number of occurrences) of each element of {1, 2, . . . , m} to within an

k+1. That is, for each symbol s, the algorithm produces a value fs in

additive term of

[fs  n

k+1, fs], where fs is the true number of occurrences of symbol s in the sequence.

It will do so using O(k log n + k log m) space by keeping k counters instead of just one

counter.

n

Algorithm Frequent

Maintain a list of items being counted. Initially the list is empty. For each

item, if it is the same as some item on the list, increment its counter by one.

If it diers from all the items on the list, then if there are less than k items

on the list, add the item to the list with its counter set to one. If there are

already k items on the list, decrement each of the current counters by one.

Delete an element from the list if its count becomes zero.

Theorem 6.2 At the end of Algorithm Frequent, for each s  {1, 2, . . . , m}, its counter

on the list fs satises fs  [fs  n

k+1, fs]. If some s does not occur on the list, its counter

is zero and the theorem asserts that fs  n

k+1.

Proof: The fact that fs  fs is immediate. To show fs  fs  n

k+1, view each decrement

counter step as eliminating some items. An item is eliminated if the current ai being read

is not on the list and there are already k symbols dierent from it on the list; in this case, ai

and k other distinct symbols are simultaneously eliminated. Thus, the elimination of each

occurrence of an s  {1, 2, . . . , m} is really the elimination of k + 1 items corresponding

to distinct symbols. Thus, no more than n/(k + 1) occurrences of any symbol can be

eliminated. It is clear that if an item is not eliminated, then it must still be on the list at

the end. This proves the theorem.

Theorem 6.2 implies that we can compute the true frequency of every s  {1, 2, . . . , m}

to within an additive term of n

k+1.

188

6.2.4 The Second Moment

This section focuses on computing the second moment of a stream with symbols from

{1, 2, . . . , m}. Let fs denote the number of occurrences of the symbol s in the stream,

and recall that the second moment of the stream is given by (cid:80)m

s . To calculate the

second moment, for each symbol s, 1  s  m, independently set a random variable xs

to 1 with probability 1/2. In particular, think of xs as the output of a random hash

function h(s) whose range is just the two buckets {1, 1}. For now, think of h as a fully

independent hash function. Maintain a sum by adding xs to the sum each time the symbol

s occurs in the stream. At the end of the stream, the sum will equal (cid:80)m

s=1 xsfs. The

expected value of the sum will be zero where the expectation is over the choice of the 1

value for the xs.

s=1 f 2

(cid:32) m

(cid:88)

E

(cid:33)

xsfs

= 0

Although the expected value of the sum is zero, its actual value is a random variable and

the expected value of the square of the sum is given by

s=1

(cid:32) m

(cid:88)

E

s=1

(cid:33)2

xsfs

= E

(cid:32) m

(cid:88)

s=1

(cid:33)

(cid:32)

sf 2

x2

s

+ 2E

(cid:88)

s(cid:54)=t

(cid:33)

xsxtfsft

=

m

(cid:88)

s=1

f 2

s ,

The last equality follows since E (xsxt) = E(xs)E(xt) = 0 for s (cid:54)= t, using pairwise

independence of the random variables. Thus

a =

(cid:33)2

xsfs

(cid:32) m

(cid:88)

s=1

is an unbiased estimator of (cid:80)m

this point we could use Markovs inequality to state that Prob(a  3 (cid:80)m

we want to get a tighter guarantee. To do so, consider the second moment of a:

s in that it has the correct expectation. Note that at

s )  1/3, but

s=1 f 2

s=1 f 2

E(a2) = E

(cid:32) m

(cid:88)

s=1

(cid:33)4

(cid:32)

xsfs

= E

(cid:88)

(cid:33)

xsxtxuxvfsftfufv

.

1s,t,u,vm

The last equality is by expansion. Assume that the random variables xs are 4-wise inde-

pendent, or equivalently that they are produced by a 4-wise independent hash function.

Then, since the xs are independent in the last sum, if any one of s, u, t, or v is distinct

from the others, then the expectation of the term is zero. Thus, we need to deal only

t for t (cid:54)= s and terms of the form x4

with terms of the form x2

s.

sx2

Each term in the above sum has four indices, s, t, u, v, and there are (cid:0)4

(cid:1) ways of

2

189

choosing two indices that have the same x value. Thus,

E(a2) 

(cid:19)

(cid:18)4

2

E

(cid:32) m

(cid:88)

m

(cid:88)

s=1

t=s+1

sx2

x2

t f 2

s f 2

t

(cid:33)

+ E

(cid:33)

sf 4

x4

s

(cid:32) m

(cid:88)

s=1

m

(cid:88)

m

(cid:88)

= 6

s f 2

f 2

t +

m

(cid:88)

s=1

f 4

s

t=s+1

s=1

(cid:32) m

(cid:88)

(cid:33)2

f 2

s

= 3E2(a).

 3

Therefore, V ar(a) = E(a2)  E2(a)  2E2(a).

s=1

Since the variance is comparable to the square of the expectation, repeating the process

several times and taking the average, gives high accuracy with high probability.

Theorem 6.3 The average x of r = 2

4-way independent random variables is

2 estimates a1, . . . , ar using independent sets of

Prob (|x  E(x)| > E(x)) <

V ar(x)

(cid:15)2E2(x)

 .

Proof: The proof follows from the fact that taking the average of r independent repe-

titions reduces variance by a factor of r, so that V ar(x)  2E2(x), and then applying

Chebyshevs inequality.

It remains to show that we can implement the desired 4-way independent random vari-

ables using O(log m) space. We earlier gave a construction for a pairwise-independent set

of hash functions; now we need 4-wise independence, though only into a range of {1, 1}.

Below we present one such construction.

Error-Correcting codes, polynomial interpolation and limited-way indepen-

dence

Consider the problem of generating a random m-dimensional vector x of 1s so that

any four coordinates are mutually independent. Such an m-dimensional vector may be

generated from a truly random seed of only O(log m) mutually independent bits. Thus,

we need only store the O(log m) bits and can generate any of the m coordinates when

needed. For any k, there is a nite eld F with exactly 2k elements, each of which can

be represented with k bits and arithmetic operations in the eld can be carried out in

O(k2) time. Here, k is the ceiling of log2 m. A basic fact about polynomial interpolation

is that a polynomial of degree at most three is uniquely determined by its value over

any eld F at four points. More precisely, for any four distinct points a1, a2, a3, a4 in F

and any four possibly not distinct values b1, b2, b3, b4 in F , there is a unique polynomial

f (x) = f0 + f1x + f2x2 + f3x3 of degree at most three, so that with computations done

190

over F , f (ai) = bi 1  i  4.

The denition of the pseudo-random 1 vector x with 4-way independence is simple.

Choose four elements f0, f1, f2, f3 at random from F and form the polynomial f (s) =

f0 + f1s + f2s2 + f3s3. This polynomial represents x as follows. For s = 1, 2, . . . , m, xs

is the leading bit of the k-bit representation of f (s).29 Thus, the m-dimensional vector x

requires only O(k) bits where k = (cid:100)log m(cid:101).

Lemma 6.4 The x dened above has 4-way independence.

Proof: Assume that the elements of F are represented in binary using 1 instead of the

traditional 0 and 1. Let s, t, u, and v be any four coordinates of x and let , , , and

 have values in 1. There are exactly 2k1 elements of F whose leading bit is  and

similarly for , , and . So, there are exactly 24(k1) 4-tuples of elements b1, b2, b3, and

b4 in F so that the leading bit of b1 is , the leading bit of b2 is , the leading bit of b3

is , and the leading bit of b4 is . For each such b1, b2, b3, and b4, there is precisely one

polynomial f so that f (s) = b1, f (t) = b2, f (u) = b3, and f (v) = b4. The probability

that xs = , xt = , xu = , and xv =  is precisely

24(k1)

total number of f

=

24(k1)

24k =

1

16

.

Four way independence follows since Prob(xs = ) = Prob(xt = ) = Prob(xu = ) =

Prob(xv = ) = 1/2 and thus

Prob(xs = )Prob(xt = )Prob(xu = )Prob(xv = )

= Prob(xs = , xt = , xu =  and xs = )

Lemma 6.4 describes how to get one vector x with 4-way independence. However, we

need r = O(1/2) mutually independent vectors. Choose r independent polynomials at

the outset.

To implement the algorithm with low space, store only the polynomials in memory.

This requires 4k = O(log m) bits per polynomial for a total of O( log m

2 ) bits. When a

symbol s in the stream is read, compute each polynomial at s to obtain the value for the

corresponding value of the xs and update the running sums. xs is just the leading bit of

the value of the polynomial evaluated at s. This calculation requires O(log m) time. Thus,

we repeatedly compute the xs from the seeds, namely the coecients of the polynomials.

This idea of polynomial interpolation is also used in other contexts. Error-correcting

codes is an important example. To transmit n bits over a channel which may introduce

noise, one can introduce redundancy into the transmission so that some channel errors

29Here we have numbered the elements of the eld F s = 1, 2, . . . , m.

191

can be corrected. A simple way to do this is to view the n bits to be transmitted as

coecients of a polynomial f (x) of degree n  1. Now transmit f evaluated at points

1, 2, 3, . . . , n + m. At the receiving end, any n correct values will suce to reconstruct

the polynomial and the true message. So up to m errors can be tolerated. But even if

the number of errors is at most m, it is not a simple matter to know which values are

corrupted. We do not elaborate on this here.

6.3 Matrix Algorithms using Sampling

We now move from the streaming model to a model where the input is stored in

memory, but because the input is so large, one would like to produce a much smaller

approximation to it, or perform an approximate computation on it in low space. For

instance, the input might be stored in a large slow memory and we would like a small

sketch that can be stored in smaller fast memory and yet retains the important prop-

erties of the original input. In fact, one can view a number of results from the chapter on

machine learning in this way: we have a large population, and we want to take a small

sample, perform some optimization on the sample, and then argue that the optimum

solution on the sample will be approximately optimal over the whole population. In the

chapter on machine learning, our sample consisted of independent random draws from

the overall population or data distribution. Here we will be looking at matrix algorithms

and to achieve errors that are small compared to the Frobenius norm of the matrix rather

than compared to the total number of entries, we will perform non-uniform sampling.

Algorithms for matrix problems like matrix multiplication, low-rank approximations,

singular value decomposition, compressed representations of matrices, linear regression

etc. are widely used but some require O(n3) time for n  n matrices.

The natural alternative to working on the whole input matrix is to pick a random

sub-matrix and compute with that. Here, we will pick a subset of columns or rows of the

input matrix. If the sample size s is the number of columns we are willing to work with,

we will do s independent identical trials. In each trial, we select a column of the matrix.

All that we have to decide is what the probability of picking each column is. Sampling

uniformly at random is one option, but it is not always good if we want our error to be

a small fraction of the Frobenius norm of the matrix. For example, suppose the input

matrix has all entries in the range [1, 1] but most columns are close to the zero vector

with only a few signicant columns. Then, uniformly sampling a small number of columns

is unlikely to pick up any of the signicant columns and essentially will approximate the

original matrix with the all-zeroes matrix.30

30There are, on the other hand, many positive statements one can make about uniform sampling.

For example, suppose the columns of A are data points in an m-dimensional space (one dimension per

row). Fix any k-dimensional subspace, such as the subspace spanned by the k top singular vectors. If

we randomly sample O(k/(cid:15)2) columns uniformly, by the VC-dimension bounds given in Chapter 6, with

high probability for every vector v in the k-dimensional space and every threshold  , the fraction of the

192

We will see that the optimal probabilities are proportional to the squared length of

columns. This is referred to as length squared sampling and since its rst discovery in the

mid-90s, has been proved to have several desirable properties which we will see. Note

that all sampling we will discuss here is done with replacement.

Two general notes on this approach:

(i) We will prove error bounds which hold for all input matrices. Our algorithms

are randomized, i.e., use a random number generator, so the error bounds are random

variables. The bounds are on the expected error or tail probability bounds on large errors

and apply to any matrix. Note that this contrasts with the situation where we have a

stochastic model of the input matrix and only assert error bounds for most matrices

drawn from the probability distribution of the stochastic model. A mnemonic is - our

algorithms can toss coins, but our data does not toss coins. A reason for proving error

bounds for any matrix is that in real problems, like the analysis of the web hypertext link

matrix or the patient-genome expression matrix, it is the one matrix the user is interested

in, not a random matrix. In general, we focus on general algorithms and theorems, not

specic applications, so the reader need not be aware of what the two matrices above

mean.

(ii) There is no free lunch. Since we only work on a small random sample and not

on the whole input matrix, our error bounds will not be good for certain matrices. For

example, if the input matrix is the identity, it is intuitively clear that picking a few ran-

dom columns will miss the other directions.

To the Reader: Why arent (i) and (ii) mutually contradictory?

6.3.1 Matrix Multiplication using Sampling

Suppose A is an m  n matrix and B is an n  p matrix and the product AB is desired.

We show how to use sampling to get an approximate product faster than the traditional

multiplication. Let A (:, k) denote the kth column of A. A (:, k) is a m  1 matrix. Let

B (k, :) be the kth row of B. B (k, :) is a 1  n matrix. It is easy to see that

AB =

n

(cid:88)

k=1

A (:, k)B (k, :) .

Note that for each value of k, A(:, k)B(k, :) is an m  p matrix each element of which is a

single product of elements of A and B. An obvious use of sampling suggests itself. Sample

some values for k and compute A (:, k) B (k, :) for the sampled ks and use their suitably

scaled sum as the estimate of AB. It turns out that nonuniform sampling probabilities

are useful. Dene a random variable z that takes on values in {1, 2, . . . , n}. Let pk denote

the probability that z assumes the value k. We will solve for a good choice of probabilities

sampled columns a that satisfy vT a   will be within (cid:15) of the fraction of the columns a in the overall

matrix A satisfying vT a   .

193

later, but for now just consider the pk as nonnegative numbers that sum to one. Dene

an associated random matrix variable that has value

X =

1

pk

A (:, k) B (k, :)

(6.1)

with probability pk. Let E (X) denote the entry-wise expectation.

E (X) =

n

(cid:88)

k=1

Prob(z = k)

1

pk

A (:, k) B (k, :) =

n

(cid:88)

k=1

A (:, k)B (k, :) = AB.

This explains the scaling by 1

pk

each of whose components is correct in expectation. We will be interested in

in X. In particular, X is a matrix-valued random variable

E (cid:0)||AB  X||2

F

(cid:1) .

This can be viewed as the variance of X, dened as the sum of the variances of all its

entries.

Var(X) =

m

(cid:88)

p

(cid:88)

i=1

j=1

Var (xij) =

(cid:88)

ij

E (cid:0)x2

ij

(cid:1)  E (xij)2 =

(cid:32)

(cid:88)

(cid:88)

pk

ij

k

1

p2

k

(cid:33)

a2

ikb2

kj

 ||AB||2

F .

We want to choose pk to minimize this quantity, and notice that we can ignore the ||AB||2

F

term since it doesnt depend on the pks at all. We can now simplify by exchanging the

order of summations to get

(cid:88)

(cid:88)

ij

k

pk

1

p2

k

ikb2

a2

kj =

(cid:88)

k

1

pk

(cid:32)

(cid:88)

(cid:33) (cid:32)

(cid:88)

(cid:33)

b2

kj

a2

ik

i

j

(cid:88)

=

k

1

pk

(cid:12)A (:, k) (cid:12)

(cid:12)

(cid:12)

(cid:12)B (k, :) (cid:12)

2(cid:12)

(cid:12)

2.

What is the best choice of pk to minimize this sum? It can be seen by calculus31 that the

minimizing pk are proportional to |A(:, k)||B(k, :)|. In the important special case when

B = AT , pick columns of A with probabilities proportional to the squared length of the

columns. Even in the general case when B is not AT , doing so simplies the bounds.

This sampling is called length squared sampling. If pk is proportional to |A (:, k) |2, i.e,

pk = |A(:,k)|2

||A||2

F

, then

E (cid:0)||AB  X||2

F

(cid:1) = Var(X)  ||A||2

F

(cid:88)

k

|B (k, :) |2 = ||A||2

F ||B||2

F .

To reduce the variance, we can do s independent trials. Each trial i, i = 1, 2, . . . , s

(cid:80)s

yields a matrix Xi as in (6.1). We take 1

i=1 Xi as our estimate of AB. Since the

s

variance of a sum of independent random variables is the sum of variances, the variance

31By taking derivatives, for any set of nonnegative numbers ck, (cid:80)



k

ck.

to

ck

pk

is minimized with pk proportional

194

























A

















































B

m  n

n  p



















































































Sampled

Scaled

columns

of

A

m  s





Corresponding

scaled rows of B

s  p





Figure 6.4: Approximate Matrix Multiplication using sampling

(cid:80)s

i=1 Xi is 1

of 1

s

in each trial. Expanding this, gives:

s Var(X) and so is at most 1

s ||A||2

F ||B||2

F . Let k1, . . . , ks be the ks chosen

1

s

s

(cid:88)

i=1

Xi =

1

s

(cid:18) A (:, k1) B (k1, :)

pk1

+

A (:, k2) B (k2, :)

pk2

+    +

A (:, ks) B (ks, :)

pks

(cid:19)

.

(6.2)

We will nd it convieneint to write this as the product of an m  s matrix with a s  p

matrix as follows: Let C be the m  s matrix consisting of the following columns which

are scaled versions of the chosen columns of A:

A(:, k1)



spk1

,

A(:, k2)



spk2

, . . .

A(:, ks)



spks

.

Note that the scaling has a nice property, which the reader can verify:

E (cid:0)CC T (cid:1) = AAT .

(6.3)

Dene R to be the sp matrix with the corresponding rows of B similarly scaled, namely,

R has rows

B(k1, :)



spk1

,

B(k2, :)



spk2

, . . .

B(ks, :)



spks

.

The reader may verify that

E (cid:0)RT R(cid:1) = BT B.

(6.4)

From (6.2), we see that 1

s

our discussion in Theorem 6.3.1.

(cid:80)s

i=1 Xi = CR. This is represented in Figure 6.4. We summarize

Theorem 6.5 Suppose A is an m  n matrix and B is an n  p matrix. The product

AB can be estimated by CR, where C is an m  s matrix consisting of s columns of A

picked according to length-squared distribution and scaled to satisfy (6.3) and R is the

195

s  p matrix consisting of the corresponding rows of B scaled to satisfy (6.4). The error

is bounded by:

E (cid:0)||AB  CR||2

F

(cid:1) 

||A||2

F ||B||2

F

s

.

Thus, to ensure E (||AB  CR||2

F , it suces to make s greater than or

equal to 1/2. If  is (1), so s  O(1), then the multiplication CR can be carried out in

time O(mp).

F )  2||A||2

F ||B||2

When is this error bound good and when is it not? Lets focus on the case that B = AT

so we have just one matrix to consider. If A is the identity matrix, then the guarantee is

F = n, but the right-hand-side of the inequality is n2

not very good. In this case, ||AAT ||2

s .

So we would need s > n for the bound to be any better than approximating the product

with the zero matrix.

More generally, the trivial estimate of the all zero matrix for AAT makes an error in

Frobenius norm of ||AAT ||F . What s do we need to ensure that the error is at most this?

If 1, 2, . . . are the singular values of A, then the singular values of AAT are 2

2, . . .

and

1, 2

||AAT ||2

F =

4

t

and

||A||2

F =

(cid:88)

(cid:88)

2

t .

So from Theorem 6.3.1, E(||AAT  CR||2

F )  ||AAT ||2

F provided

t

t

s 

1 + 2

(2

1 + 4

4

2 + . . .)2

2 + . . .

.

If rank(A) = r, then there are r non-zero t and the best general upper bound on the

ratio (2

1+2

is r, so in general, s needs to be at least r. If A is full rank, this means

1+4

4

sampling will not gain us anything over taking the whole matrix!

2+...)2

2+...

However, if there is a constant c and a small integer p such that

1 + 2

2

2 + . . . + 2

p  c(2

1 + 2

2 +    + 2

r ),

(6.5)

then,

(2

1 + 2

1 + 4

4

2 + . . .)2

2 + . . .

 c2 (2

1 + 2

1 + 4

4

2 + . . . + 2

p)2

2 + . . . + 2

p

 c2p,

and so s  c2p gives us a better estimate than the zero matrix. Increasing s by a factor

decreases the error by the same factor. Condition 6.5 is indeed the hypothesis of the

subject of Principal Component Analysis (PCA) and there are many situations when the

data matrix does satisfy the condition and sampling algorithms are useful.

196

6.3.2

Implementing Length Squared Sampling in Two Passes

Traditional matrix algorithms often assume that the input matrix is in random access

memory (RAM) and so any particular entry of the matrix can be accessed in unit time.

For massive matrices, RAM may be too small to hold the entire matrix, but may be able

to hold and compute with the sampled columns and rows.

Consider a high-level model where the input matrix or matrices have to be read from

external memory using one pass in which one can read sequentially all entries of the ma-

trix and sample.

It is easy to see that two passes suce to draw a sample of columns of A according

to length squared probabilities, even if the matrix is not in row-order or column-order

and entries are presented as a linked list. In the rst pass, compute the length squared of

each column and store this information in RAM. The lengths squared can be computed as

running sums. Then, use a random number generator in RAM to determine according to

length squared probability the columns to be sampled. Then, make a second pass picking

the columns to be sampled.

If the matrix is already presented in external memory in column-order, then one pass

will do. The idea is to use the primitive in Section 6.1: given a read-once stream of

positive numbers a1, a2, . . . , an, at the end have an i  {1, 2, . . . , n} such that the proba-

. Filling in the specics is left as an exercise for the reader.

bility that i was chosen is

(cid:80)n

ai

j=1 aj

6.3.3 Sketch of a Large Matrix

The main result of this section is that for any matrix, a sample of columns and rows,

each picked according to length squared distribution provides a good sketch of the matrix.

Let A be an m  n matrix. Pick s columns of A according to length squared distribution.

Let C be the m  s matrix containing the picked columns scaled so as to satisy (6.3), i.e.,

spk. Similarly, pick r rows of A according to length

if A(:, k) is picked, it is scaled by 1/

squared distribution on the rows of A. Let R be the r n matrix of the picked rows, scaled

rpk. We then have E(RT R) = AT A.

as follows: If row k of A is picked, it is scaled by 1/

From C and R, one can nd a matrix U so that A  CU R. The schematic diagram is

given in Figure 6.5.





One may recall that the top k singular vectors of the SVD of A give a similar picture;

however, the SVD takes more time to compute, requires all of A to be stored in RAM,

and does not have the property that the rows and columns are directly from A. The last

property, that the approximation involves actual rows/columns of the matrix rather than

linear combinations, is called an interpolative approximation and is useful in many con-

texts. On the other hand, the SVD yields the best 2-norm approximation. Error bounds

for the approximation CU R are weaker.

197

























A



















































Sample

columns

























n  m

n  s

(cid:21)(cid:20) Sample rows

(cid:21)

r  m

(cid:20) Multi

plier

s  r

Figure 6.5: Schematic diagram of the approximation of A by a sample of s columns and

r rows.

We briey touch upon two motivations for such a sketch. Suppose A is the document-

term matrix of a large collection of documents. We are to read the collection at the

outset and store a sketch so that later, when a query represented by a vector with one

entry per term arrives, we can nd its similarity to each document in the collection.

Similarity is dened by the dot product. In Figure 6.5 it is clear that the matrix-vector

product of a query with the right hand side can be done in time O(ns + sr + rm) which

would be linear in n and m if s and r are O(1). To bound errors for this process, we

need to show that the dierence between A and the sketch of A has small 2-norm. Re-

|Ax|. The fact that the sketch is an

call that the 2-norm ||A||2 of a matrix A is max

|x|=1

interpolative approximation means that our approximation essentially consists a subset

of documents and a subset of terms, which may be thought of as a representative set of

documents and terms. Additionally, if A is sparse in its rows and columns, each document

contains only a small fraction of the terms and each term is in only a small fraction of

the documents, then this sparsity property will be preserved in C and R, unlike with SVD.

A second motivation comes from analyzing gene microarray data. Here, A is a matrix

in which each row is a gene and each column is a condition. Entry (i, j) indicates the

extent to which gene i is expressed in condition j. In this context, a CU R decomposition

provides a sketch of the matrix A in which rows and columns correspond to actual genes

and conditions, respectively. This can often be easier for biologists to interpret than a

singular value decomposition in which rows and columns would be linear combinations of

the genes and conditions.

It remains now to describe how to nd U from C and R. There is a n  n matrix P

of the form P = QR that acts as the identity on the space spanned by the rows of R and

zeros out all vectors orthogonal to this space. We state this now and postpone the proof.

Lemma 6.6 If RRT is invertible, then P = RT (RRT )1R has the following properties:

198

(i) It acts as the identity matrix on the row space of R. I.e., P x = x for every vector x

of the form x = RT y (this denes the row space of R). Furthermore,

(ii) if x is orthogonal to the row space of R, then P x = 0.

If RRT is not invertible, let rank (RRT ) = r and RRT = (cid:80)r

RRT . Then,

t=1 tutvt

T be the SVD of

P = RT

(cid:32) r

(cid:88)

(cid:33)

T

utvt

R

1

2

t

satises (i) and (ii).

t=1

We begin with some intuition. In particular, we rst present a simpler idea that does

not work, but that motivates an idea that does. Write A as AI, where I is the n  n

identity matrix. Approximate the product AI using the algorithm of Theorem 6.3.1, i.e.,

by sampling s columns of A according to a length-squared distribution. Then, as in the

last section, write AI  CW , where W consists of a scaled version of the s rows of I

corresponding to the s columns of A that were picked. Theorem 6.3.1 bounds the error

||ACW ||2

F . But we would like the error to be a small fraction

of ||A||2

F which would require s  n, which clearly is of no use since this would pick as

many or more columns than the whole of A.

F by ||A||2

F /s = n

s ||A||2

F ||I||2

Lets use the identity-like matrix P instead of I in the above discussion. Using the

fact that R is picked according to length squared sampling, we will show the following

proposition later.

Proposition 6.7 A  AP and the error E (||A  AP ||2

2) is at most

1

r ||A||2

F .

We then use Theorem 6.3.1 to argue that instead of doing the multiplication AP , we can

use the sampled columns of A and the corresponding rows of P . The s sampled columns

of A form C. We have to take the corresponding s rows of P = RT (RRT )1R, which is

the same as taking the corresponding s rows of RT , and multiplying this by (RRT )1R. It

is easy to check that this leads to an expression of the form CU R. Further, by Theorem

6.3.1, the error is bounded by

E (cid:0)||AP  CU R||2

2

(cid:1)  E (cid:0)||AP  CU R||2

F

(cid:1) 

||A||2

F ||P ||2

F

s



r

s

||A||2

F ,

(6.6)

since we will show later that:

Proposition 6.8 ||P ||2

F  r.

Putting (6.6) and Proposition 6.7 together, and using the fact that by triangle inequality

||A  CU R||2  ||A  AP ||2 + ||AP  CU R||2, which in turn implies that ||A  CU R||2

2 

2||A  AP ||2

2, the main result below follows.

2 + 2||AP  CU R||2

199

Theorem 6.9 Let A be an m  n matrix and r and s be positive integers. Let C be an

m  s matrix of s columns of A picked according to length squared sampling and let R be

a matrix of r rows of A picked according to length squared sampling. Then, we can nd

from C and R an s  r matrix U so that

E (cid:0)||A  CU R||2

2

(cid:1)  ||A||2

F

(cid:18) 2



r

+

(cid:19)

.

2r

s

If s is xed, the error is minimized when r = s2/3. Choosing s = 1/3 and r = 1/2,

the bound becomes O()||A||2

F . When is this bound meaningful? We discuss this further

after rst proving all the claims used in the discussion above.

Proof of Lemma 6.6: First consider the case that RRT is invertible. For x = RT y,

RT (RRT )1Rx = RT (RRT )1RRT y = RT y = x. If x is orthogonal to every row of R,

then Rx = 0, so P x = 0. More generally, if RRT = (cid:80)

R =

(cid:80)

T , then, RT (cid:80)

t tutvt

T and clearly satises (i) and (ii).

1

2

t

t

t vtvt

Next we prove Proposition 6.7. First, recall that

||A  AP ||2

2 = max

{x:|x|=1}

|(A  AP )x|2.

Now suppose x is in the row space V of R. From Lemma 6.6, P x = x, so for x  V ,

(A  AP )x = 0. Since every vector can be written as a sum of a vector in V plus a vector

orthogonal to V , this implies that the maximum must therefore occur at some x  V .

For such x, by Lemma 6.6, (AAP )x = Ax. Thus, the question becomes: for unit-length

x  V , how large can |Ax|2 be? To analyze this, write:

|Ax|2 = xT AT Ax = xT (AT A  RT R)x  ||AT A  RT R||2|x|2  ||AT A  RT R||2.

2  ||A||4

2  ||AT A  RT R||2. So, it suces to prove that ||AT A 

This implies that ||A  AP ||2

F /r which follows directly from Theorem 6.3.1, since we can think of RT R

RT R||2

as a way of estimating AT A by picking according to length-squared distribution columns

of AT , i.e., rows of A. This proves Proposition 6.7.

Proposition 6.8 is easy to see. By Lemma 6.6, P is the identity on the space V spanned

F is the

by the rows of R, and P x = 0 for x perpendicular to the rows of R. Thus ||P ||2

sum of its singular values squared which is at most r as claimed.

We now briey look at the time needed to compute U . The only involved step in

computing U is to nd (RRT )1 or do the SVD of RRT . But note that RRT is an r  r

matrix and since r is much smaller than n and m, this is fast.

200

Understanding the bound in Theorem 6.9: To better understand the bound in

Theorem 6.9 consider when it is meaningful and when it is not. First, choose parameters

s = (1/3) and r = (1/2) so that the bound becomes E(||A  CU R||2

2)  ||A||2

F .

F = (cid:80)

Recall that ||A||2

i (A), i.e., the sum of squares of all the singular values of A.

Also, for convenience scale A so that 2

i 2

1(A) = 1. Then

1(A) = ||A||2

2

2 = 1 and E(||A  CU R||2

2)  

(cid:88)

i

2

i (A).

This, gives an intuitive sense of when the guarantee is good and when it is not. If the

top k singular values of A are all (1) for k (cid:29) m1/3, so that (cid:80)

i (A) (cid:29) m1/3, then

the guarantee is only meaningful when  = o(m1/3), which is not interesting because it

requires s > m. On the other hand, if just the rst few singular values of A are large

and the rest are quite small, e.g, A represents a collection of points that lie very close

to a low-dimensional subspace, and in particular if (cid:80)

i (A) is a constant, then to be

meaningful the bound requires  to be a small constant. In this case, the guarantee is

indeed meaningful because it implies that a constant number of rows and columns provides

a good 2-norm approximation to A.

i 2

i 2

6.4 Sketches of Documents

Suppose one wished to store all the web pages from the World Wide Web. Since there

are billions of web pages, one might want to store just a sketch of each page where a sketch

is some type of compact description that captures sucient information to do whatever

task one has in mind. For the current discussion, we will think of a web page as a string

of characters, and the task at hand will be one of estimating similarities between pairs of

web pages.

We begin this section by showing how to estimate similarities between sets via sam-

pling, and then how to convert the problem of estimating similarities between strings into

a problem of estimating similarities between sets.

Consider subsets of size 1000 of the integers from 1 to 106. Suppose one wished to

compute the resemblance of two subsets A and B by the formula

resemblance (A, B) = |AB|

|AB|

Suppose that instead of using the sets A and B, one sampled the sets and compared

random subsets of size ten. How accurate would the estimate be? One way to sample

would be to select ten elements uniformly at random from A and B. Suppose A and B

were each of size 1000, over lapped by 500, and both were represented by six samples.

Even though half of the six samples of A were in B they would not likely be among the

samples representing B. See Figure 6.6. This method is unlikely to produce overlapping

samples. Another way would be to select the ten smallest elements from each of A and

B. If the sets A and B overlapped signicantly one might expect the sets of ten smallest

201

A

B

Figure 6.6: Samples of overlapping sets A and B.

elements from each of A and B to also overlap. One diculty that might arise is that the

small integers might be used for some special purpose and appear in essentially all sets

and thus distort the results. To overcome this potential problem, rename all elements

using a random permutation.

Suppose two subsets of size 1000 overlapped by 900 elements. What might one expect

the overlap of the 10 smallest elements from each subset to be? One would expect the

nine smallest elements from the 900 common elements to be in each of the two sampled

subsets for an overlap of 90%. The expected resemblance(A, B) for the size ten sample

would be 9/11=0.81.

Another method would be to select the elements equal to zero mod m for some inte-

ger m. If one samples mod m the size of the sample becomes a function of n. Sampling

mod m allows one to handle containment.

In another version of the problem one has a string of characters rather than a set.

Here one converts the string into a set by replacing it by the set of all of its substrings

of some small length k. Corresponding to each string is a set of length k substrings. If

k is modestly large, then two strings are highly unlikely to give rise to the same set of

substrings. Thus, we have converted the problem of sampling a string to that of sampling

a set. Instead of storing all the substrings of length k, we need only store a small subset

of the length k substrings.

Suppose you wish to be able to determine if two web pages are minor modications

of one another or to determine if one is a fragment of the other. Extract the sequence of

words occurring on the page, viewing each word as a character. Then dene the set of

substrings of k consecutive words from the sequence. Let S(D) be the set of all substrings

of k consecutive words occurring in document D. Dene resemblance of A and B by

And dene containment as

resemblance (A, B) = |S(A)S(B)|

|S(A)S(B)|

containment (A, B) = |S(A)S(B)|

|S(A)|

Let  be a random permutation of all length k substrings. Dene F (A) to be the s

smallest elements of A and V (A) to be the set mod m in the ordering dened by the

202

permutation.

Then

and

F (A)  F (B)

F (A)  F (B)

|V (A)V (B)|

|V (A)V (B)|

are unbiased estimates of the resemblance of A and B. The value

is an unbiased estimate of the containment of A in B.

|V (A)V (B)|

|V (A)|

6.5 Bibliographic Notes

The hashing-based algorithm for counting the number of distrinct elements in a data

stream described in Section 6.2.1 is due to Flajolet and Martin [FM85]. Algorithm Fre-

quent for identifying the most frequent elements is due to Misra and Gries [MG82]. The

algorithm for estimating the second moment of a data stream described in Section 6.2.4

is due to Alon, Matias and Szegedy [AMS96], who also gave algorithms and lower bounds

for other kth moments. These early algorithms for streaming data signicantly inuenced

further research in the area. Improvements and generalizations of Algorithm Frequent

were made in [MM02].

Length-squared sampling was introduced by Frieze, Kannan and Vempala [FKV04];

the algorithms of Section 6.3 are from [DKM06a, DKM06b]. The material in Section 6.4

on sketches of documents is from Broder et al. [BGMZ97].

203

6.6 Exercises

Exercise 6.1 Let a1, a2, . . . , an, be a stream of symbols each an integer in {1, . . . , m}.

1. Give an algorithm that will select a symbol uniformly at random from the stream.

How much memory does your algorithm require?

2. Give an algorithm that will select a symbol with probability proportional to a2

i .

Exercise 6.2 How would one pick a random word from a very large book where the prob-

ability of picking a word is proportional to the number of occurrences of the word in the

book?

Exercise 6.3 Consider a matrix where each element has a probability of being selected.

Can you select a row according to the sum of probabilities of elements in that row by just

selecting an element according to its probability and selecting the row that the element is

in?

Exercise 6.4 For the streaming model give an algorithm to draw t independent samples

of indices i, each with probability proportional to the value of ai. Some images may be

drawn multiple times. What is its memory usage?

Exercise 6.5 For some constant c > 0, it is possible to create 2cm subsets of {1, . . . , m},

each with m/2 elements, such that no two of the subsets share more than 3m/8 elements

in common.32 Use this fact to argue that any deterministic algorithm that even guarantees

to approximate the number of distinct elements in a data stream with error less than m

16

must use (m) bits of memory on some input sequence of length n  2m.

Exercise 6.6 Consider an algorithm that uses a random hash function and gives an

estimate x of the true value x of some variable. Suppose that x

4  x  4x with probability

at least 0.6. The probability of the estimate is with respect to choice of the hash function.

How would you improve the probability that x

4  x  4x to 0.8? Hint: Since we do not

know the variance taking average may not help and we need to use some other function

of multiple runs.

Exercise 6.7 Give an example of a set H of hash functions such that h(x) is equally

likely to be any element of {0, . . . , M  1} (H is 1-universal) but H is not 2-universal.

Exercise 6.8 Let p be a prime. A set of hash functions

H = {h| {0, 1, . . . , p  1}  {0, 1, . . . , p  1}}

is 3-universal if for all x,y,z,u,v,w in {0, 1, . . . , p  1} , where x, y, z are distinct we have

(cid:16)

Prob

h (x) = u, h (y) = v, h (z) = w

(cid:17)

=

1

p3 .

32For example, choosing them randomly will work with high probability. You expect two subsets of size

m/2 to share m/4 elements in common, and with high probability they will share no more than 3m/8.

204

(a) Is the set {hab(x) = ax + b mod p | 0  a, b < p} of hash functions 3-universal?

(b) Give a 3-universal set of hash functions.

Exercise 6.9 Select a value for k and create a set

H = (cid:8)x|x = (x1, x2, . . . , xk), xi  {0, 1, . . . , k  1}(cid:9)

where the set of vectors H is pairwise independent and |H| < kk. We say that a set of vec-

tors is pairwise independent if for any subset of two of the coordinates, all of the k2 possible

pairs of values that could appear in those coordinates such as (0, 0), (0, 1), . . . , (1, 0), (1, 1), . . .

occur the exact same number of times.

Exercise 6.10 Consider a coin that comes down heads with probability p. Prove that the

expected number of ips needed to see a heads is 1/p.

Exercise 6.11 Randomly generate a string x1x2    xn of 106 0s and 1s with probability

1/2 of xi being a 1. Count the number of ones in the string and also estimate the number of

ones by the coin-ip approximate counting algorithm, in Section 6.2.2. Repeat the process

for p=1/4, 1/8, and 1/16. How close is the approximation?

Counting Frequent Elements

The Majority and Frequent Algorithms

The Second Moment

Exercise 6.12

1. Construct an example in which the majority algorithm gives a false positive, i.e.,

stores a non majority element at the end.

2. Construct an example in which the frequent algorithm in fact does as badly as in the

theorem, i.e., it under counts some item by n/(k+1).

Exercise 6.13 Let p be a prime and n  2 be an integer. What representation do you

use to do arithmetic in the nite eld with pn elements? How do you do addition? How

do you do multiplication?

Error-Correcting codes, polynomial interpolation and limited-way indepen-

dence

Exercise 6.14 Let F be a eld. Prove that for any four distinct points a1, a2, a3, and a4

in F and any four possibly not distinct values b1, b2, b3, and b4 in F , there is a unique

polynomial f (x) = f0 + f1x + f2x2 + f3x3 of degree at most three so that f (ai) = bi,

1  i  4 with all computations done over F . If you use the Vandermonde matrix you

can use the fact that the matrix is nonsingular.

205

Sketch of a Large Matrix

Exercise 6.15 Suppose we want to pick a row of a matrix at random where the probability

of picking row i is proportional to the sum of squares of the entries of that row. How would

we do this in the streaming model?

(a) Do the problem when the matrix is given in column order.

(b) Do the problem when the matrix is represented in sparse notation: it is just presented

as a list of triples (i, j, aij), in arbitrary order.

Matrix Multiplication Using Sampling

Exercise 6.16 Suppose A and B are two matrices. Prove that AB =

n

(cid:80)

k=1

A (:, k)B (k, :).

Exercise 6.17 Generate two 100 by 100 matrices A and B with integer values between

1 and 100. Compute the product AB both directly and by sampling. Plot the dierence

in L2 norm between the results as a function of the number of samples. In generating

the matrices make sure that they are skewed. One method would be the following. First

generate two 100 dimensional vectors a and b with integer values between 1 and 100. Next

generate the ith row of A with integer values between 1 and ai and the ith column of B

with integer values between 1 and bi.

Approximating a Matrix with a Sample of Rows and Columns

Exercise 6.18 Suppose a1, a2, . . . , am are nonnegative reals. Show that the minimum

xk = 1 is attained when the xk are

of

subject to the constraints xk  0 and (cid:80)

m

(cid:80)

k=1

ak

xk

proportional to



ak.

k

Sketches of Documents

Exercise 6.19 Construct two dierent strings of 0s and 1s having the same set of sub-

strings of length k = 3.

Exercise 6.20 (Random strings, empirical analysis). Consider random strings of length

n composed of the integers 0 through 9, where we represent a string x by its set Sk(x)

of length k-substrings. Perform the following experiment: choose two random strings x

and y of length n = 10, 000 and compute their resemblance |Sk(x)Sk(y)|

|Sk(x)Sk(y)| for k = 1, 2, 3 . . ..

What does the graph of resemblance as a function of k look like?

Exercise 6.21 (Random strings, theoretical analysis). Consider random strings of length

n composed of the integers 0 through 9, where we represent a string x by its set Sk(x) of

length k-substrings. Consider now drawing two random strings x and y of length n and

computing their resemblance |Sk(x)Sk(y)|

|Sk(x)Sk(y)| .

206

1. Prove that for k  1

2 log10(n), with high probability as n goes to innity the two

strings have resemblance equal to 1.

2. Prove that for k  3 log10(n), with high probability as n goes to innity the two

strings have resemblance equal to 0.

Exercise 6.22 Discuss how you might go about detecting plagiarism in term papers.

Exercise 6.23 Suppose you had one billion web pages and you wished to remove dupli-

cates. How might you do this?

Exercise 6.24 Consider the following lyrics:

When you walk through the storm hold your head up high and dont be afraid

of the dark. At the end of the storm theres a golden sky and the sweet silver

song of the lark.

Walk on, through the wind, walk on through the rain though your dreams be

tossed and blown. Walk on, walk on, with hope in your heart and youll never

walk alone, youll never walk alone.

How large must k be to uniquely recover the lyric from the set of all subsequences of

symbols of length k? Treat the blank as a symbol.

Exercise 6.25 Blast: Given a long string A, say of length 109 and a shorter string B,

say 105, how do we nd a position in A which is the start of a substring B(cid:48) that is close

to B? This problem can be solved by dynamic programming in polynomial time, but nd

a faster algorithm to solve this problem.

Hint: (Shingling approach) One possible approach would be to x a small length, say

seven, and consider the shingles of A and B of length seven. If a close approximation to

B is a substring of A, then a number of shingles of B must be shingles of A. This should

allows us to nd the approximate location in A of the approximation of B. Some nal

algorithm should then be able to nd the best match.

207

7 Clustering

7.1 Introduction

Clustering refers to partitioning a set of objects into subsets according to some de-

sired criterion. Often it is an important step in making sense of large amounts of data.

Clustering comes up in many contexts. One might want to partition a set of news articles

into clusters based on the topics of the articles. Given a set of pictures of people, one

might want to group them into clusters based on who is in the image. Or one might want

to cluster a set of protein sequences according to the protein function. A related problem

is not nding a full partitioning but rather just identifying natural clusters that exist.

For example, given a collection of friendship relations among people, one might want to

identify any tight-knit groups that exist. In some cases we have a well-dened correct

answer, e.g., in clustering photographs of individuals by who is in them, but in other cases

the notion of a good clustering may be more subjective.

Before running a clustering algorithm, one rst needs to choose an appropriate repre-

sentation for the data. One common representation is as vectors in Rd. This corresponds

to identifying d real-valued features that are then computed for each data object. For ex-

ample, to represent documents one might use a bag of words representation, where each

feature corresponds to a word in the English language and the value of the feature is how

many times that word appears in the document. Another common representation is as

vertices in a graph, with edges weighted by some measure of how similar or dissimilar the

two endpoints are. For example, given a set of protein sequences, one might weight edges

based on an edit-distance measure that essentially computes the cost of transforming one

sequence into the other. This measure is typically symmetric and satises the triangle

inequality, and so can be thought of as a nite metric. A point worth noting up front

is that often the correct clustering of a given set of data depends on your goals. For

instance, given a set of photographs of individuals, we might want to cluster the images by

who is in them, or we might want to cluster them by facial expression. When representing

the images as points in space or as nodes in a weighted graph, it is important that the

features we use be relevant to the criterion we care about. In any event, the issue of how

best to represent data to highlight the relevant information for a given task is generally

addressed using knowledge of the specic domain. From our perspective, the job of the

clustering algorithm begins after the data has been represented in some appropriate way.

In this chapter, our goals are to discuss (a) some commonly used clustering algorithms

and what one can prove about them, and (b) models and assumptions on data under which

we can nd a clustering close to the correct clustering.

7.1.1 Preliminaries

We will follow the standard notation of using n to denote the number of data points

and k to denote the number of desired clusters. We will primarily focus on the case that

208

k is known up front, but will also discuss algorithms that produce a sequence of solutions,

one for each value of k, as well as algorithms that produce a cluster tree that can encode

multiple clusterings at each value of k. We will generally use A = {a1, . . . , an} to denote

the n data points. We also think of A as a matrix with rows a1, . . . , an.

7.1.2 Two General Assumptions on the Form of Clusters

Before choosing a clustering algorithm, it is useful to have some general idea of what

a good clustering should look like. In general, there are two types of assumptions often

made that in turn lead to dierent classes of clustering algorithms.

Center-based clusters: One assumption commonly made is that clusters are center-

based. This means that the clustering can be dened by k centers c1, . . . , ck, with each

data point assigned to whichever center is closest to it. Note that this assumption does

not yet tell whether one choice of centers is better than another. For this, one needs

an objective, or optimization criterion. Three standard criteria often used are k-center,

k-median, and k-means clustering, dened as follows.

k-center clustering: Find a partition C = {C1, . . . , Ck} of A into k clusters, with corre-

sponding centers c1, . . . , ck, to minimize the maximum distance between any data

point and the center of its cluster. That is, we want to minimize

kcenter(C) =

k

max

j=1

max

aiCj

d(ai, cj).

k-center clustering makes sense when we believe clusters should be local regions in

space. It is also often thought of as the rehouse location problem since one can

think of it as the problem of locating k re-stations in a city so as to minimize the

maximum distance a re-truck might need to travel to put out a re.

k-median clustering: Find a partition C = {C1, . . . , Ck} of A into k clusters, with corre-

sponding centers c1, . . . , ck, to minimize the sum of distances between data points

and the centers of their clusters. That is, we want to minimize

kmedian(C) =

k

(cid:88)

(cid:88)

j=1

aiCj

d(ai, cj).

k-median clustering is more noise-tolerant than k-center clustering because we are

taking a sum rather than a max. A small number of outliers will typically not

change the optimal solution by much, unless they are very far away or there are

several quite dierent near-optimal solutions.

k-means clustering: Find a partition C = {C1, . . . , Ck} of A into k clusters, with cor-

responding centers c1, . . . , ck, to minimize the sum of squares of distances between

209

data points and the centers of their clusters. That is, we want to minimize

kmeans(C) =

k

(cid:88)

(cid:88)

j=1

aiCj

d2(ai, cj).

k-means clustering puts more weight on outliers than k-median clustering, because

we are squaring the distances, which magnies large values. This puts it somewhat

in between k-median and k-center clustering in that regard. Using distance squared

has some mathematical advantages over using pure distances when data are points in

Rd. For example, Corollary 7.2 that asserts that with the distance squared criterion,

the optimal center for a given group of data points is its centroid.

The k-means criterion is more often used when data consists of points in Rd, whereas

k-median is more commonly used when we have a nite metric, that is, data are nodes in

a graph with distances on edges.

When data are points in Rd, there are in general two variations of the clustering prob-

lem for each of the criteria. We could require that each cluster center be a data point or

allow a cluster center to be any point in space. If we require each center to be a data

(cid:1)

point, the optimal clustering of n data points into k clusters can be solved in time (cid:0)n

k

times a polynomial in the length of the data. First, exhaustively enumerate all sets of k

data points as the possible sets of k cluster centers, then associate each point to its nearest

center and select the best clustering. No such naive enumeration procedure is available

when cluster centers can be any point in space. But, for the k-means problem, Corol-

lary 7.2 shows that once we have identied the data points that belong to a cluster, the

best choice of cluster center is the centroid of that cluster, which might not be a data point.

When k is part of the input or may be a function of n, the above optimization prob-

lems are all NP-hard.33 So, guarantees on algorithms will typically involve either some

form of approximation or some additional assumptions, or both.

High-density clusters: If we do not believe our desired clusters will be center-based,

an alternative assumption often made is that clusters consist of high-density regions sur-

rounded by low-density moats between them. For example, in the clustering of Figure

7.1 we have one natural cluster A that looks center-based but the other cluster B consists

of a ring around cluster A. As seen in the gure, this assumption does not require clus-

ters to correspond to convex regions and it can allow them to be long and stringy. We

describe a non-center-based clustering method in Section 7.7. In Section 7.9 we prove the

eectiveness of an algorithm which nds a moat, cuts up data inside the moat and

outside into two pieces and recursively applies the same procedure to each piece.

33If k is a constant, then as noted above, the version where the centers must be data points can be

solved in polynomial time.

210

B

A

Figure 7.1: Example where the natural clustering is not center-based.

7.1.3 Spectral Clustering

An important part of a clustering toolkit when data lies in Rd is Singular Value De-

composition. Spectral Clustering refers to the following algorithm: Find the space V

spanned by the top k right singular vectors of the matrix A whose rows are the data

points. Project data points to V and cluster in the projection.

An obvious reason to do this is dimension reduction, clustering in the d dimensional

space where data lies is reduced to clustering in a k dimensional space (usually, k << d).

A more important point is that under certain assumptions one can prove that spectral

clustering gives a clustering close to the true clustering. We already saw this in the case

when data is from a mixture of spherical Gaussians, Section 3.9.3. The assumption used is

the means separated by a constant number of Standard Deviations. In Section 7.5, we

will see that in a much more general setting, which includes common stochastic models,

the same assumption, in spirit, yields similar conclusions. Section 7.4, has another setting

with a similar result.

7.2 k-Means Clustering

We assume in this section that data points lie in Rd and focus on the k-means criterion.

7.2.1 A Maximum-Likelihood Motivation

We now consider a maximum-likelihood motivation for using the k-means criterion.

Suppose that the data was generated according to an equal weight mixture of k spherical

well-separated Gaussian densities centered at 1, 2, . . . , k, each with variance one in

every direction. Then the density of the mixture is

Prob(x) =

1

(2)d/2

1

k

k

(cid:88)

i=1

e|xi|2.

Denote by (x) the center nearest to x. Since the exponential function falls o fast,

assuming x is noticeably closer to its nearest center than to any other center, we can

211

approximate (cid:80)k

Thus

i=1 e|xi|2 by e|x(x)|2 since the sum is dominated by its largest term.

1

(2)d/2k

The likelihood of drawing the sample of points x1, x2, . . . , xn from the mixture, if the

centers were 1, 2, . . . , k, is approximately

e|x(x)|2.

Prob(x) 

1

kn

1

(2)nd/2

n

(cid:89)

i=1

e|x(i)(x(i))|2 = ce (cid:80)n

i=1 |x(i)(x(i))|2.

Minimizing the sum of squared distances to cluster centers nds the maximum likelihood

1, 2, . . . , k. This motivates using the sum of distance squared to the cluster centers.

7.2.2 Structural Properties of the k-Means Objective

Suppose we have already determined the clustering or the partitioning into C1, C2, . . . , Ck.

What are the best centers for the clusters? The following lemma shows that the answer

is the centroids, the coordinate means, of the clusters.

Lemma 7.1 Let {a1, a2, . . . , an} be a set of points. The sum of the squared distances of

the ai to any point x equals the sum of the squared distances to the centroid of the ai plus

n times the squared distance from x to the centroid. That is,

|ai  x|2 =

(cid:88)

i

(cid:88)

i

|ai  c|2 + n |c  x|2

ai is the centroid of the set of points.

where c = 1

n

n

(cid:80)

i=1

Proof:

|ai  x|2 =

(cid:88)

i

=

(cid:88)

i

(cid:88)

i

|ai  c + c  x|2

|ai  c|2 + 2(c  x) 

(ai  c) + n |c  x|2

(cid:88)

i

Since c is the centroid, (cid:80)

(ai  c) = 0. Thus, (cid:80)

|ai  x|2 = (cid:80)

|ai  c|2 + n |c  x|2

i

i

i

A corollary of Lemma 7.1 is that the centroid minimizes the sum of squared distances

|ai  c|2, is a constant independent of x and setting x = c sets the

since the rst term, (cid:80)

second term, n (cid:107)c  x(cid:107)2, to zero.

i

Corollary 7.2 Let {a1, a2, . . . , an} be a set of points. The sum of squared distances of

the ai to a point x is minimized when x is the centroid, namely x = 1

n

ai.

(cid:80)

i

212

7.2.3 Lloyds Algorithm

Corollary 7.2 suggests the following natural strategy for k-means clustering, known as

Lloyds algorithm. Lloyds algorithm does not necessarily nd a globally optimal solution

but will nd a locally-optimal one. An important but unspecied step in the algorithm is

its initialization: how the starting k centers are chosen. We discuss this after discussing

the main algorithm.

Lloyds algorithm:

Start with k centers.

Cluster each point with the center nearest to it.

Find the centroid of each cluster and replace the set of old centers with the centroids.

Repeat the above two steps until the centers converge according to some criterion, such

as the k-means score no longer improving.

This algorithm always converges to a local minimum of the objective. To show conver-

gence, we argue that the sum of the squares of the distances of each point to its cluster

center always improves. Each iteration consists of two steps. First, consider the step

that nds the centroid of each cluster and replaces the old centers with the new centers.

By Corollary 7.2, this step improves the sum of internal cluster distances squared. The

second step reclusters by assigning each point to its nearest cluster center, which also

improves the internal cluster distances.

A problem that arises with some implementations of the k-means clustering algorithm

is that one or more of the clusters becomes empty and there is no center from which to

measure distance. A simple case where this occurs is illustrated in the following example.

You might think how you would modify the code to resolve this issue.

Example: Consider running the k-means clustering algorithm to nd three clusters on

the following 1-dimension data set: {2,3,7,8} starting with centers {0,5,10}.

0

0

0

1

1

1

2

2

2

3

3

3

4

4

4

5

5

5

6

6

6

7

7

7

8

8

8

9

9

9

10

10

10

The center at 5 ends up with no items and there are only two clusters instead of the

desired three.

213

Figure 7.2: A locally-optimal but globally-suboptimal k-means clustering.

As noted above, Lloyds algorithm only nds a local optimum to the k-means objective

that might not be globally optimal. Consider, for example, Figure 7.2. Here data lies in

three dense clusters in R2: one centered at (0, 1), one centered at (0, 1) and one centered

at (3, 0). If we initialize with one center at (0, 1) and two centers near (3, 0), then the

center at (0, 1) will move to near (0, 0) and capture the points near (0, 1) and (0, 1),

whereas the centers near (3, 0) will just stay there, splitting that cluster.

Because the initial centers can substantially inuence the quality of the result, there

has been signicant work on initialization strategies for Lloyds algorithm. One popular

strategy is called farthest traversal. Here, we begin by choosing one data point as initial

center c1 (say, randomly), then pick the farthest data point from c1 to use as c2, then

pick the farthest data point from {c1, c2} to use as c3, and so on. These are then used

as the initial centers. Notice that this will produce the correct solution in the example in

Figure 7.2.

Farthest traversal can unfortunately get fooled by a small number of outliers. To ad-

dress this, a smoother, probabilistic variation known as k-means++ instead weights data

points based on their distance squared from the previously chosen centers. Then it selects

the next center probabilistically according to these weights. This approach has the nice

property that a small number of outliers will not overly inuence the algorithm so long as

they are not too far away, in which case perhaps they should be their own clusters anyway.

Another approach is to run some other approximation algorithm for the k-means

problem, and then use its output as the starting point for Lloyds algorithm. Note that

applying Lloyds algorithm to the output of any other algorithm can only improve its

score. An alternative SVD-based method for initialization is described and analyzed in

Section 7.5.

214

(0,1) (0,-1) (3,0)

7.2.4 Wards Algorithm

Another popular heuristic for k-means clustering is Wards algorithm. Wards algo-

rithm begins with each data point in its own cluster, and then repeatedly merges pairs of

clusters until only k clusters remain. Specically, Wards algorithm merges the two clus-

ters that minimize the immediate increase in k-means cost. That is, for a cluster C, dene

cost(C) = (cid:80)

aiC d2(ai, c), where c is the centroid of C. Then Wards algorithm merges

the pair (C, C (cid:48)) minimizing cost(C  C (cid:48))  cost(C)  cost(C (cid:48)). Thus, Wards algorithm

can be viewed as a greedy k-means algorithm.

7.2.5

k-Means Clustering on the Line

One case where the optimal k-means clustering can be found in polynomial time is

when points lie in R1, i.e., on the line. This can be done using dynamic programming, as

follows.

First, assume without loss of generality that the data points a1, . . . , an have been

sorted, so a1  a2  . . .  an. Now, suppose that for some i  1 we have already

computed the optimal k(cid:48)-means clustering for points a1, . . . , ai for all k(cid:48)  k; note that

this is trivial to do for the base case of i = 1. Our goal is to extend this solution to points

a1, . . . , ai+1. To do so, observe that each cluster will contain a consecutive sequence of

data points. So, given k(cid:48), for each j  i + 1, compute the cost of using a single center

for points aj, . . . , ai+1, which is the sum of distances of each of these points to their mean

value. Then add to that the cost of the optimal k(cid:48)  1 clustering of points a1, . . . , aj1

which we computed earlier. Store the minimum of these sums, over choices of j, as our

optimal k(cid:48)-means clustering of points a1, . . . , ai+1. This has running time of O(kn) for a

given value of i. So overall our running time is O(kn2).

7.3 k-Center Clustering

In this section, instead of using the k-means clustering criterion, we use the k-center

criterion. Recall that the k-center criterion partitions the points into k clusters so as to

minimize the maximum distance of any point to its cluster center. Call the maximum dis-

tance of any point to its cluster center the radius of the clustering. There is a k-clustering

of radius r if and only if there are k spheres, each of radius r, which together cover all

the points. Below, we give a simple algorithm to nd k spheres covering a set of points.

The following lemma shows that this algorithm only needs to use a radius that is at most

twice that of the optimal k-center solution. Note that this algorithm is equivalent to the

farthest traversal strategy for initializing Lloyds algorithm.

The Farthest Traversal k-clustering Algorithm

Pick any data point to be the rst cluster center. At time t, for t = 2, 3, . . . , k,

pick the farthest data point from any existing cluster center; make it the tth cluster

center.

215

Theorem 7.3 If there is a k-clustering of radius r

k-clustering with radius at most r.

2, then the above algorithm nds a

Proof: Suppose for contradiction that there is some data point x that is distance greater

than r from all centers chosen. This means that each new center chosen was distance

greater than r from all previous centers, because we could always have chosen x. This

implies that we have k +1 data points, namely the centers chosen plus x, that are pairwise

more than distance r apart. Clearly, no two such points can belong to the same cluster

in any k-clustering of radius r

2, contradicting the hypothesis.

7.4 Finding Low-Error Clusterings

In the previous sections we saw algorithms for nding a local optimum to the k-means

clustering objective, for nding a global optimum to the k-means objective on the line, and

for nding a factor 2 approximation to the k-center objective. But what about nding

a clustering that is close to the correct answer, such as the true clustering of proteins

by function or a correct clustering of news articles by topic? For this we need some

assumption about the data and what the correct answer looks like. The next few sections

consider algorithms based on dierent such assumptions.

7.5 Spectral Clustering

Let A be a nd data matrix with each row a data point and suppose we want to partition

the data points into k clusters. Spectral Clustering refers to a class of clustering algorithms

which share the following outline:

Find the space V spanned by the top k (right) singular vectors of A.

Project data points into V .

Cluster the projected points.

7.5.1 Why Project?

The reader may want to read Section 3.9.3, which shows the ecacy of spectral clustering

for data stochastically generated from a mixture of spherical Gaussians. Here, we look at

general data which may not have a stochastic generation model.

We will later describe the last step in more detail. First, lets understand the central

advantage of doing the projection to V . It is simply that for any reasonable (unknown)

clustering of data points, the projection brings data points closer to their cluster centers.

This statement sounds mysterious and likely false, since the assertion is for ANY rea-

sonable unknown clustering. We quantify it in Theorem 7.4. First some notation: We

represent a k-clustering by a n  d matrix C (same dimensions as A), where row i of C is

216

Figure 7.3: Clusters in the full space and their projections

the center of the cluster to which data point i belongs. So, there are only k distinct rows

of C and each other row is a copy of one of these rows. The k-means objective function,

namely, the sum of squares of the distances of data points to their cluster centers is

n

(cid:88)

i=1

|ai  ci|2 = ||A  C||2

F .

The projection reduces the sum of distance squares to cluster centers from ||A  C||2

F

to at most 8k||A  C||2

2 in the projection. Recall that ||A  C||2 is the spectral norm,

which is the top singular value of A  C. Now, ||A  C||2

t (A) and often,

||A  C||F >>

k||A  C||2 and so the projection substantially reduces the sum of

squared distances to cluster centers.

F = (cid:80)

t 2



We will see later that in many clustering problems, including models like mixtures of

Gaussians and Stochastic Block Models of communities, there is a desired clustering C

where the regions overlap in the whole space, but are separated in the projection. Figure

7.3 is a schematic illustration. Now we state the theorem and give its surprisingly simple

proof.

Theorem 7.4 Let A be an n  d matrix with Ak the projection of the rows of A to the

subspace of the rst k right singular vectors of A. For any matrix C of rank less than or

equal to k

||Ak  C||2

F  8k||A  C||2

2.

217

Ak is a matrix that is close to every C, in the sense ||Ak  C||2

2. While

this seems contradictory, another way to state this is that for C far away from Ak in

Frobenius norm, ||A  C||2 will also be high.

F  8k||A  C||2

Proof: Since the rank of (Ak  C) is less than or equal to 2k,

||Ak  C||2

F  2k||Ak  C||2

2

and

||Ak  C||2  ||Ak  A||2 + ||A  C||2  2||A  C||2.

The last inequality follows since Ak is the best rank k approximation in spectral norm

(Theorem 3.9) and C has rank at most k. The theorem follows.



Suppose now in the clustering C we would like to nd, the cluster centers that are pairwise

at least (

k||A  C||2) apart. This holds for many clustering problems including data

generated by stochastic models. Then, it will be easy to see that in the projection,

most data points are a constant factor farther from centers of other clusters than their

own cluster center and this makes it very easy for the following algorithm to nd the

clustering C modulo a small fraction of errors.

7.5.2 The Algorithm



n by (C). In the next section, we give an interpretation of ||A  C||2

Denote ||A  C||2/

indicating that (C) is akin to the standard deviation of clustering C and hence the

notation (C). We assume for now that (C) is known to us for the desired clustering C.

This assumption can be removed by essentially doing a binary search.

Spectral Clustering - The Algorithm

1. Find the top k right singular vectors of data matrix A and project rows of A to the

space spanned by them to get Ak.(cf. Section 3.5).

2. Select a random row from Ak and form a cluster with all rows of Ak at distance less

than 6k(C)/ from it.

3. Repeat Step 2 k times.

Theorem 7.5 If in a k-clustering C, every pair of centers is separated by at least 15k(C)/

and every cluster has at least n points in it, then with probability at least 1  , Spectral

Clustering nds a clustering C (cid:48) that diers from C on at most 2n points.

Proof: Let vi denote row i of Ak. We rst show that for most data points, the projection

of data point is within distance 3k(C)/ of its cluster center. I.e., we show that |M | is

small, where,

M = {i : |vi  ci|  3k(C)/}.

218

Now, ||Ak  C||2

we get:

F = (cid:80)

i |vi  ci|2  (cid:80)

iM |vi  ci|2  |M | 9k22(C)

2

. So, using Theorem 7.4,

|M |

9k22(C)

2

 ||Ak  C||2

F  8kn2(C) = |M | 

82n

9k

.

(7.1)

Call a data point i good if i / M . For any two good data points i and j belong-

ing to the same cluster, since, their projections are within 3k(C)/ of the center of the

cluster, projections of the two data points are within 6k(C)/ of each other. On the

other hand, if two good data points i and k are in dierent clusters, since, the centers

of the two clusters are at least 15k(C)/ apart, their projections must be greater than

15k(C)/  6k(C)/ = 9k(C)/ apart. So, if we picked a good data point (say point

i) in Step 2, the set of good points we put in its cluster is exactly the set of good points

in the same cluster as i. Thus, if in each of the k executions of Step 2, we picked a good

point, all good points are correctly clustered and since |M |  2n, the theorem would hold.

To complete the proof, we must argue that the probability of any pick in Step 2 being

bad is small. The probability that the rst pick in Step 2 is bad is at most |M |/n  2/k.

For each subsequent execution of Step 2, all the good points in at least one cluster are

remaining candidates. So there are at least (2)n good points left and so the probability

that we pick a bad point is at most |M |/(  2)n which is at most /k. The union bound

over the k executions yields the desired result.

7.5.3 Means Separated by (1) Standard Deviations

For probability distribution on the real line, the mnemonic means separated by six

standard deviations suces to distinguish dierent distributions. Spectral Clustering

enables us to do the same thing in higher dimensions provided k  O(1) and six is

replaced by some constant. First we dene standard deviation for general not necessarily

stochastically generated data: it is just the maximum over all unit vectors v of the square

root of the mean squared distance of data points from their cluster centers in the direction

v, namely, the standard deviation (C) of clustering C is dened as:

(C)2 =

1

n

Maxv:|v|=1

n

(cid:88)

i=1

[(ai  ci)  v]2 =

1

n

Maxv:|v|=1|(A  C)v|2 =

1

n

||A  C||2

2.

This coincides with the denition of (C) we made earlier. Assuming k  O(1), it is easy

to see that the Theorem 7.5 can be restated as

If cluster centers in C are separated by ((C)), then the spectral clustering

algorithm nds C (cid:48) which diers from C only in a small fraction of data points.

It can be seen that the means separated by (1) standard deviations condition holds

for many stochastic models. We illustrate with two examples here. First, suppose we have

a mixture of k  O(1) spherical Gaussians, each with standard deviation one. The data

219

is generated according to this mixture. If the means of the Gaussians are (1) apart,

then the condition - means separated by (1) standard deviations- is satised and so if

we project to the SVD subspace and cluster, we will get (nearly) the correct clustering.

This was already discussed in detail in Chapter ??.

We discuss a second example. Stochastic Block Models are models of communities.

Suppose there are k communities C1, C2, . . . , Ck among a population of n people. Sup-

pose the probability of two people in the same community knowing each other is p and

if they are in dierent communities, the probability is q, where, q < p.34 We assume the

events that person i knows person j are independent across all i and j.

Specically, we are given an n  n data matrix A, where aij = 1 if and only if i and

j know each other. We assume the aij are independent random variables, and use ai to

denote the ith row of A. It is useful to think of A as the adjacency matrix of a graph, such

as the friendship network in Facebook. We will also think of the rows ai as data points.

The clustering problem is to classify the data points into the communities they belong to.

In practice, the graph is fairly sparse, i.e., p and q are small, namely, O(1/n) or O(ln n/n).

Consider the simple case of two communities with n/2 people in each and with

p =



n

q =



n

where ,   O(ln n).

Let u and v be the centroids of the data points in community one and community two

respectively; so ui  p for i  C1 and uj  q for j  C2 and vi  q for i  C1 and vj  p

for j  C2. We have

|u  v|2 =

n

(cid:88)

j=1

(uj  vj)2 

(  )2

n2

n =

(  )2

n

.

Inter-centroid distance 

  



n

.

(7.2)

We need to upper bound ||A  C||2. This is non-trivial since we have to prove a

uniform upper bound on |(A  C)v| for all unit vectors v. Fortunately, the subject to

Random Matrix Theory (RMT) already does this for us. RMT tells that

||A  C||2  O(



np) = O(



),

where, the O hides logarithmic factors. So as long as     (

means separated by (1) standard deviations and spectral clustering works.

), we have the



34More generally, for each pair of communities a and b, there could be a probability pab that a person

from community a knows a person from community b. But for the discussion here, we take paa = p for

all a and pab = q, for all a (cid:54)= b.

220

One important observation is that in these examples as well as many others, the k-

means objective function in the whole space is too high and so the projection is essential

before we can cluster.

7.5.4 Laplacians

An important special case of spectral clustering is when k = 2 and a spectral algorithm

is applied to the Laplacian matrix L of a graph, which is dened as

L = D  A

where A is the adjacency matrix and D is a diagonal matrix of degrees. Since A has a

negative sign, we look at the lowest two singular values and corresponding vectors rather

than the highest,

L is a symmetric matrix and is easily seen to be posiitve semi-denite: for any vector

x, we have

xT Lx =

(cid:88)

i

diix2

i 

(cid:88)

(i,j)E

xixj =

1

2

(cid:88)

(i,j)E

(xi  xj)2.

Also since all row sums of L (and L is symmetric) are zero, its lowest eignvalue is 0 with

the eigenvector 1 of all 1s. This is also the lowest singular vector of L. The projection

of all data points (rows) to this vector is just the origin and so gives no information. If

we take the second lowest singular vector and project to it which is essentially projecting

to the space of the bottom two singular vectors, we get the very simple problem of n real

numbers which we need to cluster into two clusters.

7.5.5 Local spectral clustering

So far our emphasis has been on partitioning the data into disjoint clusters. However,

the structure of many data sets consists of over lapping communities. In this case using

k-means with spectral clustering, the overlap of two communities shows up as a commu-

nity. This is illustrated in Figure 7.4.

(cid:19)

(cid:18) x

y

An alternative to using k-means with spectral clustering is to nd the minimum 1-

norm vector in the space spanned by the top singular vectors. Let A be the matrix whose

columns are the singular vectors. To nd a vector y in the space spanned by the columns

of A solve the linear system Ax = y. This is a slightly dierent looking problem then

Ax = c where c is a constant vector. To convert Ax = y to the more usual form write it

as [A, I]

= 0. However, if we want to minimize ||y||1 the solution is x = y = 0.

Thus we add the row 1, 1, . . . 1, 0, 0, . . . 0 to [A, I] and a 1 to the top of the vector [x, y]

to force the coordinates of x to add up to one. Minimizing ||y||1 does not appear to be a

linear program but we can write y = ya  yb and require ya  0 and yb  0. Now nding

221

the minimum one norm vector in the span of the columns of A is the linear program

(cid:32)

(cid:88)

min

yai +

(cid:88)

ybi

(cid:33)

i

i

subject to (A, I, I)



 = 0 ya  0 yb  0.





x

ya

yb

Local communities

In large social networks with a billion vertices, global clustering is likely to result in

communities of several hundred million vertices. What you may actually want is a local

community containing several individuals with only 50 vertices. To do this if one starts a

random walk at a vertex v and computes the frequency of visiting vertices, it will converge

to the rst singular vector. However, the distribution after a small number of steps will

be primarily in the small community containing v and will be proportional to the rst

singular vector distribution restricted to the vertices in the small community containing v,

only will be higher by some constant value. If one wants to determine the local communi-

ties containing vertices v1, v2, and v3, start with three probability distributions, one with

probability one at v1, one with probability one at v2, and one with probability one at v3

and nd early approximation to the rst three singular vectors. Then nd the minimum

1-norm vector in the space spanned by the early approximations.

Hidden structure

In the previous section we discussed overlapping communities. Another issue is hidden

structure. Suppose the vertices of a social network could be partitioned into a number of

strongly connected communities. By strongly connected we mean the probability of an

edge between two vertices in a community is much higher than the probability of an edge

between two vertices in dierent communities. Suppose the vertices of the graph could

be partitioned in another way which was incoherent35 with the rst partitioning and the

probability of an edge between two vertices in one of these communities is higher than an

edge between two vertices in dierent communities. If the probability of an edge between

two vertices in a community of this second partitioning is less than that in the rst, then

a clustering algorithm is likely to nd the rst partitioning rather than the second. How-

ever, the second partitioning, which we refer to as hidden structure, may be the structure

that we want to nd. The way to do this is to use your favorite clustering algorithm to

produce the dominant structure and then stochastically weaken the dominant structure

by removing some community edges in the graph. Now if you apply the clustering algo-

rithm to the modied graph, it can nd the hidden community structure. Having done

this, go back to the original graph, weaken the hidden structure and reapply the cluster-

ing algorithm. It will now nd a better approximation to the dominant structure. This

technology can be used to nd a number of hidden levels in several types of social networks.

35incoherent, give denition

222





























1 1 1 1 1 1 0 0 0

1 1 1 1 1 1 0 0 0

1 1 1 1 1 1 0 0 0

1 1 1 1 1 1 0 0 0

1 1 1 1 1 1 1 1 1

1 1 1 1 1 1 1 1 1

0 0 0 0 1 1 0 0 0

0 0 0 0 1 1 0 0 0

0 0 0 0 1 1 0 0 0

(a)





















































































0.31

0.33

0.31

0.33

0.31

0.33

0.33

0.31

0.44 0.09

0.44 0.09

0.24 0.49

0.24 0.49

0.24 0.49

(b)

(c);

Figure 7.4: (a) illustrates the adjacency matrix of a graph with a six vertex clique that

overlaps a ve vertex clique in two vertices. (b) illustrates the matrix where columns

consist of the top two singular vectors, and (c) illustrates the mapping of rows in the

singular vector matrix to three points in two dimensional space. Instead of two cliques

we get the non overlapping portion of each of the two clique plus their intersection as

communities instead of the two cliques as communities.

Block model

One technique for generating graphs with communities is to use the block model where

the vertices are partitioned into blocks and each block is a GNP graph generated with

some edge probability. The edges in o diagonal blocks are generated with a lower prob-

ability. One can also generate graphs with hidden structure. For example, the vertices in

an n vertex graph might be partitioned into two communities, the rst community having

vertices 1 to n/2 and the second community having vertices n/2 + 1 to n. The dominant

structure is generated with probability p1 for edges within communities and probability q1

for edges between communities. The the vertices are randomly permuted and the hidden

structure is generated using the rst n/2 vertices in the permuted order for one commu-

nity and the remaining vertices for the second community with probabilities p2 and q2

which ar lower than p1 and q1..

An interesting question is how to determine the quality of a community found. Many

researchers use an existing standard of what the communities are. However, if you want

to using clustering techniques to nd communities there probably is no external standard

or you would just use that instead of clustering. A way to determine if you have found a

real community structure is to ask if the graph is more likely generated by a model of the

structure found than by a completely random model. Suppose you found a partition of

two communities each with n/2 vertices. Using the number of edges in each community

and the number of inter community edges ask what is the probability of the graph being

generated by a bloc model where p and q are the probabilities determined by the edge

density within communities and the edge density between communities. One can compare

this probability with the probability that the graph was generated by a GNP model with

probability (p + q)/2.

223

7.6 Approximation Stability

7.6.1 The Conceptual Idea

We now consider another condition that will allow us to produce accurate clusters

from data. To think about this condition, imagine that we are given a few thousand news

articles that we want to cluster by topic. These articles could be represented as points in

a high-dimensional space (e.g., axes could correspond to dierent meaningful words, with

coordinate i indicating the frequency of that word in a given article). Or, alternatively, it

could be that we have developed some text-processing program that given two articles x

and y computes some measure of distance d(x, y) between them. We assume there exists

some correct clustering CT of our news articles into k topics; of course, we do not know

what CT is, that is what we want our algorithm to nd.

If we are clustering with an algorithm that aims to minimize the k-means score of its

solution, then implicitly this means we believe that the clustering COP T

kmeans of minimum k-

means score is either equal to, or very similar to, the clustering CT . Unfortunately, nding

the clustering of minimum k-means score is NP-hard. So, let us broaden our belief a bit

and assume that any clustering C whose k-means score is within 10% of the minimum

is also very similar to CT . This should give us a little bit more slack. Unfortunately,

nding a clustering of score within 10% of the minimum is also an NP-hard problem.

Nonetheless, we will be able to use this assumption to eciently nd a clustering that is

close to CT . The trick is that NP-hardness is a worst-case notion, whereas in contrast,

this assumption implies structure on our data.In particular, it implies that all clusterings

that have score within 10% of the minimum have to be similar to each other. We will

then be able to utilize this structure in a natural ball-growing clustering algorithm.

7.6.2 Making this Formal

1, . . . , C (cid:48)

To make this discussion formal, we rst specify what we mean when we say that two

dierent ways of clustering some data are similar to each other. Let C = {C1, . . . , Ck}

and C(cid:48) = {C (cid:48)

k} be two dierent k-clusterings of some dataset A. For example, C

could be the clustering that our algorithm produces, and C(cid:48) could be the clustering CT .

Let us dene the distance between these two clusterings to be the fraction of points that

would have to be re-clustered in C to make C match C(cid:48), where by match we mean that

there should be a bijection between the clusters of C and the clusters of C(cid:48). We can write

this distance mathematically as:

dist(C, C(cid:48)) = min



1

n

k

(cid:88)

i=1

|Ci \ C (cid:48)

(i)|,

where the minimum is over all permutations  of {1, . . . , k}.

For c > 1 and (cid:15) > 0 we say that a data set satises (c, (cid:15))-approximation-stability

with respect to a given objective (such as k-means or k-median) if every clustering C

224

whose cost is within a factor c of the minimum-cost clustering for that objective satises

dist(C, CT ) < (cid:15). That is, it is sucient to be within a factor c of optimal to the our

objective in order for the fraction of points clustered incorrectly to be less than (cid:15). We will

specically focus in this discussion on the k-median objective rather than the k-means

objective, since it is a bit easier to work with.

What we will now show is that under this condition, even though it may be NP-hard

in general to nd a clustering that is within a factor c of optimal, we can nonetheless

eciently nd a clustering C(cid:48) such that dist(C(cid:48), CT )  (cid:15), so long as all clusters in CT are

reasonably large. To simplify notation, let C denote the clustering of minimum k-median

cost, and to keep the discussion simpler, let us also assume that CT = C; that is, the

target clustering is also the clustering with the minimum k-median score.

7.6.3 Algorithm and Analysis

Before presenting an algorithm, we begin with a helpful lemma that will guide our

design. For a given data point ai, dene its weight w(ai) to be its distance to the center

of its cluster in C. Notice that the k-median cost of C is OP T = (cid:80)n

i=1 w(ai). Dene

wavg = OP T /n to be the average weight of the points in A. Finally, dene w2(ai) to be

the distance of ai to its second-closest center in C.

Lemma 7.6 Assume dataset A satises (c, (cid:15)) approximation-stability with respect to the

k-median objective, each cluster in CT has size at least 2(cid:15)n, and CT = C. Then,

1. Fewer than (cid:15)n points ai have w2(ai)  w(ai)  (c  1)wavg/(cid:15).

2. At most 5(cid:15)n/(c  1) points ai have w(ai)  (c  1)wavg/(5(cid:15)).

Proof: For part (1), suppose that (cid:15)n points ai have w2(ai)  w(ai)  (c  1)wavg/(cid:15).

Consider modifying CT to a new clustering C(cid:48) by moving each of these points ai into

the cluster containing its second-closest center. By assumption, the k-means cost of the

clustering has increased by at most (cid:15)n(c  1)wavg/(cid:15) = (c  1)  OP T. This means that

the cost of the new clustering is at most c OP T . However, dist(C(cid:48), CT ) = (cid:15) because (a) we

moved (cid:15)n points to dierent clusters, and (b) each cluster in CT has size at least 2(cid:15)n so the

optimal permutation  in the denition of dist remains the identity. So, this contradicts

approximation stability. Part (2) follows from the denition of average; if it did not

hold then (cid:80)n

i=1 w(ai) > nwavg, a contradiction.

A datapoint ai is bad if it satises either item (1) or (2) of Lemma 7.6 and good if it

satises neither one. So, there are at most b = (cid:15)n + 5(cid:15)n

c1 bad points and the rest are good.

Dene critical distance dcrit = (c1)wavg

. Lemma 7.6 implies that the good points have

distance at most dcrit to the center of their own cluster in C and distance at least 5dcrit

to the center of any other cluster in C.

5(cid:15)

225

This suggests the following algorithm. Suppose we create a graph G with the points

ai as vertices, and edges between any two points ai and aj with d(ai, aj) < 2dcrit. Notice

that by triangle inequality, the good points within the same cluster in C have distance

less than 2dcrit from each other so they will be fully connected and form a clique. Also,

again by triangle inequality, any edge that goes between dierent clusters must be be-

tween two bad points. In particular, if ai is a good point in one cluster, and it has an edge

to some other point aj, then aj must have distance less than 3dcrit to the center of ais

cluster. This means that if aj had a dierent closest center, which obviously would also

be at distance less than 3dcrit, then ai would have distance less than 2dcrit + 3dcrit = 5dcrit

to that center, violating its goodness. So, bridges in G between dierent clusters can only

occur between bad points.

Assume now that each cluster in CT has size at least 2b+1; this is the sense in which we

are requiring that (cid:15)n be small compared to the smallest cluster in CT . In this case, create

a new graph H by connecting any two points ai and aj that share at least b + 1 neighbors

in common in G, themselves included. Since every cluster has at least 2b + 1  b = b + 1

good points, and these points are fully connected in G, this means that H will contain an

edge between every pair of good points in the same cluster. On the other hand, since the

only edges in G between dierent clusters are between bad points, and there are at most

b bad points, this means that H will not have any edges between dierent clusters in CT .

Thus, if we take the k largest connected components in H, these will all correspond to

subsets of dierent clusters in CT , with at most b points remaining.

At this point we have a correct clustering of all but at most b points in A. Call these

clusters C1, . . . , Ck, where Cj  C 

j . To cluster the remaining points ai, we assign them

to the cluster Cj that minimizes the median distance between ai and points in Cj. Since

each Cj has more good points than bad points, and each good point in Cj has distance at

most dcrit to center c

j , by triangle inequality the median of these distances must lie in the

range [d(ai, c

i ) + dcrit]. This means that this second step will correctly

cluster all points ai for which w2(ai)  w(ai) > 2dcrit. In particular, we correctly cluster

all points except possibly for some of the at most (cid:15)n satisfying item (1) of Lemma 7.6.

i )  dcrit, d(ai, c

The above discussion assumes the value dcrit is known to our algorithm; we leave it as

an exercise to the reader to modify the algorithm to remove this assumption. Summariz-

ing, we have the following algorithm and theorem.

Algorithm k-Median Stability (given c, (cid:15), dcrit)

1. Create a graph G with a vertex for each datapoint in A, and an edge between

vertices i and j if d(ai, aj)  2dcrit.

2. Create a graph H with a vertex for each vertex in G and an edge between vertices i

and j if i and j share at least b + 1 neighbors in common, themselves included, for

b = (cid:15)n + 5(cid:15)n

c1. Let C1, . . . , Ck denote the k largest connected components in H.

226

3. Assign each point not in C1  . . .  Ck to the cluster Cj of smallest median distance.

Theorem 7.7 Assume A satises (c, (cid:15)) approximation-stability with respect to the k-

median objective, that each cluster in CT has size at least 10(cid:15)

c1n+2(cid:15)n+1, and that CT = C.

Then Algorithm k-Median Stability will nd a clustering C such that dist(C, CT )  (cid:15).

7.7 High-Density Clusters

We now turn from the assumption that clusters are center-based to the assumption

that clusters consist of high-density regions, separated by low-density moats such as in

Figure 7.1.

7.7.1 Single Linkage

One natural algorithm for clustering under the high-density assumption is called single

linkage. This algorithm begins with each point in its own cluster and then repeatedly

merges the two closest clusters into one, where the distance between two clusters is

dened as the minimum distance between points in each cluster. That is, dmin(C, C (cid:48)) =

minxC,yC(cid:48) d(x, y), and the algorithm merges the two clusters C and C (cid:48) whose dmin value

is smallest over all pairs of clusters breaking ties arbitrarily. It then continues until there

are only k clusters. This is called an agglomerative clustering algorithm because it begins

with many clusters and then starts merging, or agglomerating them together.36 Single-

linkage is equivalent to running Kruskals minimum-spanning-tree algorithm, but halting

when there are k trees remaining. The following theorem is fairly immediate.

Theorem 7.8 Suppose the desired clustering C 

exists some distance  such that

1 , . . . , C 

k satises the property that there

1. any two data points in dierent clusters have distance at least , and

2. for any cluster C 

i and any partition of C 

i into two non-empty sets A and C 

i \ A,

there exist points on each side of the partition of distance less than .

Then, single-linkage will correctly recover the clustering C 

1 , . . . , C 

k .

Proof: Consider running the algorithm until all pairs of clusters C and C (cid:48) have dmin(C, C (cid:48))  .

At that point, by (2), each target cluster C 

i will be fully contained within some cluster

of the single-linkage algorithm. On the other hand, by (1) and by induction, each cluster

C of the single-linkage algorithm will be fully contained within some C 

i of the target

clustering, since any merger of subsets of distinct target clusters would require dmin  .

Therefore, the single-linkage clusters are indeed the target clusters.

36Other agglomerative algorithms include complete linkage which merges the two clusters whose max-

imum distance between points is smallest, and Wards algorithm described earlier that merges the two

clusters that cause the k-means cost to increase by the least.

227

7.7.2 Robust Linkage

The single-linkage algorithm is fairly brittle. A few points bridging the gap between

two dierent clusters can cause it to do the wrong thing. As a result, there has been

signicant work developing more robust versions of the algorithm.

One commonly used robust version of single linkage is Wisharts algorithm. A ball

of radius r is created for each point with the point as center. The radius r is gradually

increased starting from r = 0. The algorithm has a parameter t. When a ball has t or

more points the center point becomes active. When two balls with active centers intersect

the two center points are connected by an edge. The parameter t prevents a thin string

of points between two clusters from causing a spurious merger. Note that Wisharts al-

gorithm with t = 1 is the same as single linkage.

In fact, if one slightly modies the algorithm to dene a point to be live if its ball

of radius r/2 contains at least t points, then it is known [CD10] that a value of t =

O(d log n) is sucient to recover a nearly correct solution under a natural distributional

formulation of the clustering problem. Specically, suppose data points are drawn from

some probability distribution D over Rd, and that the clusters correspond to high-density

regions surrounded by lower-density moats. More specically, the assumption is that

1. for some distance  > 0, the -interior of each target cluster C 

i has density at least

some quantity  (the -interior is the set of all points at distance at least  from

the boundary of the cluster),

2. the region between target clusters has density less than (1  (cid:15)) for some (cid:15) > 0,

3. the clusters should be separated by distance greater than 2, and

4. the -interior of the clusters contains most of their probability mass.

Then, for suciently large n, the algorithm will with high probability nd nearly correct

clusters. In this formulation, we allow points in low-density regions that are not in any

target clusters at all. For details, see [CD10].

Robust Median Neighborhood Linkage robusties single linkage in a dierent way.

This algorithm guarantees that if it is possible to delete a small fraction of the data such

that for all remaining points x, most of their |C (x)| nearest neighbors indeed belong to

their own cluster C (x), then the hierarchy on clusters produced by the algorithm will

include a close approximation to the true clustering. We refer the reader to [BLG14] for

the algorithm and proof.

7.8 Kernel Methods

Kernel methods combine aspects of both center-based and density-based clustering.

In center-based approaches like k-means or k-center, once the cluster centers are xed, the

228

Voronoi diagram of the cluster centers determines which cluster each data point belongs

to. This implies that clusters are pairwise linearly separable.

If we believe that the true desired clusters may not be linearly separable, and yet we

wish to use a center-based method, then one approach, as in the chapter on learning,

is to use a kernel. Recall that a kernel function K(x, y) can be viewed as performing

an implicit mapping  of the data into a possibly much higher dimensional space, and

then taking a dot-product in that space. That is, K(x, y) = (x)  (y). This is then

viewed as the anity between points x and y. We can extract distances in this new

space using the equation |z1  z2|2 = z1  z1 + z2  z2  2z1  z2, so in particular we have

|(x)  (y)|2 = K(x, x) + K(y, y)  2K(x, y). We can then run a center-based clustering

algorithm on these new distances.

One popular kernel function to use is the Gaussian kernel. The Gaussian kernel uses

an anity measure that emphasizes closeness of points and drops o exponentially as the

points get farther apart. Specically, we dene the anity between points x and y by

K(x, y) = e 1

22 (cid:107)xy(cid:107)2

.

Another way to use anities is to put them in an anity matrix, or weighted graph.

This graph can then be separated into clusters using a graph partitioning procedure such

as the one in following section.

7.9 Recursive Clustering based on Sparse Cuts

We now consider the case that data are nodes in an undirected connected graph

G(V, E) where an edge indicates that the end point vertices are similar. Recursive clus-

tering starts with all vertices in one cluster and recursively splits a cluster into two parts

whenever there is a small number of edges from one part to the other part of the cluster.

Formally, for two disjoint sets S and T of vertices, dene

(S, T ) =

Number of edges from S to T

Total number of edges incident to S in G

.

(S, T ) measures the relative strength of similarities between S and T . Let d(i) be the

degree of vertex i and for a subset S of vertices, let d(S) = (cid:80)

iS d(i). Let m be the total

number of edges in the graph. The following algorithm aims to cut only a small fraction

of the edges and to produce clusters that are internally consistent in that no subset of the

cluster has low similarity to the rest of the cluster.

Recursive Clustering: Select an appropriate value for (cid:15). If a current cluster

W has a subset S with d(S)  1

2d(W ) and (S, S  W )  , then split W

into two clusters S and W \ S. Repeat until no such split is possible.

Theorem 7.9 At termination of Recursive Clustering, the total number of edges between

vertices in dierent clusters is at most O(m ln n).

229

Proof: Each edge between two dierent clusters at the end was deleted at some stage

by the algorithm. We will charge edge deletes to vertices and bound the total charge.

When the algorithm partitions a cluster W into S and W \S with d(S)  (1/2)d(W ), each

k  S is charged d(k)

d(W ) times the number of edges being deleted. Since (S, W \ S)  ,

the charge added to each k  W is a most d(k). A vertex is charged only when it is

in the smaller part, d(S)  d(W )/2, of the cut. So between any two times it is charged,

d(W ) is reduced by a factor of at least two and so a vertex can be charged at most

log2 m  O(ln n) times, proving the theorem.

Implementing the algorithm requires computing MinSW (S, W \ S) which is an NP-

hard problem. So the theorem cannot be implemented right away. Luckily, eigenvalues and

eigenvectors, which can be computed fast, give an approximate answer. The connection

between eigenvalues and sparsity, known as Cheegers inequality, is deep with applications

to Markov chains among others. We do not discuss this here.

7.10 Dense Submatrices and Communities

Represent n data points in d-space by the rows of an n  d matrix A. Assume that A

has all nonnegative entries. Examples to keep in mind for this section are the document-

term matrix and the customer-product matrix. We address the question of how to dene

and nd eciently a coherent large subset of rows. To this end, the matrix A can be

represented by a bipartite graph Figure 7.5. One side has a vertex for each row and the

other side a vertex for each column. Between the vertex for row i and the vertex for

column j, there is an edge with weight aij.

We want a subset S of row vertices and a subset T of column vertices so that

A(S, T ) =

(cid:88)

aij

iS,jT

is high. This simple denition is not good since A(S, T ) will be maximized by taking

all rows and columns. We need a balancing criterion that ensures that A(S, T ) is high

relative to the sizes of S and T . One possibility is to maximize A(S,T )

|S||T | . This is not a good

measure either, since it is maximized by the single edge of highest weight. The denition

we use is the following. Let A be a matrix with nonnegative entries. For a subset S of

rows and a subset T of columns, the density d(S, T ) of S and T is d(S, T ) = A(S,T )

. The

|S||T |

density d(A) of A is dened as the maximum value of d(S, T ) over all subsets of rows and

columns. This denition applies to bipartite as well as non bipartite graphs.

One important case is when As rows and columns both represent the same set and

aij is the similarity between object i and object j. Here d(S, S) = A(S,S)

. If A is an n  n

0-1 matrix, it can be thought of as the adjacency matrix of an undirected graph, and

d(S, S) is the average degree of a vertex in S. The subgraph of maximum average degree

in a graph can be found exactly by network ow techniques, as we will show in the next

|S|

230

Figure 7.5: Example of a bipartite graph.

section. We do not know an ecient (polynomial-time) algorithm for nding d(A) exactly

in general. However, we show that d(A) is within a O(log2 n) factor of the top singular

value of A assuming |aij|  1 for all i and j. This is a theoretical result. The gap may be

much less than O(log2 n) for many problems, making singular values and singular vectors

quite useful. Also, S and T with d(S, T )  (d(A)/ log2 n) can be found algorithmically.

Theorem 7.10 Let A be an n  d matrix with entries between 0 and 1. Then

1(A)  d(A) 

1(A)

4 log n log d

.

Furthermore, subsets S and T satisfying d(S, T )  1(A)

singular vector of A.

4 log n log d may be found from the top

Proof: Let S and T be the subsets of rows and columns that achieve d(A) = d(S, T ).

Consider an n-vector u that is

on S and 0 elsewhere and a d-vector v that is

1

1

|S|

|T |

on T and 0 elsewhere. Then,

1 (A)  uT Av = (cid:80)

uivjaij = d(S, T ) = d(A)

ij

establishing the rst inequality.

To prove the second inequality, express 1 (A) in terms of the rst left and right

singular vectors x and y.

1(A) = xT Ay =

(cid:88)

i,j

xiaijyj,

|x| = |y| = 1.

Since the entries of A are nonnegative, the components of the rst left and right singular

vectors must all be nonnegative, that is, xi  0 and yj  0 for all i and j. To bound

(cid:80)

xiaijyj, break the summation into O (log n log d) parts. Each part corresponds to a

i,j

given  and  and consists of all i such that   xi < 2 and all j such that   yi < 2.

The log n log d parts are dened by breaking the rows into log n blocks with  equal to

1

n , . . . , 1 and by breaking the columns into log d blocks with  equal

2

n , 2 1

1

n , 4 1

1

n ,

231

,

1

d

1

d

to 1

2

,

2

d

be ignored at a loss of at most 1

, . . . , 1. The i such that xi < 1



2

will

41(A). Exercise 7.27 proves the loss is at most this amount.

n and the j such that yj < 1

4

d



,

d

2

Since (cid:80)

i

i = 1, the set S = {i|  xi < 2} has |S|  1

x2

2 and similarly,

T = {j|  yj  2} has |T |  1

2 . Thus

(cid:88)

(cid:88)

xiyjaij  4A(S, T )

i

xi2

j

yj 2

 4d(S, T )(cid:112)|S||T |

 4d(S, T )

 4d(A).

From this it follows that

or

proving the second inequality.

1 (A)  4d (A) log n log d

d (A)  1(A)

4 log n log d

It is clear that for each of the values of (, ), we can compute A(S, T ) and d(S, T )

as above and taking the best of these d(S, T ) s gives us an algorithm as claimed in the

theorem.

Note that in many cases, the nonzero values of xi and yj after zeroing out the low

entries will only go from 1

for yj, since the singular vectors

2

are likely to be balanced given that aij are all between 0 and 1. In this case, there will

be O(1) groups only and the log factors disappear.

n for xi and 1

n to c

1

to c

d

1

d

2

Another measure of density is based on similarities. Recall that the similarity between

objects represented by vectors (rows of A) is dened by their dot products. Thus, simi-

larities are entries of the matrix AAT . Dene the average cohesion f (S) of a set S of rows

of A to be the sum of all pairwise dot products of rows in S divided by |S|. The average

cohesion of A is the maximum over all subsets of rows of the average cohesion of the subset.

Since the singular values of AAT are squares of singular values of A, we expect f (A)

to be related to 1(A)2 and d(A)2. Indeed it is. We state the following without proof.

Lemma 7.11 d(A)2  f (A)  d(A) log n. Also, 1(A)2  f (A)  c1(A)2

log n .

f (A) can be found exactly using ow techniques as we will see later.

232

7.11 Community Finding and Graph Partitioning

Assume that data are nodes in a possibly weighted graph where edges represent some

notion of anity between their endpoints. In particular, let G = (V, E) be a weighted

graph. Given two sets of nodes S and T , dene

E(S, T ) =

eij.

(cid:88)

iS

jT

We then dene the density of a set S to be

d(S, S) =

E(S, S)

|S|

.

If G is an undirected graph, then d(S, S) can be viewed as the average degree in the

vertex-induced subgraph over S. The set S of maximum density is therefore the subgraph

of maximum average degree. Finding such a set can be viewed as nding a tight-knit

community inside some network. In the next section, we describe an algorithm for nding

such a set using network ow techniques.

Flow Methods

Here we consider dense induced subgraphs of a graph. An

induced subgraph of a graph consisting of a subset of the vertices of the graph along with

all edges of the graph that connect pairs of vertices in the subset of vertices. We show

that nding an induced subgraph with maximum average degree can be done by network

ow techniques. This is simply maximizing the density d(S, S) over all subsets S of the

graph. First consider the problem of nding a subset of vertices such that the induced

subgraph has average degree at least  for some parameter . Then do a binary search

on the value of  until the maximum  for which there exists a subgraph with average

degree at least  is found.

Given a graph G in which one wants to nd a dense subgraph, construct a directed

graph H from the given graph and then carry out a ow computation on H. H has a

node for each edge of the original graph, a node for each vertex of the original graph,

plus two additional nodes s and t. There is a directed edge with capacity one from s to

each node corresponding to an edge of the original graph and a directed edge with innite

capacity from each node corresponding to an edge of the original graph to the two nodes

corresponding to the vertices the edge connects. Finally, there is a directed edge with

capacity  from each node corresponding to a vertex of the original graph to t.

Notice there are three types of cut sets of the directed graph that have nite capacity,

Figure 7.6. The rst cuts all arcs from the source. It has capacity e, the number of edges

of the original graph. The second cuts all edges into the sink. It has capacity v, where v

is the number of vertices of the original graph. The third cuts some arcs from s and some

arcs into t. It partitions the set of vertices and the set of edges of the original graph into

two blocks. The rst block contains the source node s, a subset of the edges es, and a

233

1

1

1

s













[u,v]

[v,w]

[w,x]

edges

u

v

w

x

vertices









t

Figure 7.6: The directed graph H used by the ow technique to nd a dense subgraph

subset of the vertices vs dened by the subset of edges. The rst block must contain both

end points of each edge in es; otherwise an innite arc will be in the cut. The second block

contains t and the remaining edges and vertices. The edges in this second block either

connect vertices in the second block or have one endpoint in each block. The cut set will

cut some innite arcs from edges not in es coming into vertices in vs. However, these

arcs are directed from nodes in the block containing t to nodes in the block containing s.

Note that any nite capacity cut that leaves an edge node connected to s must cut the

two related vertex nodes from t, Figure 7.6. Thus, there is a cut of capacity e  es + vs

where vs and es are the vertices and edges of a subgraph. For this cut to be the minimal

cut, the quantity e  es + vs must be minimal over all subsets of vertices of the original

graph and the capcity must be less than e and also less than v.

> e

If there is a subgraph with vs vertices and es edges where the ratio es

vs

is suciently

large so that eS

v , then for  such that eS

v , es  vs > 0 and e  es + vs < e.

vS

vS

Similarly e < v and thus e  es + vs < v. This implies that the cut e  es + vs is less

than either e or v and the ow algorithm will nd a nontrivial cut and hence a proper

subset. For dierent values of  in the above range there maybe dierent nontrivial cuts.

>  > e

Note that for a given density of edges, the number of edges grows as the square of the

v if vS is small. Thus, the ow method

. To nd small communities one

number of vertices and es

vs

works well in nding large subsets since it works with eS

vS

would need to use a method that worked with eS

v2

S

as the following example illustrates.

is less likely to exceed e

Example: Consider nding a dense subgraph of 1,000 vertices and 2,000 internal edges in

a graph of 106 vertices and 6106 edges. For concreteness, assume the graph was generated

by the following process. First, a 1,000-vertex graph with 2,000 edges was generated as a

234

s

1









t

cut

edges and vertices

in the community

Figure 7.7: Cut in ow graph

random regular degree four graph. The 1,000-vertex graph was then augmented to have

106 vertices and edges were added at random until all vertices were of degree 12. Note

that each vertex among the rst 1,000 has four edges to other vertices among the rst

1,000 and eight edges to other vertices. The graph on the 1,000 vertices is much denser

than the whole graph in some sense. Although the subgraph induced by the 1,000 vertices

has four edges per vertex and the full graph has twelve edges per vertex, the probability

of two vertices of the 1,000 being connected by an edge is much higher than for the graph

as a whole. The probability is given by the ratio of the actual number of edges connecting

vertices among the 1,000 to the number of possible edges if the vertices formed a complete

graph.

p =

e

(cid:1)(cid:1) =

(cid:0)(cid:0)v

2

2e

v(v  1)

For the 1,000 vertices, this number is p = 22,000

1,000999

number is p = 26106

connected should allow us to nd the dense subgraph.

= 4  103. For the entire graph this

106106 = 12  106. This dierence in probability of two vertices being

In our example, the cut of all arcs out of s is of capacity 6  106, the total number

of edges in the graph, and the cut of all arcs into t is of capacity  times the number

of vertices or   106. A cut separating the 1,000 vertices and 2,000 edges would have

capacity 6  106  2, 000 +   1, 000. This cut cannot be the minimum cut for any value

of  since es

v . The point is that to nd the 1,000 vertices, we

vs

have to maximize A(S, S)/|S|2 rather than A(S, S)/|S|. Note that A(S, S)/|S|2 penalizes

large |S| much more and therefore can nd the 1,000 node dense subgraph.

v = 6, hence es

vs

= 2 and e

< e

235

A =













1 1 1 0 0

1 1 1 0 0

1 1 1 0 0

0 0 0 1 1

0 0 0 1 1













V =













1 0

1 0

1 0

0 1

0 1













Figure 7.8: Illustration of spectral clustering.

7.12 Spectral clustering applied to social networks

Finding communities in social networks is dierent from other clustering for several

reasons. First we often want to nd communities of size say 20 to 50 in networks with

100 million vertices. Second a person is in a number of overlapping communities and thus

we are not nding disjoint clusters. Third there often are a number of levels of structure

and a set of dominant communities may be hiding a set of weaker communities that are

of interest. Spectral clustering is one approach to these issues.

In spectral clustering of the vertices of a graph, one creates a matrix V whose columns

correspond to the rst k singular vectors of the adjacency matrix. Each row of V is

the projection of a row of the adjacency matrix to the space spanned by the k singular

vectors. In the example below, the graph has ve vertices divided into two cliques, one

consisting of the rst three vertices and the other the last two vertices. The top two right

singular vectors of the adjacency matrix, not normalized to length one, are (1, 1, 1, 0, 0)T

and (0, 0, 0, 1, 1)T . The ve rows of the adjacency matrix projected to these vectors form

the 5  2 matrix in Figure 7.8. Here, there are two ideal clusters with all edges inside a

cluster being present including self-loops and all edges between clusters being absent. The

ve rows project to just two points, depending on which cluster the rows are in. If the

clusters were not so ideal and instead of the graph consisting of two disconnected cliques,

the graph consisted of two dense subsets of vertices where the two sets were connected by

only a few edges, then the singular vectors would not be indicator vectors for the clusters

but close to indicator vectors. The rows would be mapped to two clusters of points instead

of two points. A k-means clustering algorithm would nd the clusters.

If the clusters were overlapping, then instead of two clusters of points, there would be

three clusters of points where the third cluster corresponds to the overlapping vertices of

the two clusters. Instead of using k-means clustering, we might instead nd the minimum

1-norm vector in the space spanned by the two singular vectors. The minimum 1-norm

vector will not be an indicator vector, so we would threshold its values to create an

indicator vector for a cluster. Instead of nding the minimum 1-norm vector in the space

spanned by the singular vectors in V, we might look for a small 1-norm vector close to

the subspace.

min

x

(1  |x|1 +  cos())

236

Here  is the cosine of the angle between x and the space spanned by the two singular

vectors.  is a control parameter that determines how close we want the vector to be to

the subspace. When  is large, x must be close to the subspace. When  is zero, x can

be anywhere.

Finding the minimum 1-norm vector in the space spanned by a set of vectors can be

formulated as a linear programming problem. To nd the minimum 1-norm vector in V,

write V x = y where we want to solve for both x and y. Note that the format is dierent

from the usual format for a set of linear equations Ax = b where b is a known vector.

Finding the minimum 1-norm vector looks like a nonlinear problem.

min |y|1 subject to V x = y

To remove the absolute value sign, write y = y1  y2 with y1  0 and y2  0. Then solve

min

(cid:32) n

(cid:88)

i=1

y1i +

n

(cid:88)

i=1

(cid:33)

y2i

subject to V x = y, y1  0, and y2  0.

Write V x = y1  y2 as V x  y1 + y2 = 0. then we have the linear equations in a format

we are accustomed to.

[V, I, I]



 =





x

y1

y2





















0

0

...

0

This is a linear programming problem. The solution, however, happens to be x = 0,

y1 = 0, and y2 = 0. To resolve this, add the equation y1i = 1 to get a community con-

taining the vertex i.

Often we are looking for communities of 50 or 100 vertices in graphs with hundreds of

million of vertices. We want a method to nd such communities in time proportional to

the size of the community and not the size of the entire graph. Here spectral clustering

can be used but instead of calculating singular vectors of the entire graph, we do some-

thing else. Consider a random walk on a graph. If we walk long enough the probability

distribution converges to the rst eigenvector. However, if we take only a few steps from a

start vertex or small group of vertices that we believe dene a cluster, the probability will

distribute over the cluster with some of the probability leaking out to the remainder of

the graph. To get the early convergence of several vectors that ultimately converge to the

rst few singular vectors, take a subspace [x, Ax, A2x, A3x] and propagate the subspace.

At each iteration nd an orthonormal basis and then multiply each basis vector by A.

Then take the resulting basis vectors after a few steps, say ve, and nd a minimum

1-norm vector in the subspace.

237

A third issue that arises is when a dominant structure hides an important weaker

structure. One can run their algorithm to nd the dominant structure and then weaken the

dominant structure by randomly removing edges in the clusters so that the edge density is

similar to the remainder of the network. Then reapplying the algorithm often will uncover

weaker structure. Real networks often have several levels of structure. The technique

can also be used to improve state of the art clustering algorithms. After weakening the

dominant structure to nd the weaker hidden structure one can go back to the original data

and weaken the hidden structure and reapply the algorithm to again nd the dominant

structure. This improves most state of the art clustering algorithms.

238

7.13 Bibliographic Notes

Clustering has a long history. For a good general survey, see [Jai10]. For a collection

of surveys giving an in-depth treatment of many dierent approaches to, applications of,

and perspectives on clustering, see [HMMR15]; e.g., see [AB15] for a good discussion of

center-based clustering. Lloyds algorithm for k-means clustering is from [Llo82], and

The k-means++ initialization method is due to Arthur and Vassilvitskii [AV07]. Wards

algorithm, from 1963, appears in [War63]. Analysis of the farthest-traversal algorithm for

the k-center problem is due to Gonzalez [Gon85].

Theorem 7.4 is from [KV09]. Analysis of spectral clustering in stochastic models is

given in [McS01], and the analysis of spectral clustering without a stochastic model and

7.5.2 is due to [KK10].

The denition of approximation-stability is from [BBG13] and [BBV08], and the anal-

ysis given in Section 7.6 is due to [BBG13].

Single-linkage clustering goes back at least to Florek et al. [F(cid:32)LP+51], and Wisharts

robust version is from [Wis69]. Extensions of Theorem 7.8 are given in [BBV08], and the-

oretical guarantees for dierent forms of robust linkage are given in [CD10] and [BLG14].

A good survey of kernel methods in clustering appears in [FCMR08].

Section 7.9 is a simplied version of [KVV04]. Section 7.10 is from [RV99].

239

7.14 Exercises

Exercise 7.1 Construct examples where using distances instead of distance squared gives

bad results for Gaussian densities. For example, pick samples from two 1-dimensional

unit variance Gaussians, with their centers 10 units apart. Cluster these samples by trial

and error into two clusters, rst according to k-means and then according to the k-median

criteria. The k-means clustering should essentially yield the centers of the Gaussians as

cluster centers. What cluster centers do you get when you use the k-median criterion?

Exercise 7.2 Let v = (1, 3). What is the L1 norm of v? The L2 norm? The square of

the L1 norm?

Exercise 7.3 Show that in 1-dimension, the center of a cluster that minimizes the sum

of distances of data points to the center is in general not unique. Suppose we now require

the center also to be a data point; then show that it is the median element (not the mean).

Further in 1-dimension, show that if the center minimizes the sum of squared distances

to the data points, then it is unique.

Exercise 7.4 Construct a block diagonal matrix A with three blocks of size 50. Each

matrix element in a block has value p = 0.7 and each matrix element not in a block has

value q = 0.3. Generate a 150  150 matrix B of random numbers in the range [0,1]. If

bij  aij replace aij with the value one. Otherwise replace aij with value zero. The rows

of A have three natural clusters. Generate a random permutation and use it to permute

the rows and columns of the matrix A so that the rows and columns of each cluster are

randomly distributed.

1. Apply the k-means algorithm to A with k = 3. Do you nd the correct clusters?

2. Apply the k-means algorithm to A for 1  k  10. Plot the value of the sum of

squares to the cluster centers versus k. Was three the correct value for k?

Exercise 7.5 Let M be a k  k matrix whose elements are numbers in the range [0,1].

A matrix entry close to one indicates that the row and column of the entry correspond to

closely related items and an entry close to zero indicates unrelated entities. Develop an

algorithm to match each row with a closely related column where a column can be matched

with only one row.

Exercise 7.6 The simple greedy algorithm of Section 7.3 assumes that we know the clus-

tering radius r. Suppose we do not. Describe how we might arrive at the correct r?

Exercise 7.7 For the k-median problem, show that there is at most a factor of two ratio

between the optimal value when we either require all cluster centers to be data points or

allow arbitrary points to be centers.

Exercise 7.8 For the k-means problem, show that there is at most a factor of four ratio

between the optimal value when we either require all cluster centers to be data points or

allow arbitrary points to be centers.

240

Exercise 7.9 Consider clustering points in the plane according to the k-median criterion,

where cluster centers are required to be data points. Enumerate all possible clusterings

and select the one with the minimum cost. The number of possible ways of labeling n

points, each with a label from {1, 2, . . . , k} is kn which is prohibitive. Show that we can

nd the optimal clustering in time at most a constant times (cid:0)n

(cid:1)  nk

which is much smaller than kn when k << n.

(cid:1) + k2. Note that (cid:0)n

k

k

Exercise 7.10 Suppose in the previous exercise, we allow any point in space (not neces-

sarily data points) to be cluster centers. Show that the optimal clustering may be found

in time at most a constant times n2k2.

Exercise 7.11 Corollary 7.2 shows that for a set of points {a1, a2, . . . , an}, there is a

|ai  x|2. Show examples

unique point x, namely their centroid, which minimizes

n

(cid:80)

i=1

where the x minimizing

|ai  x| is not unique. (Consider just points on the real line.)

n

(cid:80)

i=1

Show examples where the x dened as above are far apart from each other.

n

(cid:80)

i=1

ai

Exercise 7.12 Let {a1, a2, . . . , an} be a set of unit vectors in a cluster. Let c = 1

n

be the cluster centroid. The centroid c is not in general a unit vector. Dene the similarity

between two points ai and aj as their dot product. Show that the average cluster similarity

1

T is the same whether it is computed by averaging all pairs or computing the

n2

aiaj

(cid:80)

i,j

average similarity of each point with the centroid of the cluster.

Exercise 7.13 For some synthetic data estimate the number of local minima for k-means

by using the birthday estimate. Is your estimate an unbaised estimate of the number? an

upper bound? a lower bound? Why?

Exercise 7.14 Examine the example in Figure 7.9 and discuss how to x it. Optimizing

according to the k-center or k-median criteria would seem to produce clustering B while

clustering A seems more desirable.

Exercise 7.15 Prove that for any two vectors a and b, |a  b|2  1

2|a|2  |b|2.

Exercise 7.16 Let A be an nd data matrix, B its best rank k approximation, and C the

optimal centers for k-means clustering of rows of A. How is it possible that (cid:107)A  B(cid:107)2

F <

(cid:107)A  C(cid:107)2

F ?

Exercise 7.17 Suppose S is a nite set of points in space with centroid (S). If a set T

of points is added to S, show that the centroid (S  T ) of S  T is at distance at most

|S|+|T | |(T )  (S)| from (S).

|T |

241

A

B

Figure 7.9: insert caption

Exercise 7.18 What happens if we relax this restriction, for example, if we allow for S,

the entire set?

Exercise 7.19 Given the graph G = (V, E) of a social network where vertices represent

individuals and edges represent relationships of some kind, one would like to dene the

concept of a community. A number of dierent denitions are possible.

1. A subgraph S = (VS, ES) whose density ES

V 2

S

is greater than that of the graph E

V 2 .

2. A subgraph S with a low conductance like property such as the number of graph edges

leaving the subgraph normalized by the minimum size of S or V  S where size is

measured by the sum of degrees of vertices in S or in V  S.

3. A subgraph that has more internal edges than in a random graph with the same

degree distribution.

Which would you use and why?

Exercise 7.20 A stochastic matrix is a matrix with non negative entries in which each

row sums to one. Show that for a stochastic matrix, the largest eigenvalue is one. Show

that the eigenvalue has multiplicity one if and only if the corresponding Markov Chain is

connected.

Exercise 7.21 Show that if P is a stochastic matrix and  satises ipij = jpji, then

for any left eigenvector v of P , the vector u with components ui = vi

is a right eigenvector

i

with the same eigenvalue.

Exercise 7.22 Give an example of a clustering problem where the clusters are not linearly

separable in the original space, but are separable in a higher dimensional space.

Hint: Look at the example for Gaussian kernels in the chapter on learning.

Exercise 7.23 The Gaussian kernel maps points to a higher dimensional space. What is

this mapping?

242

Exercise 7.24 Agglomerative clustering requires that one calculate the distances between

all pairs of points. If the number of points is a million or more, then this is impractical.

One might try speeding up the agglomerative clustering algorithm by maintaining a 100

clusters at each unit of time. Start by randomly selecting a hundred points and place each

point in a cluster by itself. Each time a pair of clusters is merged randomly select one of

the remaining data points and create a new cluster containing that point. Suggest some

other alternatives.

Exercise 7.25 Let A be the adjacency matrix of an undirected graph. Let d(S, S) = A(S,S)

be the density of the subgraph induced by the set of vertices S. Prove that d (S, S) is the

average degree of a vertex in S. Recall that A(S, T ) = (cid:80)

|S|

aij

Exercise 7.26 Suppose A is a matrix with non negative entries. Show that A(S, T )/(|S||T |)

is maximized by the single edge with highest aij. Recall that A(S, T ) = (cid:80)

aij

iS,jT

iS,jT

Exercise 7.27 Suppose A is a matrix with non negative entries and

1(A) = xT Ay =

(cid:88)

i,j

xiaijyj,

|x| = |y| = 1.

Zero out all xi less than 1/2

th of 1(A).

than 1/4



n and all yj less than 1/2

d. Show that the loss is no more



Exercise 7.28 Consider other measures of density such as A(S,T )

. Discuss the signicance of the densest subgraph according to these measures.

|S||T | for dierent values of

Exercise 7.29 Let A be the adjacency matrix of an undirected graph. Let M be the

2m . Partition the vertices into two groups S and S.

matrix whose ijth element is aij  didj

Let s be the indicator vector for the set S and let s be the indicator variable for S. Then

sT M s is the number of edges in S above the expected number given the degree distribution

and sT M s is the number of edges from S to S above the expected number given the degree

distribution. Prove that if sT M s is positive sT M s must be negative.

Exercise 7.30 Which of the three axioms, scale invariance, richness, and consistency

are satised by the following clustering algorithms.

1. k-means

2. Spectral Clustering.

Exercise 7.31 (Research Problem): What are good measures of density that are also

eectively computable? Is there empirical/theoretical evidence that some are better than

others?

243

Exercise 7.32 Create a graph with a small community and start a random walk in the

community. Calculate the frequency distribution over the vertices of the graph and normal-

ize the frequency distribution by the stationary probability. Plot the ratio of the normalized

frequency for the vertices of the graph. What is the shape of the plot for vertices in the

small community?

Exercise 7.33

1. Create a random graph with the following two structures imbedded in it. The rst

structure has three equal size communities with no edges between communities and

the second structure has ve equal size communities with no edges between commu-

nities.

2. Apply a clustering algorithm to nd the dominate structure. Which structure did

you get?

3. Weaken the dominant structure by removing a fraction of its edges and see if you

can nd the hidden structure.

Exercise 7.34 Experiment with nding hidden communities.

Exercise 7.35 Generate a bloc model with two equal size communities where p and q

are the probabilities for the edge density within communities and the edge density between

communities. Then generated a GNP model with probability (p + q)/2. Which of two

models most likely generates a community with half of the vertices?

244

8 Random Graphs

Large graphs appear in many contexts such as the World Wide Web, the internet,

social networks, journal citations, and other places. What is dierent about the modern

study of large graphs from traditional graph theory and graph algorithms is that here

one seeks statistical properties of these very large graphs rather than an exact answer

to questions on specic graphs. This is akin to the switch physics made in the late 19th

century in going from mechanics to statistical mechanics. Just as the physicists did, one

formulates abstract models of graphs that are not completely realistic in every situation,

but admit a nice mathematical development that can guide what happens in practical

situations. Perhaps the most basic model is the G (n, p) model of a random graph. In

this chapter, we study properties of the G(n, p) model as well as other models.

8.1 The G(n, p) Model

The G (n, p) model, due to Erdos and Renyi, has two parameters, n and p. Here n is

the number of vertices of the graph and p is the edge probability. For each pair of distinct

vertices, v and w, p is the probability that the edge (v,w) is present. The presence of each

edge is statistically independent of all other edges. The graph-valued random variable

with these parameters is denoted by G (n, p). When we refer to the graph G (n, p), we

mean one realization of the random variable. In many cases, p will be a function of n

such as p = d/n for some constant d. For example, if p = d/n then the expected degree

of a vertex of the graph is (n  1) d

n  d. In order to simplify calculations in this chapter,

we will often use the approximation that n1

n  1. In fact, conceptually it is helpful to

think of n as both the total number of vertices and as the number of potential neighbors

of any given node, even though the latter is really n  1; for all our calculations, when n

is large, the correction is just a low-order term.

The interesting thing about the G(n, p) model is that even though edges are chosen

independently with no collusion, certain global properties of the graph emerge from the

independent choices. For small p, with p = d/n, d < 1, each connected component in the

graph is small. For d > 1, there is a giant component consisting of a constant fraction of

the vertices. In addition, there is a rapid transition at the threshold d = 1. Below the

threshold, the probability of a giant component is very small, and above the threshold,

the probability is almost one.

The phase transition at the threshold d = 1 from very small o(n) size components to a

giant (n) sized component is illustrated by the following example. Suppose the vertices

represent people and an edge means the two people it connects know each other. Given a

chain of connections, such as A knows B, B knows C, C knows D, ..., and Y knows Z, we

say that A indirectly knows Z. Thus, all people belonging to a connected component of

the graph indirectly know each other. Suppose each pair of people, independent of other

pairs, tosses a coin that comes up heads with probability p = d/n. If it is heads, they

245

1  o(1)

Probability

of a giant

component

o(1)

1  

Expected number of friends per person

1 + 

Figure 8.1: Probability of a giant component as a function of the expected number of

people each person knows directly.

know each other; if it comes up tails, they dont. The value of d can be interpreted as the

expected number of people a single person directly knows. The question arises as to how

large are sets of people who indirectly know each other?

If the expected number of people each person knows is more than one, then a giant

component of people, all of whom indirectly know each other, will be present consisting

of a constant fraction of all the people. On the other hand, if in expectation, each person

knows less than one person, the largest set of people who know each other indirectly is a

vanishingly small fraction of the whole. Furthermore, the transition from the vanishing

fraction to a constant fraction of the whole, happens abruptly between d slightly less than

one to d slightly more than one. See Figure 8.1. Note that there is no global coordination

of who knows whom. Each pair of individuals decides independently. Indeed, many large

real-world graphs, with constant average degree, have a giant component. This is perhaps

the most important global property of the G(n, p) model.

8.1.1 Degree Distribution

One of the simplest quantities to observe in a real graph is the number of vertices

It is also very simple to study

of given degree, called the vertex degree distribution.

these distributions in G (n, p) since the degree of each vertex is the sum of n independent

random variables, which results in a binomial distribution.

Example: In G(n, 1

2), each vertex is of degree close to n/2. In fact, for any  > 0, the

degree of each vertex almost surely is within 1   times n/2. To see this, note that the

degree of a vertex is the sum of n  1  n indicator variables that take on value one or

246

35

34

33

32

31

40

39

38

37

36

5

4

3

2

1

10

9

8

7

6

15

14

13

12

11

20

19

18

17

16

25

24

23

22

21

A graph with 40 vertices and 24 edges

17

22

1

30

34

3

2

18

23

31

35

19

7

9

6

8

4

5

10

11

12

13

14

15

16

36

37

38

20

24

26

28

32

39

30

29

28

27

26

21

25

27

29

33

40

A randomly generated G(n, p) graph with 40 vertices and 24 edges

Figure 8.2: Two graphs, each with 40 vertices and 24 edges. The second graph was

randomly generated using the G(n, p) model with p = 1.2/n. A graph similar to the top

graph is almost surely not going to be randomly generated in the G(n, p) model, whereas

a graph similar to the lower graph will almost surely occur. Note that the lower graph

consists of a giant component along with a number of small components that are trees.

247

Binomial distribution

Power law distribution

Figure 8.3: Illustration of the binomial and the power law distributions.

zero depending on whether the edge is present or not, each of mean 1

4. The

expected value of the sum is the sum of the expected values and the variance of the sum

2 and variance  n

is the sum of the variances, and hence the degree has mean  n

4 . Thus,

n of the mean for some constant

the probability mass is within an additive term of c

c and thus within a multiplicative factor of 1  (cid:15) of n

2 and variance 1

2 for suciently large n.



The degree distribution of G (n, p) for general p is also binomial. Since p is the prob-

ability of an edge being present, the expected degree of a vertex is p(n  1)  pn. The

degree distribution is given by

Prob(vertex has degree k) = (cid:0)n1

(cid:1)pk(1  p)nk1  (cid:0)n

The quantity (cid:0)n

(cid:1) is the number of ways of choosing k edges, out of the possible n edges,

and pk(1  p)nk is the probability that the k selected edges are present and the remaining

n  k are not.

(cid:1)pk(1  p)nk.

k

k

k

The binomial distribution falls o exponentially fast as one moves away from the mean.

However, the degree distributions of graphs that appear in many applications do not ex-

hibit such sharp drops. Rather, the degree distributions are much broader. This is often

referred to as having a heavy tail. The term tail refers to values of a random variable

far away from its mean, usually measured in number of standard deviations. Thus, al-

though the G (n, p) model is important mathematically, more complex models are needed

to represent real world graphs.

Consider an airline route graph. The graph has a wide range of degrees from degree

one or two for a small city to degree 100 or more, for a major hub. The degree distribution

is not binomial. Many large graphs that arise in various applications appear to have power

law degree distributions. A power law degree distribution is one in which the number of

vertices having a given degree decreases as a power of the degree, as in

248

Number(degree k vertices) = c n

kr ,

for some small positive real r, often just slightly less than three. Later, we will consider

a random graph model giving rise to such degree distributions.

The following theorem states that the degree distribution of the random graph G (n, p)

is tightly concentrated about its expected value. That is, the probability that the degree

np, drops o exponentially

of a vertex diers from its expected degree by more than 

fast with .



Theorem 8.1 Let v be a vertex of the random graph G(n, p). Let  be a real number in

(0,

np).



Prob(|np  deg(v)|  



np)  3e2/8.

Proof: The degree deg(v) is the sum of n  1 independent Bernoulli random variables,

x1, x2, . . . , xn1, where, xi is the indicator variable that the ith edge from v is present. So,

approximating n  1 with n, the theorem follows from Theorem 12.6 in the appendix.

Although the probability that the degree of a single vertex diers signicantly from

its expected value drops exponentially, the statement that the degree of every vertex is

close to its expected value requires that p is ( log n

n ). That is, the expected degree grows

at least logarithmically with the number of vertices.

Corollary 8.2 Suppose  is a positive constant. If p  9 ln n

vertex has degree in the range (1  )np to (1 + )np.

n2 , then almost surely every

np to get that the probability that an individual

Proof: Apply Theorem 8.1 with  = 

vertex has degree outside the range [(1  )np, (1 + )np] is at most 3e2np/8. By the

union bound, the probability that some vertex has degree outside this range is at most

3ne2np/8. For this to be o(1), it suces for p  9 ln n

n2 .



Note that the assumption p is ( log n

If p = d/n for d a constant,

then some vertices may well have degrees outside the range [(1  )d, (1 + )d]. Indeed,

shortly we will see that it is highly likely that for p = 1

n there is a vertex of degree

(log n/ log log n). Moreover, for p = 1

n it is easy to see that with high probability there

will be at least one vertex of degree zero.

n ) is necessary.

2

When p is a constant, the expected degree of vertices in G (n, p) increases with n. In

(cid:1) the expected degree of a vertex is approximately n/2. In many real applications,

G (cid:0)n, 1

we will be concerned with G (n, p) where p = d/n, for d a constant, i.e., graphs whose

expected degree is a constant d independent of n. As n goes to innity, the binomial

distribution with p = d

n

Prob(k) =

(cid:18)n

k

(cid:19) (cid:18) d

n

(cid:19)k (cid:18)

1 

d

n

(cid:19)nk

249

approaches the Poisson distribution

Prob(k) =

dk

k!

ed.

To see this, assume k = o(n) and use the approximations (cid:0)n

(cid:0)1  d

(cid:1)n  ed. Then

(cid:1)nk  (cid:0)1  d

k

n

n

(cid:1)  nk

k! , n  k  n, and

(cid:18)n

k

(cid:19) (cid:18) d

n

lim

n

(cid:19)k (cid:18)

1 

d

n

(cid:19)nk

=

nk

k!

dk

nk ed =

dk

k!

ed.

Note that for p = d

n , where d is a constant independent of n, the probability of the

binomial distribution falls o rapidly for k > d, and is essentially zero once k! dominates

dk. This justies the k = o(n) assumption. Thus, the Poisson distribution is a good

approximation.

Example: In G(n, 1

n ) many vertices are of degree one, but not all. Some are of degree

zero and some are of degree greater than one. In fact, it is highly likely that there is a

vertex of degree (log n/ log log n). The probability that a given vertex is of degree k is

Prob (k) =

(cid:18)n  1

k

(cid:19) (cid:18) 1

n

(cid:19)k(cid:18)

1 

1

n

(cid:19)n1k

(cid:19)k(cid:18)



(cid:18)n

k

(cid:19) (cid:18) 1

n

1 

1

n

(cid:19)nk



e1

k!

.

If k = log n/ log log n,

log kk = k log k =

log n

log log n

(log log n  log log log n)  log n

k! e1  1

and thus kk  n. Since k!  kk  n, the probability that a vertex has degree k =

log n/ log log n is at least 1

en. If the degrees of vertices were independent random

variables, then this would be enough to argue that there would be a vertex of degree

1

log n/ log log n with probability at least 1  (cid:0)1  1

e = 0.31. But the degrees

are not quite independent since when an edge is added to the graph it aects the degree

of two vertices. This is a minor technical point, which one can get around.

(cid:1)n = 1  e

en

8.1.2 Existence of Triangles in G(n, d/n)

What is the expected number of triangles in G (cid:0)n, d

(cid:1), when d is a constant? As the

number of vertices increases one might expect the number of triangles to increase, but this

is not the case. Although the number of triples of vertices grows as n3, the probability

of an edge between two specic vertices decreases linearly with n. Thus, the probability

of all three edges between the pairs of vertices in a triple of vertices being present goes

down as n3, exactly canceling the rate of growth of triples.

n

250

A random graph with n vertices and edge probability d/n, has an expected number

(cid:1) triples of vertices.

of triangles that is independent of n, namely d3/6. There are (cid:0)n

Each triple has probability (cid:0) d

(cid:1)3 of being a triangle. Let ijk be the indicator variable

for the triangle with vertices i, j, and k being present. That is, all three edges (i, j),

(j, k), and (i, k) being present. Then the number of triangles is x = (cid:80)

ijk ijk. Even

though the existence of the triangles are not statistically independent events, by linearity

of expectation, which does not assume independence of the variables, the expected value

of a sum of random variables is the sum of the expected values. Thus, the expected

number of triangles is

n

3

E(x) = E

(cid:16) (cid:88)

(cid:17)

=

(cid:88)

ijk

E(ijk) =

ijk

ijk

(cid:19)3

(cid:18)n

3

(cid:19) (cid:18) d

n



d3

6

.

Even though on average there are d3

6 triangles per graph, this does not mean that with

high probability a graph has a triangle. Maybe half of the graphs have d3

3 triangles and

the other half have none for an average of d3

6 triangles. Then, with probability 1/2, a

graph selected at random would have no triangle. If 1/n of the graphs had d3

6 n triangles

and the remaining graphs had no triangles, then as n goes to innity, the probability that

a graph selected at random would have a triangle would go to zero.

We wish to assert that with some nonzero probability there is at least one triangle

in G(n, p) when p = d

n . If all the triangles were on a small number of graphs, then the

number of triangles in those graphs would far exceed the expected value and hence the

variance would be high. A second moment argument rules out this scenario where a small

fraction of graphs have a large number of triangles and the remaining graphs have none.

Lets calculate E(x2) where x is the number of triangles. Write x as x = (cid:80)

ijk ijk,

where ijk is the indicator variable of the triangle with vertices i, j, and k being present.

Expanding the squared term

E(x2) = E

(cid:16) (cid:88)

(cid:17)2

= E

(cid:16) (cid:88)

ijk

ijki(cid:48)j(cid:48)k(cid:48)

(cid:17)

.

i,j,k

i, j, k

i(cid:48),j(cid:48),k(cid:48)

Split the above sum into three parts. In Part 1, let S1 be the set of i, j, k and i(cid:48), j(cid:48), k(cid:48)

which share at most one vertex and hence the two triangles share no edge. In this case,

ijk and i(cid:48)j(cid:48)k(cid:48) are independent and

(cid:16) (cid:88)

E

S1

ijki(cid:48)j(cid:48)k(cid:48)

(cid:17)

=

(cid:88)

S1

E(ijk)E(i(cid:48)j(cid:48)k(cid:48)) 

(cid:16) (cid:88)

E(ijk)

(cid:17) (cid:16) (cid:88)

E(i(cid:48)j(cid:48)k(cid:48))

(cid:17)

= E2(x).

all

ijk

all

i(cid:48)j(cid:48)k(cid:48)

251

or

The two triangles of Part 1 are either

disjoint or share at most one vertex

The two triangles

of Part 2 share an

edge

The two triangles in

Part 3 are the same tri-

angle

Figure 8.4: The triangles in Part 1, Part 2, and Part 3 of the second moment argument

for the existence of triangles in G(n, d

n ).

In Part 2, i, j, k and i(cid:48), j(cid:48), k(cid:48) share two vertices and hence one edge. See Figure 8.4.

Four vertices and ve edges are involved overall. There are at most (cid:0)n

(cid:1)  O(n4), 4-vertex

subsets and (cid:0)4

(cid:1) ways to partition the four vertices into two triangles with a common edge.

The probability of all ve edges in the two triangles being present is p5, so this part sums

to O(n4p5) = O(d5/n) and is o(1). There are so few triangles in the graph, the probability

of two triangles sharing an edge is extremely unlikely.

2

4

In Part 3, i, j, k and i(cid:48), j(cid:48), k(cid:48) are the same sets. The contribution of this part of the

summation to E(x2) is (cid:0)n

(cid:1)p3 = d3

3

6 . Thus, putting all three parts together, we have:

which implies

E(x2)  E2(x) +

d3

6

+ o(1),

Var(x) = E(x2)  E2(x) 

d3

6

+ o(1).

For x to be equal to zero, it must dier from its expected value by at least its expected

value. Thus,

Prob(x = 0)  Prob (cid:0)|x  E(x)|  E(x)(cid:1) .

By Chebychev inequality,

Prob(x = 0) 

Var(x)

E2(x)



d3/6 + o(1)

d6/36



6

d3 + o(1).

(8.1)



Thus, for d > 3



probability. For d < 3

graph for there to be a triangle.

6 = 1.8, Prob(x = 0) < 1 and G(n, p) has a triangle with nonzero

6 < 1 and there simply are not enough edges in the

6, E(x) = d3

8.2 Phase Transitions

Many properties of random graphs undergo structural changes as the edge probability

passes some threshold value. This phenomenon is similar to the abrupt phase transitions in

252

physics, as the temperature or pressure increases. Some examples of this are the abrupt

appearance of cycles in G(n, p) when p reaches 1/n and the disappearance of isolated

vertices when p reaches ln n

n . The most important of these transitions is the emergence of

a giant component, a connected component of size (n), which happens at d = 1. Recall

Figure 8.1.

Probability Transition

p = o( 1

n)

p = d

n , d < 1

p = d

p = d

n , d = 1

n , d > 1

p = 1

ln n

2

n

(cid:113) 2 ln n

n

p =

p = ln n

n

p = 1

2

Forest of trees, no component

of size greater than O(log n)

Cycles appear, no component

of size greater than O(log n)

Components of size O(n 2

3 )

Giant component plus O(log n)

components

Giant component plus isolated

vertices

Diameter two

Disappearance of isolated vertices

Appearance of Hamilton circuit

Diameter O(log n)

Clique of size (2  (cid:15)) ln n

Table 1: Phase transitions

have the property, and when lim

n

For these and many other properties of random graphs, a threshold exists where an

abrupt transition from not having the property to having the property occurs. If there

p1(n)

p(n) = 0, G (n, p1 (n)) almost surely does not

exists a function p (n) such that when lim

n

p2(n)

p(n) = , G (n, p2 (n)) almost surely has the property,

then we say that a phase transition occurs, and p (n) is the threshold. Recall that G(n, p)

almost surely does not have the property means that the probability that it has the

property goes to zero in the limit, as n goes to innity. We shall soon see that every

increasing property has a threshold. This is true not only for increasing properties of

G (n, p), but for increasing properties of any combinatorial structure. If for cp (n), c < 1,

the graph almost surely does not have the property and for cp (n) , c > 1, the graph

almost surely has the property, then p (n) is a sharp threshold. The existence of a giant

component has a sharp threshold at 1/n. We will prove this later.

In establishing phase transitions, we often use a variable x(n) to denote the number

of occurrences of an item in a random graph. If the expected value of x(n) goes to zero as

n goes to innity, then a graph picked at random almost surely has no occurrence of the

253

1

Prob(x > 0)

0

1

n1+(cid:15)

1

n log n

1

n

log n

n

1

2

0.6

n

0.8

n

1

n

1.2

n

1.4

n

1o(1)

n

1

n

1+o(1)

n

(a)

(b)

(c)

Figure 8.5: Figure 8.5(a) shows a phase transition at p = 1

n . The dotted line shows

an abrupt transition in Prob(x) from 0 to 1. For any function asymptotically less than

1

n , Prob(x)>0 is zero and for any function asymptotically greater than 1

n , Prob(x)>0 is

one. Figure 8.5(b) expands the scale and shows a less abrupt change in probability unless

the phase transition is sharp as illustrated by the dotted line. Figure 8.5(c) is a further

expansion and the sharp transition is now more smooth.

item. This follows from Markovs inequality. Since x is a nonnegative random variable

Prob(x  a)  1

a E(x), which implies that the probability of x(n)  1 is at most E(x(n)).

That is, if the expected number of occurrences of an item in a graph goes to zero, the

probability that there are one or more occurrences of the item in a randomly selected

graph goes to zero. This is called the rst moment method.

The previous section showed that the property of having a triangle has a threshold at

p(n) = 1/n. If the edge probability p1(n) is o(1/n), then the expected number of triangles

goes to zero and by the rst moment method, the graph almost surely has no triangle.

However, if the edge probability p2(n) satises p2(n)

1/n  , then from (8.1), the probability

of having no triangle is at most 6/d3 + o(1) = 6/(np2(n))3 + o(1), which goes to zero. This

latter case uses what we call the second moment method. The rst and second moment

methods are broadly used. We describe the second moment method in some generality

now.

When the expected value of x(n), the number of occurrences of an item, goes to

innity, we cannot conclude that a graph picked at random will likely have a copy since

the items may all appear on a vanishingly small fraction of the graphs. We resort to a

technique called the second moment method. It is a simple idea based on Chebyshevs

inequality.

Theorem 8.3 (Second Moment method) Let x(n) be a random variable with E(x) > 0.

If

Var(x) = o(cid:0)E2(x)(cid:1),

then x is almost surely greater than zero.

254

No items

E(x)  0.1

At least one

occurrence

of item in

10% of the

graphs

For 10% of the

graphs, x  1

Figure 8.6: If the expected fraction of the number of graphs in which an item occurs

did not go to zero, then E (x), the expected number of items per graph, could not be

zero. Suppose 10% of the graphs had at least one occurrence of the item. Then the

expected number of occurrences per graph must be at least 0.1. Thus, E (x)  0 implies

the probability that a graph has an occurrence of the item goes to zero. However, the

other direction needs more work. If E (x) is large, a second moment argument is needed

to conclude that the probability that a graph picked at random has an occurrence of the

item is nonnegligible, since there could be a large number of occurrences concentrated on

a vanishingly small fraction of all graphs. The second moment argument claims that for

a nonnegative random variable x with E (x) > 0, if Var(x) is o(E2 (x)) or alternatively if

E (x2)  E2 (x) (1 + o(1)), then almost surely x > 0.

Proof: If E(x) > 0, then for x to be less than or equal to zero, it must dier from its

expected value by at least its expected value. Thus,

Prob(x  0)  Prob

(cid:16)

|x  E(x)|  E(x)

(cid:17)

.

By Chebyshev inequality

(cid:16)

Prob

|x  E(x)|  E(x)

(cid:17)



Var(x)

E2(x)

 0.

Thus, Prob(x  0) goes to zero if Var(x) is o (E2(x)) .

Corollary 8.4 Let x be a random variable with E(x) > 0. If

E(x2)  E2(x)(cid:0)1 + o(1)(cid:1),

then x is almost surely greater than zero.

Proof: If E(x2)  E2(x)(1 + o(1)), then

V ar(x) = E(x2)  E2(x)  E2(x)o(1) = o(cid:0)E2(x)(cid:1).

255

Second moment arguments are more dicult than rst moment arguments since they

deal with variance and without independence we do not have E(xy) = E(x)E(y). In the

triangle example, dependence occurs when two triangles share a common edge. However,

if p = d

n , there are so few triangles that almost surely no two triangles share a common

edge and the lack of statistical independence does not aect the answer. In looking for

a phase transition, almost always the transition in probability of an item being present

occurs when the expected number of items transitions.

Threshold for graph diameter two (two degrees of separation)

We now present the rst example of a sharp phase transition for a property. This

means that slightly increasing the edge probability p near the threshold takes us from

almost surely not having the property to almost surely having it. The property is that

of a random graph having diameter less than or equal to two. The diameter of a graph

is the maximum length of the shortest path between a pair of nodes. In other words, the

property is that every pair of nodes has at most two degrees of separation.

The following technique for deriving the threshold for a graph having diameter two

is a standard method often used to determine the threshold for many other objects. Let

x be a random variable for the number of objects such as triangles, isolated vertices, or

Hamiltonian circuits, for which we wish to determine a threshold. Then we determine

the value of p, say p0, where the expected value of x goes from vanishingly small to un-

boundedly large. For p < p0 almost surely a graph selected at random will not have a

copy of the item. For p > p0, a second moment argument is needed to establish that the

items are not concentrated on a vanishingly small fraction of the graphs and that a graph

picked at random will almost surely have a copy.

Our rst task is to gure out what to count to determine the threshold for a graph

having diameter two. A graph has diameter two if and only if for each pair of vertices i

and j, either there is an edge between them or there is another vertex k to which both i

and j have an edge. So, what we will count is the number of pairs i and j that fail, i.e.,

the number of pairs i and j that have more than two degrees of separation. The set of

neighbors of i and the set of neighbors of j are random subsets of expected cardinality

n . Such statements often

np. For these two sets to intersect requires np 

go under the general name of birthday paradox though it is not a paradox. In what

n) for a graph to have diameter two. The

follows, we will prove a threshold of O(

(cid:1) pairs of i and j has a common

extra factor of

2, the graph almost surely has diameter greater

ln n ensures that every one of the (cid:0)n

n or p  1

(cid:113) ln n

ln n/











2

n , for c <

2, the graph almost surely has diameter less than or equal to two.

neighbor. When p = c



than two and for c >

Theorem 8.5 The property that G (n, p) has diameter two has a sharp threshold at

p =



(cid:113) ln n

n .

2

256

Proof: If G has diameter greater than two, then there exists a pair of nonadjacent ver-

tices i and j such that no other vertex of G is adjacent to both i and j. This motivates

calling such a pair bad .

Introduce a set of indicator variables Iij, one for each pair of vertices (i, j) with i < j,

where Iij is 1 if and only if the pair (i, j) is bad. Let

x =

(cid:88)

i<j

Iij

be the number of bad pairs of vertices. Putting i < j in the sum ensures each pair (i, j)

is counted only once. A graph has diameter at most two if and only if it has no bad pair,

E (x) = 0, then for large n, almost surely, a graph has no bad

i.e., x = 0. Thus, if

pair and hence has diameter at most two.

lim

n

The probability that a given vertex is adjacent to both vertices in a pair of vertices

(i, j) is p2. Hence, the probability that the vertex is not adjacent to both vertices is

1  p2. The probability that no vertex is adjacent to the pair (i, j) is (1  p2)n2 and the

probability that i and j are not adjacent is 1  p. Since there are (cid:0)n

(cid:1) pairs of vertices,

the expected number of bad pairs is

2

E (x) =

(cid:19)

(cid:18)n

2

(1  p) (cid:0)1  p2(cid:1)n2 .

Setting p = c

(cid:113) ln n

n ,

(cid:16)

1  c

(cid:113) ln n

n

(cid:17) (cid:0)1  c2 ln n

n

(cid:1)n

E (x) = n2

2

= n2

= 1

2 ec2 ln n

2n2c2.

E (x) = 0. By the rst moment method, for p = c

(cid:113) ln n

n with c >



2,



For c >

2, lim

n

G (n, p) almost surely has no bad pair and hence has diameter at most two.

Next, consider the case c <

E (x) = . We appeal to a second moment

2 where lim

n

argument to claim that almost surely a graph has a bad pair and thus has diameter greater

than two.



E(x2) = E

(cid:33)2

Iij

= E

(cid:32)

(cid:88)

i<j

(cid:32)

(cid:88)

(cid:88)

Iij

i<j

k<l

(cid:33)

Ikl

= E



IijIkl



 =







(cid:88)

i<j

k<l

(cid:88)

i<j

k<l

E (IijIkl).

The summation can be partitioned into three summations depending on the number of

distinct indices among i, j, k, and l. Call this number a.

257

E (cid:0)x2(cid:1) =

(cid:88)

E (IijIkl) +

(cid:88)

E (IijIik) +

(cid:88)

E (cid:0)I 2

ij

(cid:1).

(8.2)

i < j

k < l

a = 4

{i, j, k}

i < j

i < j

a = 3

a = 2

Consider the case a = 4 where i, j, k, and l are all distinct. If IijIkl = 1, then both

pairs (i, j) and (k, l) are bad and so for each u not in {i, j, k, l}, at least one of the edges

(i, u) or (j, u) is absent and, in addition, at least one of the edges (k, u) or (l, u) is absent.

The probability of this for one u not in {i, j, k, l} is (1  p2)2. As u ranges over all the

n  4 vertices not in {i, j, k, l}, these events are all independent. Thus,

E(IijIkl)  (cid:0)1  p2(cid:1)2(n4) 

(cid:16)

1  c2 ln n

n

(cid:17)2n(cid:0)1 + o(1)(cid:1)  n2c2(cid:0)1 + o(1)(cid:1)

and the rst sum is

E(IijIkl) 

1

4

n42c2(cid:0)1 + o(1)(cid:1),

(cid:88)

i < j

k < l

where, the 1

4 is because only a fourth of the 4-tupples (i, j, k, l) have i < j and k < l.

For the second summation, observe that if IijIik = 1, then for every vertex u not equal

to i, j, or k, either there is no edge between i and u or there is an edge (i, u) and both

edges (j, u) and (k, u) are absent. The probability of this event for one u is

1  p + p(1  p)2 = 1  2p2 + p3  1  2p2.

Thus, the probability for all such u is (1  2p2)n3. Substituting c

(cid:113) ln n

n for p yields

(cid:16)

1  2c2 ln n

n

(cid:17)n3

= e2c2 ln n = n2c2,

which is an upper bound on E(IijIkl) for one i, j, k, and l with a = 3. Summing over all

distinct triples yields n32c2 for the second summation in (8.2).

For the third summation, since the value of Iij is zero or one, E (cid:0)I 2

(cid:88)

ij

E (cid:0)I 2

ij

(cid:1) = E (x) .

(cid:1) = E (Iij). Thus,

ij

4n42c2 + n32c2 + n2c2 and E (x) = 1

2n2c2, from which it follows that

Hence, E (x2)  1



2, E (x2)  E2 (x) (1 + o(1)). By a second moment argument, Corollary 8.4, a

for c <

graph almost surely has at least one bad pair of vertices and thus has diameter greater

than two. Therefore, the property that the diameter of G(n, p) is less than or equal to

two has a sharp threshold at p =



(cid:113) ln n

2

n

258

Disappearance of Isolated Vertices

The disappearance of isolated vertices

n . At

this point the giant component has absorbed all the small components and with the

disappearance of isolated vertices, the graph becomes connected.

in G (n, p) has a sharp threshold at ln n

Theorem 8.6 The disappearance of isolated vertices in G (n, p) has a sharp threshold of

ln n

n .

Proof: Let x be the number of isolated vertices in G (n, p). Then,

E (x) = n (1  p)n1 .

Since we believe the threshold to be ln n

lim

n

E (x) = lim

n

n , consider p = c ln n

(cid:1)n = lim

n . Then,

nec ln n = lim

n

n

n

n (cid:0)1  c ln n

n1c.

If c >1, the expected number of isolated vertices, goes to zero. If c < 1, the expected

number of isolated vertices goes to innity. If the expected number of isolated vertices

goes to zero, it follows that almost all graphs have no isolated vertices. On the other

hand, if the expected number of isolated vertices goes to innity, a second moment ar-

gument is needed to show that almost all graphs have an isolated vertex and that the

isolated vertices are not concentrated on some vanishingly small set of graphs with almost

all graphs not having isolated vertices.

Assume c < 1. Write x = I1 + I2 +    + In where Ii is the indicator variable indicating

E (IiIj). Since Ii

whether vertex i is an isolated vertex. Then E (x2) =

E (I 2

i ) + 2 (cid:80)

n

(cid:80)

i=1

i<j

equals 0 or 1, I 2

sum are equal

i = Ii and the rst sum has value E (x). Since all elements in the second

E (cid:0)x2(cid:1) = E (x) + n (n  1) E (I1I2)

= E (x) + n (n  1) (1  p)2(n1)1 .

The minus one in the exponent 2(n  1)  1 avoids counting the edge from vertex 1 to

vertex 2 twice. Now,

E (x2)

E2 (x)

=

=

n (1  p)n1 + n (n  1) (1  p)2(n1)1

n2 (1  p)2(n1)

(cid:17) 1

(cid:16)

1

n (1  p)n1 +

1 

1

n

.

1  p

For p = c ln n

n with c < 1,

lim

n

E (x2)

E2 (x)

= lim

n

lim

n

(cid:34)

1

n1c +

E (x) =  and

(cid:35)

(cid:16)

1 

(cid:17)

1

n

1

1  c ln n

n

= lim

n

(cid:16)

1 + c

(cid:17)

ln n

n

= o(1) + 1.

259

Figure 8.7: A degree three vertex with three adjacent degree two vertices. Graph cannot

have a Hamilton circuit.

By the second moment argument, Corollary 8.4, the probability that x = 0 goes to zero

implying that almost all graphs have an isolated vertex. Thus, ln n

n is a sharp threshold

for the disappearance of isolated vertices. For p = c ln n

n , when c > 1 there almost surely

are no isolated vertices, and when c < 1 there almost surely are isolated vertices.

Hamilton circuits

So far in establishing phase transitions in the G(n, p) model for an item such as the

disappearance of isolated vertices, we introduced a random variable x that was the number

of occurrences of the item. We then determined the probability p for which the expected

value of x went from zero to innity. For values of p for which E(x)  0, we argued that

with high probability, a graph generated at random had no occurrences of x. For values of

x for which E(x)  , we used the second moment argument to conclude that with high

probability, a graph generated at random had occurrences of x. That is, the occurrences

that forced E(x) to innity were not all concentrated on a vanishingly small fraction of

the graphs. One might raise the question for the G(n, p) graph model, do there exist

items that are so concentrated on a small fraction of the graphs that the value of p where

E(x) goes from zero to innity is not the threshold? An example where this happens is

Hamilton circuits.

A Hamilton circuit is a simple cycle that includes all the vertices. For example, in a

graph of 4 vertices, there are three possible Hamilton circuits: (1, 2, 3, 4), (1, 2, 4, 3), and

(1, 3, 2, 4). Note that our graphs are undirected, so the circuit (1, 2, 3, 4) is the same as

the circuit (1, 4, 3, 2).

Let x be the number of Hamilton circuits in G(n, p) and let p = d

n for some constant

2(n  1)! potential Hamilton circuits in a graph and each has probability

d. There are 1

260

( d

n)n of actually being a Hamilton circuit. Thus,

(n  1)!

1

2

(cid:16)n

e

(cid:26) 0

E(x) =

(cid:39)



(cid:18) d

n

(cid:19)n

(cid:19)n

.

(cid:17)n (cid:18) d

n

d < e

 d > e

This suggests that the threshold for Hamilton circuits occurs when d equals Eulers con-

stant e. This is not possible since the graph still has isolated vertices and is not even

connected for p = e

n . Thus, the second moment argument is indeed necessary.

The actual threshold for Hamilton circuits is 1

n log n. For any p(n) asymptotically

greater, G(n, p) will have a Hamilton circuit with probability one. This is the same

threshold as for the disappearance of degree one vertices. Clearly a graph with a degree

one vertex cannot have a Hamilton circuit. But it may seem surprising that Hamilton

circuits appear as soon as degree one vertices disappear. You may ask why at the moment

degree one vertices disappear there cannot be a subgraph consisting of a degree three

vertex adjacent to three degree two vertices as shown in Figure 8.7. The reason is that the

frequency of degree two and three vertices in the graph is very small and the probability

that four such vertices would occur together in such a subgraph is too small for it to

happen with nonnegligible probability.

8.3 Giant Component

Consider G(n, p) for p = 1+(cid:15)

n where (cid:15) is a constant greater than zero. We now show that

with high probability, such a graph contains a giant component, namely a component of

size (n). Moreover, with high probability, the graph contains only one such component,

and all other components are much smaller, of size only O(log n). We begin by arguing

existence of a giant component.

8.3.1 Existence of a giant component

To see that with high probability the graph has a giant component, do a depth rst search

(dfs) on G(n, p) where p = (1 + (cid:15))/n with 0 < (cid:15) < 1/8. Note that it suces to consider

this range of (cid:15) since increasing the value of p only increases the probability that the graph

has a giant component.

To perform the dfs, generate (cid:0)n

(cid:1) Bernoulli(p) independent random bits and answer

2

261

U

E

unvisited vertices

frontier

F

Small connected

components

already found

Figure 8.8: Picture after (cid:15)n2/2 edge queries. The potential edges from the small con-

nected components to unvisited vertices do not exist in the graph. However, since many

edges must have been found the frontier must be big and hence there is a giant component.

the tth edge query according to the tth bit. As the dfs proceeds, let

E = set of fully explored vertices whose exploration is complete

U = set of unvisited vertices

F = frontier of visited and still being explored vertices .

Initially the set of fully explored vertices, E, and the frontier, F are empty and the

set of unvisited vertices, U equals {1, 2, . . . , n}. If the frontier is not empty and u is the

active vertex of the dfs, the dfs queries each unvisited vertex in U until it nds a vertex

v for which there is an edge (u, v) and moves v from U to the frontier and v becomes the

active vertex. If no edge is found from u to an unvisited vertex in U, then u is moved from

the frontier to the set of fully explored vertices E. If frontier is empty, the dfs moves an

unvisited vertex from U to frontier and starts a new component. If both frontier and U

are empty all connected components of G have been found. At any time all edges between

the current fully explored vertices, E, and the current unvisited vertices, U, have been

queried since a vertex is moved from the frontier to E only when there is no edge from

the vertex to U.

Intuitively, after (cid:15)n2/2 edge queries a large number of edges must have been found since

p = 1+(cid:15)

n . None of these can connect components already found with the set of unvisited

vertices, and we will use this to show that with high probability the frontier must be large.

Since the frontier will be in a connected component, a giant component exists with high

probability. We rst prove that after (cid:15)n2/2 edge queries the set of fully explored vertices

is of size less than n/3.

Lemma 8.7 After (cid:15)n2/2 edge queries, with high probability |E| < n/3.

Proof: If not, at some t  n2/2,

|E| = n/3. A vertex is added to frontier only when

an edge query is answered yes. So at time t, |F | is less than or equal to the sum of n2/2

Bernoulli(p) random variables, which with high probability is at most n2p  n/3. So,

262

at t, |U | = n  |E|  |F |  n/3. Since there are no edges between fully explored vertices

and unvisited vertices, |E| |U |  n2/9 edge queries must have already been answered in

the negative. But t > n2/9 contradicts t  (cid:15)n2/2  n2/16. Thus |E|  n/3.

The frontier vertices in the search of a connected component are all in the component

being searched. Thus if at any time the frontier set has (n) vertices there is a giant

component.

Lemma 8.8 After (cid:15)n2/2 edge queries, with high probability the set F consists of at least

(cid:15)2n/30 vertices.

Proof: After n2/2 queries, say, |F | < 2n/30. Thus

|U | = n  |E|  |F | = n 

n

3



(cid:15)2n

30

 1

and so the dfs is still active. Each positive answer to an edge query so far resulted in some

vertex moving from U to F, which possibly later moved to E. The expected number of

yes answers so far is pn2/2 = (1 + )n/2 and with high probability, the number of yes

answers is at least (n/2) + (2n/3). So,

|E| + |F | 

n

2

+

2n

3

= |E| 

n

2

+

32n

10

.

We must have |E| |U |  n2/2. Now, |E||U | = |E|(n|E||F |) increases as |E| increases

from n

to n/3, so we have

2 + 32n

10

|E||U | 

(cid:18) n

2

+

32n

10

(cid:19) (cid:18)

n 

n

2



32n

10



(cid:19)

2n

30

>

n2

2

,

a contradiction.

8.3.2 No other large components

We now argue that for p = (1 + (cid:15))/n for constant (cid:15) > 0, with high probability there is

only one giant component, and in fact all other components have size O(log n).

We begin with a preliminary observation. Suppose that a G(n, p) graph had at least

a  probability of having two (or more) components of size (log n), i.e., asymptotically

greater than log n. Then, there would be at least a /2 probability of the graph having

two (or more) components with (log n) vertices inside the subset A = {1, 2, . . . , (cid:15)n/2}.

The reason is that an equivalent way to construct a graph G(n, p) is to rst create it in the

usual way and then to randomly permute the vertices. Any component of size (log n)

will with high probability after permutation have at least an (cid:15)/4 fraction of its vertices

within the rst (cid:15)n/2. Thus, it suces to prove that with high probability at most one

component has (log n) vertices within the set A to conclude that with high probability

263

the graph has only one component with (log n) vertices overall.

We now prove that with high probability, a G(n, p) graph for p = (1 + (cid:15))/n has at

most one component with (log n) vertices inside the set A. To do so, let B be the set

of (1  (cid:15)/2)n vertices not in A. Now, construct the graph as follows. First, randomly

ip coins of bias p to generate the edges within set A and the edges within set B. At

this point, with high probability, B has at least one giant component, by the argument

from Section 8.3.1, since p = (1 + (cid:15))/n  (1 + (cid:15)/4)/|B| for 0 < (cid:15)  1/2. Let C  be

a giant component inside B. Now, ip coins of bias p to generate the edges between A

and B except for those incident to C . At this point, let us name all components with

(log n) vertices inside A as C1, C2, C3, . . .. Finally, ip coins of bias p to generate the

edges between A and C .

In the nal step above, notice that with high probability, each Ci is connected to C .

In particular, there are (n log n) possible edges between any given Ci and C , each one

of which is present with probability p. Thus the probability that this particular Ci is not

connected to C  is at most (1  p)(n log n) = 1/n(1). Thus, by the union bound, with

high probability all such Ci are connected to C , and there is only one component with

(log n) vertices within A as desired.

8.3.3 The case of p < 1/n

When p < 1/n, then with high probability all components in G(n, p) are of size O(log n).

This is easiest to see by considering a variation on the above dfs that (a) begins with

F containing a specic start vertex ustart, and then (b) when a vertex u is taken from

F to explore, it pops u o of F , explores u fully by querying to nd all edges between

u and U , and then pushes the endpoints v of those edges onto F . Thus, this is like an

explicit-stack version of dfs, compared to the previous recursive-call version of dfs. Let us

call the exploration of such a vertex u a step. To make this process easier to analyze, let

us say that if F ever becomes empty, we create a brand-new, fake red vertex, connect it

to each vertex in U with probability p, place the new red vertex into F , and then continue

the dfs from there.

Let zk denote the number of real (non-red) vertices discovered after k steps, not in-

cluding ustart. For any given real vertex u (cid:54)= ustart, the probability that u is not discovered

in k steps is (1  p)k, and notice that these events are independent over the dierent ver-

tices u (cid:54)= ustart. Therefore, the distribution of zk is Binomial(cid:0)n  1, 1  (1  p)k(cid:1). Note

that if zk < k then the process must have required creating a fake red vertex by step k,

meaning that ustart is in a component of size at most k. Thus, it suces to prove that

Prob(zk  k) < 1/n2, for k = c ln n for a suitably large constant c, to then conclude by

union bound over choices of ustart that with high probability all vertices are in components

of size at most c ln n.

264

To prove that Prob(zk  k) < 1/n2 for k = c ln n, we use the fact that (1p)k  1pk

so 1  (1  p)k  pk. So, the probability that zk is greater than or equal to k is at most

the probability that a coin of bias pk ipped n  1 times will have at least k heads. But

since pk(n  1)  (1  (cid:15))k for some constant (cid:15) > 0, by Cherno bounds this probability

is at most ec0k for some constant c0 > 0. When k = c ln n for a suitably large constant

c, this probability is at most 1/n2, as desired.

8.4 Cycles and Full Connectivity

This section considers when cycles form and when the graph becomes fully connected.

For both of these problems, we look at each subset of k vertices and see when they form

either a cycle or when they form a connected component.

8.4.1 Emergence of Cycles

The emergence of cycles in G (n, p) has a threshold when p equals to 1/n. However,

the threshold is not sharp.

Theorem 8.9 The threshold for the existence of cycles in G (n, p) is p = 1/n.

Proof: Let x be the number of cycles in G (n, p). To form a cycle of length k, the vertices

can be selected in (cid:0)n

(cid:1) ways. Given the k vertices of the cycle, they can be ordered by

arbitrarily selecting a rst vertex, then a second vertex in one of k-1 ways, a third in one

of k  2 ways, etc. Since a cycle and its reversal are the same cycle, divide by 2. Thus,

there are (cid:0)n

possible cycles of length k and

k

(cid:1) (k1)!

2

k

E (x) =

n

(cid:88)

k=3

(cid:19)

(cid:18)n

k

(k1)!

2 pk 

n

(cid:80)

k=3

nk

2k pk 

n

(cid:80)

k=3

(np)k = (np)3 1(np)n2

1np  2(np)3,

provided that np < 1/2. When p is asymptotically less than 1/n, then lim

n

np = 0 and

n

(cid:80)

k=3

(np)k = 0. So, as n goes to innity, E(x) goes to zero. Thus, the graph almost

lim

n

surely has no cycles by the rst moment method. A second moment argument can be

used to show that for p = d/n, d > 1, a graph will have a cycle with probability tending

to one.

The argument above does not yield a sharp threshold since we argued that E(x)  0

n . A sharp threshold requires

only under the assumption that p is asymptotically less than 1

E(x)  0 for p = d/n, d < 1.

265

Property

Threshold

cycles

giant component

giant component

+ isolated vertices

connectivity, disappearance

of isolated vertices

diameter two

1/n

1/n

1

2

ln n

n

ln n

n

(cid:113) 2 ln n

n

Table 2: Thresholds for various properties

Consider what happens in more detail when p = d/n, d a constant.

n

(cid:88)

E (x) =

(cid:18)n

k

(cid:19) (k  1)!

2

pk

n

(cid:88)

k=3

1

2

k=3

n

(cid:88)

1

2

k=3

=

=

n(n  1)    (n  k + 1)

k!

(k  1)! pk

n(n  1)    (n  k + 1)

nk

dk

k

.

E (x) converges if d < 1, and diverges if d  1. If d < 1, E (x)  1

2

equals a constant greater than zero. If d = 1, E (x) = 1

2

only the rst log n terms of the sum. Since

n(n1)(nk+1)

nk

 1/2. Thus,

n

ni = 1 + i

n

(cid:80)

k=3

dk

k and lim

n

E (x)

n

(cid:80)

k=3

n(n1)(nk+1)

nk

1

k . Consider

ni  ei/ni, it follows that

log n

(cid:80)

k=3

Then, in the limit as n goes to innity

E (x)  1

2

n(n1)(nk+1)

nk

1

k  1

4

log n

(cid:80)

k=3

1

k .

lim

n

E (x)  lim

n

1

4

log n

(cid:80)

k=3

1

k  lim

n

(log log n) = .

For p = d/n, d < 1, E (x) converges to a nonzero constant. For d > 1, E(x) converges

to innity and a second moment argument shows that graphs will have an unbounded

number of cycles increasing with n.

8.4.2 Full Connectivity

As p increases from p = 0, small components form. At p = 1/n a giant component

emerges and swallows up smaller components, starting with the larger components and

266

ending up swallowing isolated vertices forming a single connected component at p = ln n

n ,

at which point the graph becomes connected. We begin our development with a technical

lemma.

Lemma 8.10 The expected number of connected components of size k in G(n, p) is at

most

(cid:19)

(cid:18)n

k

kk2pk1(1  p)knk2.

Proof: The probability that k vertices form a connected component consists of the prod-

uct of two probabilities. The rst is the probability that the k vertices are connected,

and the second is the probability that there are no edges out of the component to the

remainder of the graph. The rst probability is at most the sum over all spanning trees

of the k vertices, that the edges of the spanning tree are present. The at most in the

lemma statement is because G (n, p) may contain more than one spanning tree on these

nodes and, in this case, the union bound is higher than the actual probability. There are

kk2 spanning trees on k nodes. See Section 12.10.5 in the appendix. The probability of

all the k  1 edges of one spanning tree being present is pk1 and the probability that

there are no edges connecting the k vertices to the remainder of the graph is (1  p)k(nk).

Thus, the probability of one particular set of k vertices forming a connected component

is at most kk2pk1 (1  p)knk2

. Thus, the expected number of connected components of

size k is at most (cid:0)n

(cid:1)kk2pk1(1  p)knk2.

k

We now prove that for p = 1

2

nents except for isolated vertices.

ln n

n , the giant component has absorbed all small compo-

Theorem 8.11 For p = c ln n

and a giant component. For c > 1, almost surely the graph is connected.

n with c > 1/2, almost surely there are only isolated vertices

Proof: We prove that almost surely for c > 1/2, there is no connected component with

k vertices for any k, 2  k  n/2. This proves the rst statement of the theorem since, if

there were two or more components that are not isolated vertices, both of them could not

be of size greater than n/2. The second statement that for c > 1 the graph is connected

then follows from Theorem 8.6 which states that isolated vertices disappear at c = 1.

We now show that for p = c ln n

n , the expected number of components of size k,

2  k  n/2, is less than n12c and thus for c > 1/2 there are no components, except

for isolated vertices and the giant component. Let xk be the number of connected com-

ponents of size k. Substitute p = c ln n

and simplify using

(cid:0)n

k

(cid:1)  (en/k)k, 1  p  ep, k  1 < k, and x = eln x to get

(cid:1)kk2pk1 (1  p)knk2

n into (cid:0)n

k

(cid:18)

E(xk)  exp

ln n + k + k ln ln n  2 ln k + k ln c  ck ln n + ck2 ln n

n

(cid:19)

.

267

Keep in mind that the leading terms here for large k are the last two and, in fact, at k = n,

they cancel each other so that our argument does not prove the fallacious statement for

c  1 that there is no connected component of size n, since there is. Let

f (k) = ln n + k + k ln ln n  2 ln k + k ln c  ck ln n + ck2 ln n

n

.

Dierentiating with respect to k,

f (cid:48)(k) = 1 + ln ln n 

2

k

+ ln c  c ln n +

2ck ln n

n

and

f (cid:48)(cid:48) (k) =

2c ln n

n

2

k2 +

Thus, the function f (k) attains its maximum over the range [2, n/2] at one of the extreme

points 2 or n/2. At k = 2, f (2)  (1  2c) ln n and at k = n/2, f (n/2)  c n

4 ln n. So

f (k) is maximum at k = 2. For k = 2, E(xk) = ef (k) is approximately e(12c) ln n = n12c

and is geometrically falling as k increases from 2. At some point E(xk) starts to increase

but never gets above n c

4 n. Thus, the expected sum of the number of components of size

k, for 2  k  n/2 is

> 0.



E



n/2

(cid:88)

k=2

xk



 = O(n12c).

This expected number goes to zero for c > 1/2 and the rst-moment method implies that,

almost surely, there are no components of size between 2 and n/2. This completes the

proof of Theorem 8.11.

8.4.3 Threshold for O(ln n) Diameter

We now show that within a constant factor of the threshold for graph connectivity, not

n for suciently

only is the graph connected, but its diameter is O(ln n). That is, if p > c ln n

large constant c, the diameter of G(n, p) is O(ln n) with high probability.

Consider a particular vertex v. Let Si be the set of vertices at distance i from v. We

argue that as i increases, with high probability |S1| + |S2| +    + |Si| grows by at least a

factor of two, up to a size of n/1000. This implies that in O(ln n) steps, at least n/1000

vertices are connected to v. Then, there is a simple argument at the end of the proof of

Theorem 8.13 that a pair of n/1000 sized subsets, connected to two dierent vertices v

and w, have an edge between them with high probability.

Lemma 8.12 Consider G(n, p) for suciently large n with p = c ln n

n for any c > 0. Let

Si be the set of vertices at distance i from some xed vertex v. If |S1| + |S2| +    + |Si| 

n/1000, then

Prob (cid:0)|Si+1| < 2(|S1| + |S2| +    + |Si|)(cid:1)  e10|Si|.

268

Proof: Let |Si| = k. For each vertex u not in S1  S2  . . .  Si, the probability that

u is not in Si+1 is (1  p)k and these events are independent. So, |Si+1| is the sum of

n  (|S1| + |S2| +    + |Si|) independent Bernoulli random variables, each with probability

of

1  (1  p)k  1  eck ln n/n

of being one. Note that n  (|S1| + |S2| +    + |Si|)  999n/1000. So,

E(|Si+1|) 

999n

1000

(1  eck ln n

n ).

Subtracting 200k from each side

E(|Si+1|)  200k 

(cid:18)

n

2

1  eck ln n

n  400

(cid:19)

.

k

n

Let  = k

n and f () = 1  ec ln n  400. By dierentiation f (cid:48)(cid:48)()  0, so f is concave

and the minimum value of f over the interval [0, 1/1000] is attained at one of the end

points. It is easy to check that both f (0) and f (1/1000) are greater than or equal to

zero for suciently large n. Thus, f is nonnegative throughout the interval proving that

E(|Si+1|)  200|Si|. The lemma follows from Cherno bounds.

Theorem 8.13 For p  c ln n/n, where c is a suciently large constant, almost surely,

G(n, p) has diameter O(ln n).

Proof: By Corollary 8.2, almost surely, the degree of every vertex is (np) = (ln n),

which is at least 20 ln n for c suciently large. Assume that this holds. So, for a xed

vertex v, S1 as dened in Lemma 8.12 satises |S1|  20 ln n.

Let i0 be the least i such that |S1|+|S2|+  +|Si| > n/1000. From Lemma 8.12 and the

union bound, the probability that for some i, 1  i  i01, |Si+1| < 2(|S1|+|S2|+  +|Si|)

is at most (cid:80)n/1000

k=20 ln n e10k  1/n4. So, with probability at least 1  (1/n4), each Si+1 is

at least double the sum of the previous Sj s, which implies that in O(ln n) steps, i0 + 1

is reached.

Consider any other vertex w. We wish to nd a short O(ln n) length path between

v and w. By the same argument as above, the number of vertices at distance O(ln n)

from w is at least n/1000. To complete the argument, either these two sets intersect in

which case we have found a path from v to w of length O(ln n) or they do not intersect.

In the latter case, with high probability there is some edge between them. For a pair of

disjoint sets of size at least n/1000, the probability that none of the possible n2/106 or

more edges between them is present is at most (1p)n2/106 = e(n ln n). There are at most

22n pairs of such sets and so the probability that there is some such pair with no edges

is e(n ln n)+O(n)  0. Note that there is no conditioning problem since we are arguing

this for every pair of such sets. Think of whether such an argument made for just the n

subsets of vertices, which are vertices at distance at most O(ln n) from a specic vertex,

would work.

269

8.5 Phase Transitions for Increasing Properties

For many graph properties such as connectivity, having no isolated vertices, having a

cycle, etc., the probability of a graph having the property increases as edges are added to

the graph. Such a property is called an increasing property. Q is an increasing property

of graphs if when a graph G has the property, any graph obtained by adding edges to G

must also have the property. In this section we show that any increasing property has a

threshold, although not necessarily a sharp one.

The notion of increasing property is dened in terms of adding edges. The following

intuitive lemma proves that if Q is an increasing property, then increasing p in G (n, p)

increases the probability of the property Q.

Lemma 8.14 If Q is an increasing property of graphs and 0  p  q  1, then the

probability that G (n, q) has property Q is greater than or equal to the probability that

G (n, p) has property Q.

Proof: This proof uses an interesting relationship between G (n, p) and G (n, q). Generate

G (n, q) as follows. First generate G (n, p). This means generating a graph on n vertices

and

with edge probabilities p. Then, independently generate another graph G

take the union by including an edge if either of the two graphs has the edge. Call the

resulting graph H. The graph H has the same distribution as G (n, q). This follows since

the probability that an edge is in H is p + (1  p) qp

1p = q, and, clearly, the edges of H are

independent. The lemma follows since whenever G (n, p) has the property Q, H also has

the property Q.

n, qp

1p

(cid:16)

(cid:17)

We now introduce a notion called replication. An m-fold replication of G(n, p) is a

random graph obtained as follows. Generate m independent copies of G(n, p) on the

same set of vertices. Include an edge in the m-fold replication if the edge is in any one

of the m copies of G(n, p). The resulting random graph has the same distribution as

G(n, q) where q = 1  (1  p)m since the probability that a particular edge is not in the

m-fold replication is the product of probabilities that it is not in any of the m copies

of G(n, p). If the m-fold replication of G(n, p) does not have an increasing property Q,

then none of the m copies of G(n, p) has the property. The converse is not true. If no

copy has the property, their union may have it. Since Q is an increasing property and

q = 1  (1  p)m  1  (1  mp) = mp

Prob (cid:0)G(n, mp) has Q(cid:1)  Prob (cid:0)G(n, q) has Q(cid:1)

(8.3)

We now show that every increasing property Q has a phase transition. The transition

occurs at the point p(n) at which the probability that G(n, p(n)) has property Q is 1

2.

We will prove that for any function asymptotically less then p(n) that the probability of

having property Q goes to zero as n goes to innity.

270

(cid:124)

(cid:123)(cid:122)

copies of G

(cid:125)

If any graph has three or more edges, then the

m-fold replication has three or more edges.

The m-fold

replication H

(cid:124)

(cid:123)(cid:122)

copies of G

(cid:125)

Even if no graph has three or more edges, the

m-fold replication might have three or more edges.

The m-fold

replication H

Figure 8.9: The property that G has three or more edges is an increasing property. Let

H be the m-fold replication of G. If any copy of G has three or more edges, H has three

or more edges. However, H can have three or more edges even if no copy of G has three

or more edges.

Theorem 8.15 Each increasing property Q of G(n, p) has a phase transition at p(n),

where for each n, p(n) is the minimum real number an for which the probability that

G(n, an) has property Q is 1/2.

Proof: Let p0(n) be any function such that

lim

n

p0(n)

p(n)

= 0.

We assert that almost surely G(n, p0) does not have the property Q. Suppose for con-

tradiction, that this is not true. That is, the probability that G(n, p0) has the property

Q does not converge to zero. By the denition of a limit, there exists  > 0 for which

the probability that G(n, p0) has property Q is at least  on an innite set I of n. Let

m = (cid:100)(1/)(cid:101). Let G(n, q) be the m-fold replication of G(n, p0). The probability that

G(n, q) does not have Q is at most (1  )m  e1  1/2 for all n  I. For these n, by

(11.4)

Prob(G(n, mp0) has Q)  Prob(G(n, q) has Q)  1/2.

Since p(n) is the minimum real number an for which the probability that G(n, an) has

property Q is 1/2, it must be that mp0(n)  p(n). This implies that p0(n)

is at least 1/m

p(n)

innitely often, contradicting the hypothesis that lim

n

p0(n)

p(n) = 0.

271

A symmetric argument shows that for any p1(n) such that lim

n

almost surely has property Q.

p(n)

p1(n) = 0, G(n, p1)

8.6 Branching Processes

A branching process is a method for creating a random tree. Starting with the root

node, each node has a probability distribution for the number of its children. The root of

the tree is a parent and its descendants are the children with their descendants being the

grandchildren. The children of the root are the rst generation, their children the second

generation, and so on. Branching processes have obvious applications in population stud-

ies.

We analyze a simple case of a branching process where the distribution of the number

of children at each node in the tree is the same. The basic question asked is what is the

probability that the tree is nite, i.e., the probability that the branching process dies out?

This is called the extinction probability.

Our analysis of the branching process will give the probability of extinction, as well

as the expected size of the components conditioned on extinction.

An important tool in our analysis of branching processes is the generating func-

tion. The generating function for a nonnegative integer valued random variable y is

pixi where pi is the probability that y equals i. The reader not familiar with

f (x) =



(cid:80)

i=0

generating functions should consult Section 12.9 of the appendix.

Let the random variable zj be the number of children in the jth generation and let

fj (x) be the generating function for zj. Then f1 (x) = f (x) is the generating function for

the rst generation where f (x) is the generating function for the number of children at a

node in the tree. The generating function for the 2nd generation is f2(x) = f (f (x)). In

general, the generating function for the j + 1st generation is given by fj+1 (x) = fj (f (x)).

To see this, observe two things.

First, the generating function for the sum of two identically distributed integer valued

random variables x1 and x2 is the square of their generating function

f 2 (x) = p2

0 + (p0p1 + p1p0) x + (p0p2 + p1p1 + p2p0) x2 +    .

For x1 + x2 to have value zero, both x1 and x2 must have value zero, for x1 + x2 to have

value one, exactly one of x1 or x2 must have value zero and the other have value one, and

so on. In general, the generating function for the sum of i independent random variables,

each with generating function f (x), is f i (x).

272

m < 1

f (x)

m > 1

p0

m = 1 and p1 < 1

q

x

Figure 8.10: Illustration of the root of equation f (x) = x in the interval [0,1).

The second observation is that the coecient of xi in fj (x) is the probability of

there being i children in the jth generation. If there are i children in the jth generation,

the number of children in the j + 1st generation is the sum of i independent random

variables each with generating function f (x). Thus, the generating function for the j + 1st

generation, given i children in the jth generation, is f i(x). The generating function for

the j + 1st generation is given by

fj+1(x) =



(cid:88)

i=0

Prob(zj = i)f i(x).

If fj(x) =



(cid:80)

i=0

aixi, then fj+1 is obtained by substituting f (x) for x in fj(x).

Since f (x) and its iterates, f2, f3, . . ., are all polynomials in x with nonnegative co-

ecients, f (x) and its iterates are all monotonically increasing and convex on the unit

interval. Since the probabilities of the number of children of a node sum to one, if p0 < 1,

some coecient of x to a power other than zero in f (x) is nonzero and f (x) is strictly

increasing.

Let q be the probability that the branching process dies out. If there are i children

in the rst generation, then each of the i subtrees must die out and this occurs with

probability qi. Thus, q equals the summation over all values of i of the product of the

probability of i children times the probability that i subtrees will die out. This gives

q = (cid:80)

i=0 piqi. Thus, q is the root of x = (cid:80)

i=0 pixi, that is x = f (x).

This suggests focusing on roots of the equation f (x) = x in the interval [0,1]. The value

x = 1 is always a root of the equation f (x) = x since f (1) =

pi = 1. When is there a

smaller nonnegative root? The derivative of f (x) at x = 1 is f (cid:48)(1) = p1 + 2p2 + 3p3 +    .

Let m = f (cid:48)(1). Thus, m is the expected number of children of a node. If m > 1, one

might expect the tree to grow forever, since each node at time j is expected to have more



(cid:80)

i=0

273

f (x)

q

p0

f (f (x))

f (x)

x

Figure 8.11: Illustration of convergence of the sequence of iterations f1(x), f2(x), . . . to

q.

than one child. But this does not imply that the probability of extinction is zero. In fact,

if p0 > 0, then with positive probability, the root will have no children and the process

will become extinct right away. Recall that for G(n, d

n), the expected number of children

is d, so the parameter m plays the role of d.

If m < 1, then the slope of f (x) at x = 1 is less than one. This fact along with

convexity of f (x) implies that f (x) > x for x in [0, 1) and there is no root of f (x) = x in

the interval [0, 1).

If m = 1 and p1 < 1, then once again convexity implies that f (x) > x for x  [0, 1)

and there is no root of f (x) = x in the interval [0, 1). If m = 1 and p1 = 1, then f (x) is

the straight line f (x) = x.

If m >1, then the slope of f (x) is greater than the slope of x at x = 1. This fact,

along with convexity of f (x), implies f (x) = x has a unique root in [0,1). When p0 = 0,

the root is at x = 0.

Let q be the smallest nonnegative root of the equation f (x) = x. For m < 1 and for

m=1 and p0 < 1, q equals one and for m >1, q is strictly less than one. We shall see

that the value of q is the extinction probability of the branching process and that 1  q is

the immortality probability. That is, q is the probability that for some j, the number of

children in the jth generation is zero. To see this, note that for m > 1, lim

fj (x) = q for

j

0  x < 1. Figure 8.11 illustrates the proof which is given in Lemma 8.16. Similarly note

that when m < 1 or m = 1 with p0 < 1, fj (x) approaches one as j approaches innity.

274

Lemma 8.16 Assume m > 1. Let q be the unique root of f(x)=x in [0,1). In the limit as

j goes to innity, fj (x) = q for x in [0, 1).

Proof: If 0  x  q, then x < f (x)  f (q) and iterating this inequality

x < f1 (x) < f2 (x) <    < fj (x) < f (q) = q.

Clearly, the sequence converges and it must converge to a xed point where f (x) = x.

Similarly, if q  x < 1, then f (q)  f (x) < x and iterating this inequality

x > f1 (x) > f2 (x) >    > fj (x) > f (q) = q.

In the limit as j goes to innity fj (x) = q for all x, 0  x < 1. That is

lim

j

fj(x) = q + 0x + 0x2 +   

and there are no children with probability q and no nite number of children with prob-

ability zero.

Recall that fj (x) is the generating function

Prob (zj = i) xi. The fact that in the

limit the generating function equals the constant q, and is not a function of x, says that

Prob (zj = 0) = q and Prob (zj = i) = 0 for all nite nonzero values of i. The remaining

probability is the probability of a nonnite component. Thus, when m >1, q is the

extinction probability and 1-q is the probability that zj grows without bound.



(cid:80)

i=0

Theorem 8.17 Consider a tree generated by a branching process. Let f (x) be the gener-

ating function for the number of children at each node.

1. If the expected number of children at each node is less than or equal to one, then the

probability of extinction is one unless the probability of exactly one child is one.

2. If the expected number of children of each node is greater than one, then the proba-

bility of extinction is the unique solution to f (x) = x in [0, 1).

Proof: Let pi be the probability of i children at each node. Then f (x) = p0 + p1x +

p2x2 +    is the generating function for the number of children at each node and f (cid:48)(1) =

is the slope of f (x) at x = 1. Observe that f (cid:48)(1) is the expected

p1 + 2p2 + 3p3 +   

number of children at each node.

Since the expected number of children at each node is the slope of f (x) at x = 1, if

the expected number of children is less than or equal to one, the slope of f (x) at x = 1

is less than or equal to one and the unique root of f (x) = x in (0, 1] is at x = 1 and the

probability of extinction is one unless f (cid:48)(1) = 1 and p1 = 1. If f (cid:48)(1) = 1 and p1 = 1,

f (x) = x and the tree is an innite degree one chain. If the slope of f (x) at x = 1 is

greater than one, then the probability of extinction is the unique solution to f (x) = x in

[0, 1).

275

A branching process can be viewed as the process of creating a component in an in-

nite graph. In a nite graph, the probability distribution of descendants is not a constant

as more and more vertices of the graph get discovered.

The simple branching process dened here either dies out or goes to innity. In bio-

logical systems there are other factors, since processes often go to stable populations. One

possibility is that the probability distribution for the number of descendants of a child

depends on the total population of the current generation.

Expected size of extinct families

We now show that the expected size of an extinct family is nite, provided that m (cid:54)= 1.

Note that at extinction, the size must be nite. However, the expected size at extinction

could conceivably be innite, if the probability of dying out did not decay fast enough. For

example, suppose that with probability 1

2 it became extinct with size 3, with probability

1

4 it became extinct with size 9, with probability 1

8 it became extinct with size 27, etc. In

such a case the expected size at extinction would be innite even though the process dies

out with probability one. We now show this does not happen.

Lemma 8.18 If the slope m = f (cid:48) (1) does not equal one, then the expected size of an

extinct family is nite. If the slope m equals one and p1 = 1, then the tree is an innite

degree one chain and there are no extinct families. If m=1 and p1 < 1, then the expected

size of the extinct family is innite.

Proof: Let zi be the random variable denoting the size of the ith generation and let q be

the probability of extinction. The probability of extinction for a tree with k children in

the rst generation is qk since each of the k children has an extinction probability of q.

Note that the expected size of z1, the rst generation, over extinct trees will be smaller

than the expected size of z1 over all trees since when the root node has a larger number

of children than average, the tree is more likely to be innite.

By Bayes rule

Prob (z1 = k|extinction) = Prob (z1 = k)

Prob (extinction|z1 = k)

Prob (extinction)

= pk

qk

q

= pkqk1.

Knowing the probability distribution of z1 given extinction, allows us to calculate the

expected size of z1 given extinction.

E (z1|extinction) =



(cid:88)

k=0

kpkqk1 = f (cid:48) (q) .

We now prove, using independence, that the expected size of the ith generation given

extinction is

276

E (zi|extinction) =

(cid:16)

(cid:17)i

f (cid:48) (q)

.

For i = 2, z2 is the sum of z1 independent random variables, each independent of the ran-

dom variable z1. So, E(z2|z1 = j and extinction) = E( sum of j copies of z1|extinction) =

jE(z1|extinction). Summing over all values of j

E(z2|extinction) =

=



(cid:88)

j=1



(cid:88)

j=1

E(z2|z1 = j and extinction)Prob(z1 = j|extinction)

jE(z1|extinction)Prob(z1 = j|extinction)

= E(z1|extinction)



(cid:88)

j=1

jProb(z1 = j|extinction) = E2(z1|extinction).

Since E(z1|extinction) = f (cid:48)(q), E (z2|extinction) = (f (cid:48) (q))2. Similarly, E (zi|extinction) =

(f (cid:48) (q))i . The expected size of the tree is the sum of the expected sizes of each generation.

That is,

Expected size of

tree given extinction

=



(cid:88)

i=0

E (zi|extinction) =



(cid:88)

i=0

(f (cid:48) (q))i =

1

1  f (cid:48) (q)

.

Thus, the expected size of an extinct family is nite since f (cid:48) (q) < 1 provided m (cid:54)= 1.

The fact that f (cid:48)(q) < 1 is illustrated in Figure 8.10. If m <1, then q=1 and f (cid:48)(q) = m

is less than one. If m >1, then q  [0, 1) and again f (cid:48)(q) <1 since q is the solution to

f (x) = x and f (cid:48)(q) must be less than one for the curve f (x) to cross the line x. Thus,

for m <1 or m >1, f (cid:48)(q) <1 and the expected tree size of

1f (cid:48)(q) is nite. For m=1 and

p1 < 1, one has q=1 and thus f (cid:48)(q) = 1 and the formula for the expected size of the tree

diverges.

1

8.7 CNF-SAT

Phase transitions occur not only in random graphs, but in other random structures

as well. An important example is that of satisability of Boolean formulas in conjunctive

normal form. A conjunctive normal form (CNF) formula over n variables x1, . . . , xn is

an AND of ORs of literals, where a literal is a variable or its negation. For example, the

following is a CNF formula over the variables {x1, x2, x3, x4}:

(x1  x2  x3)(x2  x4)(x1  x4)(x3  x4)(x2  x3  x4).

Each OR of literals is called a clause; for example, the above formula has ve clauses. A

k-CNF formula is a CNF formula in which each clause has size at most k, so the above

formula is a 3-CNF formula. An assignment of true/false values to variables is said to

277

satisfy a CNF formula if it satises every clause in it. Setting all variables to true satises

the above CNF formula, and in fact this formula has multiple satisfying assignments. A

formula is said to be satisable it there exists at least one assignment of truth values to

variables that satises it.

Many important problems can be converted into questions of nding satisfying as-

signments of CNF formulas.

Indeed, the CNF-SAT problem of whether a given CNF

formula is satisable is NP-Complete, meaning that any problem in the class NP can be

converted into it. As a result, it is believed to be highly unlikely that there will ever

exist an ecient algorithm for worst-case instances. However, there are solvers that turn

out to work very well in practice on instances arising from a wide range of applications.

There is also substantial structure and understanding of the satisability of random CNF

formulas. The next two sections discuss each in turn.

8.7.1 SAT-solvers in practice

While the SAT problem is NP-complete, a number of algorithms have been developed

that perform extremely well in practice on SAT formulas arising in a range of applica-

tions. Such applications include hardware and software verication, creating action plans

for robots and robot teams, solving combinatorial puzzles, and even proving mathematical

theorems.

Broadly, there are two classes of solvers: complete solvers and incomplete solvers. Com-

plete solvers are guaranteed to nd a satisfying assignment whenever one exists; if they

do not return a solution, then you know the formula is not satisable. Complete solvers

are often based on some form of recursive tree search. Incomplete solvers instead make a

best eort; they are typically based on some local-search heuristic, and they may fail

to output a solution even when a formula is satisable. However, they are typically much

faster than complete solvers.

An example of a complete solver is the following DPLL (Davis-Putnam-Logemann-

Loveland) style procedure. First, if there are any variables xi that never appear in negated

form in any clause, then set those variables to true and delete clauses where the literal xi

appears. Similarly, if there are any xi that only appear in negated form, then set those

variables to false and delete clauses where the literal xi appears. Second, if there are

any clauses that have only one literal in them (such clauses are called unit clauses), then

set that literal as needed to satisfy the clause. E.g., if the clause was (x3) then one

would set x3 to false. Then remove that clause along with any other clause containing

that literal, and shrink any clause containing the negation of that literal (e.g., a clause

such as (x3  x4) would now become just (x4), and one would then run this rule again

on this clause). Finally, if neither of the above two cases applies, then one chooses some

literal and recursively tries both settings for it. Specically, choose some literal (cid:96) and re-

cursively check if the formula is satisable conditioned on setting (cid:96) to true; if the answer

278

is yes then we are done, but if the answer is no then recursively check if the formula

is satisable conditioned on setting (cid:96) to false. Notice that this procedure is guaranteed to

nd a satisfying assignment whenever one exists.

An example of an incomplete solver is the following local-search procedure called

Walksat. Walksat begins with a random assignment of truth-values to variables. If this

happens to satisfy the formula, then it outputs success.

If not, then it chooses some

unsatised clause C at random. If C contains some variable xi whose truth-value can

be ipped (causing C to be satised) without causing any other clause to be unsatised,

then xis truth-value is ipped. Otherwise, Walksat either (a) ips the truth-value of the

variable in C that causes the fewest other clauses to become unsatised, or else (b) ips

the truth-value of a random xi in C; the choice of whether to perform (a) or (b) is deter-

mined by ipping a coin of bias p. Thus, Walksat is performing a kind of random walk

in the space of truth-assignments, hence the name. Walksat also has two time-thresholds

Tf lips and Trestarts. If the above procedure has not found a satisfying assignment after

Tf lips ips, it then restarts with a fresh initial random assignment and tries again; if that

entire process has not found a satisfying assignment after Trestarts restarts, then it outputs

no assignment found.

The above solvers are just two simple examples. Due to the importance of the CNF-

SAT problem, development of faster SAT-solvers is an active area of computer science

research. SAT-solving competitions are held each year, and solvers are routinely being

used to solve challenging verication, planning, and scheduling problems.

8.7.2 Phase Transitions for CNF-SAT

We now consider the question of phase transitions in the satisability of random k-

CNF formulas.

k

Generate a random CNF formula f with n variables, m clauses, and k literals per

clause, where recall that a literal is a variable or its negation. Specically, each clause

(cid:1)2k possible clauses of size

in f is selected independently at random from the set of all (cid:0)n

k. Equivalently, to generate a clause, choose a random set of k distinct variables, and

then for each of those variables choose to either negate it or not with equal probabil-

ity. Here, the number of variables n is going to innity, m is a function of n, and k is

a xed constant. A reasonable value to think of for k is k = 3. Unsatisability is an

increasing property since adding more clauses preserves unsatisability. By arguments

similar to Section 8.5, there is a phase transition, i.e., a function m(n) such that if m1(n)

is o(m(n)), a random formula with m1(n) clauses is, almost surely, satisable and for

m2(n) with m2(n)/m(n)  , a random formula with m2(n) clauses is, almost surely,

unsatisable. It has been conjectured that there is a constant rk independent of n such

that rkn is a sharp threshold.

279

Here we derive upper and lower bounds on rk. It is relatively easy to get an upper

bound on rk. A xed truth assignment satises a random k clause with probability

1  1

2k because of the 2k truth assignments to the k variables in the clause, only one

fails to satisfy the clause. Thus, with probability 1

2k , the clause is not satised, and with

probability 1  1

2k , the clause is satised. Let m = cn. Now, cn independent clauses are

(cid:1)cn. Since there are 2n truth

all satised by the xed assignment with probability (cid:0)1  1

2k

assignments, the expected number of satisfying assignments for a formula with cn clauses

(cid:1)cn. If c = 2k ln 2, the expected number of satisfying assignments is

is 2n (cid:0)1  1

2k

2n (cid:0)1  1

2k

(cid:1)n2k ln 2 .

(cid:0)1  1

2k

(cid:1)2k

is at most 1/e and approaches 1/e in the limit. Thus,

2n (cid:0)1  1

2k

(cid:1)n2k ln 2  2nen ln 2 = 2n2n = 1.

For c > 2k ln 2, the expected number of satisfying assignments goes to zero as n  .

Here the expectation is over the choice of clauses which is random, not the choice of a

truth assignment. From the rst moment method, it follows that a random formula with

cn clauses is almost surely not satisable. Thus, rk  2k ln 2.

The other direction, showing a lower bound for rk, is not that easy. From now on, we

focus only on the case k = 3. The statements and algorithms given here can be extended

It turns out that the second moment method

to k  4, but with dierent constants.

cannot be directly applied to get a lower bound on r3 because the variance is too high. A

simple algorithm, called the Smallest Clause Heuristic (abbreviated SC), yields a satisfy-

ing assignment with probability tending to one if c < 2

3. Other more

dicult to analyze algorithms, push the lower bound on r3 higher.

3, proving that r3  2

The Smallest Clause Heuristic repeatedly executes the following. Assign true to a

random literal in a random shortest clause and delete the clause since it is now satised.

In more detail, pick at random a 1-literal clause, if one exists, and set that literal to

true. If there is no 1-literal clause, pick a 2-literal clause, select one of its two literals and

set the literal to true. Otherwise, pick a 3-literal clause and a literal in it and set the

literal to true. If we encounter a 0-length clause, then we have failed to nd a satisfying

assignment; otherwise, we have found one.

A related heuristic, called the Unit Clause Heuristic, selects a random clause with one

literal, if there is one, and sets the literal in it to true. Otherwise, it picks a random as

yet unset literal and sets it to true. Another variation is the pure literal heuristic. It

sets a random pure literal, a literal whose negation does not occur in any clause, to

true, if there are any pure literals; otherwise, it sets a random literal to true.

When a literal w is set to true, all clauses containing w are deleted, since they are

satised, and w is deleted from any clause containing w. If a clause is reduced to length

280

zero (no literals), then the algorithm has failed to nd a satisfying assignment to the

formula. The formula may, in fact, be satisable, but the algorithm has failed.

Example: Consider a 3-CNF formula with n variables and cn clauses. With n variables

there are 2n literals, since a variable and its complement are distinct literals. The expected

number of times a literal occurs is calculated as follows. Each clause has three literals.

Thus, each of the 2n dierent literals occurs (3cn)

2c times on average. Suppose c = 5.

Then each literal appears 7.5 times on average. If one sets a literal to true, one would

expect to satisfy 7.5 clauses. However, this process is not repeatable since after setting a

literal to true there is conditioning so that the formula is no longer random.

2n = 3

Theorem 8.19 If the number of clauses in a random 3-CNF formula grows as cn where

c is a constant less than 2/3, then with probability 1  o(1), the Shortest Clause (SC)

Heuristic nds a satisfying assignment.

The proof of this theorem will take the rest of the section. A general impediment to

proving that simple algorithms work for random instances of many problems is condition-

ing. At the start, the input is random and has properties enjoyed by random instances.

But, as the algorithm is executed, the data is no longer random; it is conditioned on the

steps of the algorithm so far. In the case of SC and other heuristics for nding a satisfying

assignment for a Boolean formula, the argument to deal with conditioning is relatively

simple.

We supply some intuition before giving the proof. Imagine maintaining a queue of 1

and 2-clauses. A 3-clause enters the queue when one of its literals is set to false and it

becomes a 2-clause. SC always picks a 1 or 2-clause if there is one and sets one of its

literals to true. At any step when the total number of 1 and 2-clauses is positive, one of

the clauses is removed from the queue. Consider the arrival rate, that is, the expected

number of arrivals into the queue at a given time t. For a particular clause to arrive into

the queue at time t to become a 2-clause, it must contain the negation of the literal being

set to true at time t. It can contain any two other literals not yet set. The number of

such clauses is (cid:0)nt

(cid:1)22. So, the probability that a particular clause arrives in the queue at

time t is at most

2

(cid:0)nt

2

(cid:0)n

3

(cid:1)22

(cid:1)23



3

2(n  2)

.

Since there are cn clauses in total, the arrival rate is 3c

2 , which for c < 2/3 is a constant

strictly less than one. The arrivals into the queue of dierent clauses occur independently

(Lemma 8.20), the queue has arrival rate strictly less than one, and the queue loses one

or more clauses whenever it is nonempty. This implies that the queue never has too many

clauses in it. A slightly more complicated argument will show that no clause remains as

a 1 or 2-clause for (ln n) steps (Lemma 8.21). This implies that the probability of two

contradictory 1-length clauses, which is a precursor to a 0-length clause, is very small.

281

Lemma 8.20 Let Ti be the rst time that clause i turns into a 2-clause. Ti is  if clause

i gets satised before turning into a 2-clause. The Ti are mutually independent over the

randomness in constructing the formula and the randomness in SC, and for any t,

Prob(Ti = t) 

3

2(n  2)

.

Proof: For the proof, generate the clauses in a dierent way. The important thing is

that the new method of generation, called the method of deferred decisions, results in

the same distribution of input formulae as the original. The method of deferred decisions

is tied in with the SC algorithm and works as follows. At any time, the length of each

clause (number of literals) is all that we know; we have not yet picked which literals are

in each clause. At the start, every clause has length three and SC picks one of the clauses

uniformly at random. Now, SC wants to pick one of the three literals in that clause to

set to true, but we do not know which literals are in the clause. At this point, we pick

uniformly at random one of the 2n possible literals. Say for illustration, we picked x102.

The literal x102 is placed in the clause and set to true. The literal x102 is set to false. We

must also deal with occurrences of the literal or its negation in all other clauses, but again,

we do not know which clauses have such an occurrence. We decide that now. For each

clause, independently, with probability 3/n include either the literal x102 or its negation

x102, each with probability 1/2. In the case that we included x102 (the literal we had set

to true), the clause is now deleted, and if we included x102 (the literal we had set to false),

we decrease the residual length of the clause by one.

At a general stage, suppose the fates of i variables have already been decided and

n  i remain. The residual length of each clause is known. Among the clauses that are

not yet satised, choose a random shortest length clause. Among the n  i variables

remaining, pick one uniformly at random, then pick it or its negation as the new literal.

Include this literal in the clause thereby satisfying it. Since the clause is satised, the

algorithm deletes it. For each other clause, do the following.

If its residual length is

l, decide with probability l/(n  i) to include the new variable in the clause and if so

with probability 1/2 each, include it or its negation. If the literal that was set to true is

included in a clause, delete the clause as it is now satised. If its negation is included

in a clause, then just delete the literal and decrease the residual length of the clause by one.

Why does this yield the same distribution as the original one? First, observe that the

order in which the variables are picked by the method of deferred decisions is independent

of the clauses; it is just a random permutation of the n variables. Look at any one clause.

For a clause, we decide in order whether each variable or its negation is in the clause. So

for a particular clause and a particular triple i, j, and k with i < j < k, the probability

that the clause contains the ith, the jth, and kth literal (or their negations) in the order

282

determined by deferred decisions is:

n

(cid:0)1  3

(cid:0)1  2

(cid:16)

ni

1  1

nj

(cid:1) (cid:0)1  3

n1

(cid:1)

(cid:1)    (cid:0)1  3

(cid:1)   

(cid:17)

(cid:16)

ni+2

1  2

nj+2

3

ni+1

(cid:17)

  (cid:0)1  1

nk+2

(cid:1) (cid:0)1  2

(cid:17) (cid:16)

ni1

1  1

nj1

2

nj+1

(cid:1)

1

nk+1 =

3

n(n1)(n2),

where the (1     ) factors are for not picking the current variable or negation to be in-

cluded and the others are for including the current variable or its negation. Independence

among clauses follows from the fact that we have never let the occurrence or nonoccur-

rence of any variable in any clause inuence our decisions on other clauses.

Now, we prove the lemma by appealing to the method of deferred decisions to generate

the formula. Ti = t if and only if the method of deferred decisions does not put the current

literal at steps 1, 2, . . . , t  1 into the ith clause, but puts the negation of the literal at

step t into it. Thus, the probability is precisely

(cid:0)1  3

n

1

2

(cid:1) (cid:0)1  3

n1

(cid:1)    (cid:0)1  3

nt+2

(cid:1)

3

nt+1  3

2(n2),

as claimed. Clearly the Ti are independent since again deferred decisions deal with dier-

ent clauses independently.

Lemma 8.21 There exists a constant c2 such that with probability 1  o(1), no clause

remains a 2 or 1-clause for more than c2 ln n steps.

I.e., once a 3-clause becomes a

2-clause, it is either satised or reduced to a 0-clause in O(ln n) steps.

Proof: Say that t is a busy time if there exists at least one 2-clause or 1-clause at time

t, and dene a time-window [r + 1, s] to be a busy window if time r is not busy but

then each t  [r + 1, s] is a busy time. We will prove that for some constant c2, with

probability 1  o(1), all busy windows have length at most c2 ln n.

Fix some r and s and consider the event that [r + 1, s] is a busy window. Since SC always

decreases the total number of 1 and 2-clauses by one whenever it is positive, we must have

generated at least s  r new 2-clauses between r and s. Now, dene an indicator variable

for each 3-clause which has value one if the clause turns into a 2-clause between r and

s. By Lemma 8.20 these variables are independent and the probability that a particular

3-clause turns into a 2-clause at a time t is at most 3/(2(n  2)). Summing over t between

r and s,

Prob (cid:0)a 3-clause turns into a 2-clause during [r, s](cid:1) 

3(s  r)

2(n  2)

.

Since there are cn clauses in all, the expected sum of the indicator variables is cn 3(sr)

3c(sr)

2

2(n2) 

. Note that 3c/2 < 1, which implies the arrival rate into the queue of 2 and 1-

clauses is a constant strictly less than one. Using Cherno bounds, if s  r  c2 ln n for

283

appropriate constant c2, the probability that more than s  r clauses turn into 2-clauses

between r and s is at most 1/n3. Applying the union bound over all O(n2) possible choices

of r and s, we get that the probability that any clause remains a 2 or 1-clause for more

than c2 ln n steps is o(1).

Now, assume the 1  o(1) probability event of Lemma 8.21 that no clause remains a

2 or 1-clause for more than c2 ln n steps. We will show that this implies it is unlikely the

SC algorithm terminates in failure.

Suppose SC terminates in failure. This means that at some time t, the algorithm

generates a 0-clause. At time t  1, this clause must have been a 1-clause. Suppose the

clause consists of the literal w. Since at time t  1, there is at least one 1-clause, the

shortest clause rule of SC selects a 1-clause and sets the literal in that clause to true.

This other clause must have been w. Let t1 be the rst time either of these two clauses,

w or w, became a 2-clause. We have t  t1  c2 ln n. Clearly, until time t, neither of these

two clauses is picked by SC. So, the literals which are set to true during this period are

chosen independent of these clauses. Say the two clauses were w + x + y and w + u + v

at the start. x, y, u, and v must all be negations of literals set to true during steps t1 to

t. So, there are only O (cid:0)(ln n)4(cid:1) choices for x, y, u, and v for a given value of t. There are

O(n) choices of w, O(n2) choices of which two clauses i and j of the input become these

w and w, and n choices for t. Thus, there are O (cid:0)n4(ln n)4(cid:1) choices for what these clauses

contain and which clauses they are in the input. On the other hand, for any given i and j,

the probability that clauses i and j both match a given set of literals is O(1/n6). Thus the

probability that these choices are actually realized is therefore O (cid:0)n4(ln n)4/n6(cid:1) = o(1),

as required.

8.8 Nonuniform Models of Random Graphs

So far we have considered the G(n, p) random graph model in which all vertices have

the same expected degree, and moreover degrees are concentrated close to their expecta-

tion. However, large graphs occurring in the real world tend to have power law degree

distributions. For a power law degree distribution, the number f (d) of vertices of degree

d scales as 1/d for some constant  > 0.

One way to generate such graphs is to stipulate that there are f (d) vertices of degree

d and choose uniformly at random from the set of graphs with this degree distribution.

Clearly, in this model the graph edges are not independent and this makes these random

graphs harder to analyze. But the question of when phase transitions occur in random

graphs with arbitrary degree distributions is still of interest. In this section, we consider

when a random graph with a nonuniform degree distribution has a giant component. Our

treatment in this section, and subsequent ones, will be more intuitive without providing

rigorous proofs.

284

Consider a graph in which half of the vertices are degree one and half

are degree two. If a vertex is selected at random, it is equally likely to be

degree one or degree two. However, if we select an edge at random and

walk to a random endpoint, the vertex is twice as likely to be degree

two as degree one.

In many graph algorithms, a vertex is reached

by randomly selecting an edge and traversing the edge to reach an

endpoint. In this case, the probability of reaching a degree i vertex is

proportional to ii where i is the fraction of vertices that are degree

i.

Figure 8.12: Probability of encountering a degree d vertex when following a path in a

graph.

8.8.1 Giant Component in Graphs with Given Degree Distribution

Molloy and Reed address the issue of when a random graph with a nonuniform degree

distribution has a giant component. Let i be the fraction of vertices of degree i. There

will be a giant component if and only if

i(i  2)i > 0.



(cid:80)

i=0

To see intuitively that this is the correct formula, consider exploring a component

of a graph starting from a given seed vertex. Degree zero vertices do not occur except

If a degree one vertex is encountered, then

in the case where the vertex is the seed.

that terminates the expansion along the edge into the vertex. Thus, we do not want to

encounter too many degree one vertices. A degree two vertex is neutral in that the vertex

is entered by one edge and left by the other. There is no net increase in the size of the

frontier. Vertices of degree i greater than two increase the frontier by i  2 vertices. The

vertex is entered by one of its edges and thus there are i  1 edges to new vertices in the

frontier for a net gain of i  2. The ii in (i  2) ii is proportional to the probability of

reaching a degree i vertex and the i  2 accounts for the increase or decrease in size of

the frontier when a degree i vertex is reached.

Example: Consider applying the Molloy Reed conditions to the G(n, p) model, and use

pi to denote the probability that a vertex has degree i, i.e., in analog to i. It turns out

that the summation (cid:80)n

i=0 i(i  2)pi gives value zero precisely when p = 1/n, the point

at which the phase transition occurs. At p = 1/n, the average degree of each vertex is

one and there are n/2 edges. However, the actual degree distribution of the vertices is

binomial, where the probability that a vertex is of degree i is given by pi = (cid:0)n

(cid:1)pi(1p)ni.

i(i  2)pi = 0 for pi = (cid:0)n

We now show that lim

n

(cid:1)pi(1  p)ni when p = 1/n.

i

i

n

(cid:80)

i=0

285

lim

n

n

(cid:88)

i=0

i(i  2)

(cid:18)n

i

(cid:19) (cid:18) 1

n

(cid:19)i (cid:18)

1 

1

n

(cid:19)ni

= lim

n

n

(cid:88)

i=0

i(i  2)

n(n  1)    (n  i + 1)

i! ni

(cid:18)

1 

1

n

(cid:18) n

(cid:19)n (cid:18)

1 

(cid:19)i

1

n

(cid:19)i

n(n  1)    (n  i + 1)

i! ni

n  1

=



n

(cid:88)

i(i  2)

1

e

lim

n

i=0

i(i  2)

i!

.



(cid:88)

i=0

To see that



(cid:80)

i=0

i(i2)

i! = 0, note that



(cid:88)

i=0

i

i!

=



(cid:88)

i=1

i

i!

=



(cid:88)

i=1

1

(i  1)!

=



(cid:88)

i=0

1

i!



(cid:88)

i=0

i2

i!

=



(cid:88)

i=1

i

(i  1)!

=



(cid:88)

i=0

i + 1

i!

=



(cid:88)

i=0

i

i!

+



(cid:88)

i=0

1

i!

= 2



(cid:88)

i=0

1

i!

.

and

Thus,



(cid:80)

i=0

i(i2)

i! =



(cid:80)

i=0

i2

i!  2



(cid:80)

i=0

i

i! = 0.

8.9 Growth Models

Many graphs that arise in the outside world started as small graphs that grew over

time. In a model for such graphs, vertices and edges are added to the graph over time.

In such a model there are many ways in which to select the vertices for attaching a new

edge. One is to select two vertices uniformly at random from the set of existing vertices.

Another is to select two vertices with probability proportional to their degree. This latter

method is referred to as preferential attachment. A variant of this method would be to

add a new vertex at each unit of time and with probability  add an edge where one

end of the edge is the new vertex and the other end is a vertex selected with probability

proportional to its degree. The graph generated by this latter method is a tree with a

power law degree distribution.

286

8.9.1 Growth Model Without Preferential Attachment

Consider a growth model for a random graph without preferential attachment. Start

with zero vertices. At each unit of time a new vertex is created and with probability 

two vertices chosen at random are joined by an edge. The two vertices may already have

an edge between them. In this case, we add another edge. So, the resulting structure is a

multi-graph, rather then a graph. Since at time t, there are t vertices and in expectation

only O(t) edges where there are t2 pairs of vertices, it is very unlikely that there will be

many multiple edges.

The degree distribution for this growth model is calculated as follows. The number of

vertices of degree k at time t is a random variable. Let dk(t) be the expectation of the

number of vertices of degree k at time t. The number of isolated vertices increases by one

at each unit of time and decreases by the number of isolated vertices, b(t), that are picked

to be end points of the new edge. b(t) can take on values 0,1, or 2. Taking expectations,

d0(t + 1) = d0(t) + 1  E(b(t)).

Now b(t) is the sum of two 0-1 valued random variables whose values are the number

of degree zero vertices picked for each end point of the new edge. Even though the

two random variables are not independent, the expectation of b(t) is the sum of the

expectations of the two variables and is 2 d0(t)

. Thus,

t

d0(t + 1) = d0(t) + 1  2

d0(t)

t

.

The number of degree k vertices increases whenever a new edge is added to a degree k  1

vertex and decreases when a new edge is added to a degree k vertex. Reasoning as above,

dk (t + 1) = dk(t) + 2

dk1(t)

t

 2

dk(t)

t

.

(8.4)

Note that this formula, as others in this section, is not quite precise. For example, the

same vertex may be picked twice, so that the new edge is a self-loop. For k << t, this

problem contributes a minuscule error. Restricting k to be a xed constant and letting

t   in this section avoids these problems.

Assume that the above equations are exactly valid. Clearly, d0(1) = 1 and d1(1) =

d2(1) =    = 0. By induction on t, there is a unique solution to (8.4), since given dk(t)

for all k, the equation determines dk(t + 1) for all k. There is a solution of the form

dk(t) = pkt, where pk depends only on k and not on t, provided k is xed and t  .

Again, this is not precisely true since d1(1) = 0 and d1(2) > 0 clearly contradict the

existence of a solution of the form d1(t) = p1t.

Set dk(t) = pkt. Then,

(t + 1) p0 = p0t + 1  2

p0t

t

287

Figure 8.13: In selecting a component at random, each of the two components is equally

likely to be selected. In selecting the component containing a random vertex, the larger

component is twice as likely to be selected.

and

p0 = 1  2p0

p0 =

1

1 + 2

(t + 1) pk = pkt + 2

pk1t

t

 2

pkt

t

pk = 2pk1  2pk

pk =

=

=

2

1 + 2

(cid:18) 2

pk1

(cid:19)k

1 + 2

p0

1

1 + 2

(cid:18) 2

(cid:19)k

1 + 2

.

(8.5)

Thus, the model gives rise to a graph with a degree distribution that falls o exponentially

fast with the degree.

The generating function for component size

Let nk(t) be the expected number of components of size k at time t. Then nk(t) is

proportional to the probability that a randomly picked component is of size k. This is

not the same as picking the component containing a randomly selected vertex (see Figure

8.13). Indeed, the probability that the size of the component containing a randomly se-

lected vertex is k is proportional to knk(t). We will show that there is a solution for nk(t)

of the form akt where ak is a constant independent of t. After showing this, we focus on

the generating function g(x) for the numbers kak(t) and use g(x) to nd the threshold

for giant components.

Consider n1(t), the expected number of isolated vertices at time t. At each unit of

t many isolated

time, an isolated vertex is added to the graph and an expected 2n1(t)

288

vertices are chosen for attachment and thereby leave the set of isolated vertices. Thus,

n1(t + 1) = n1(t) + 1  2

n1(t)

t

.

For k >1, nk(t) increases when two smaller components whose sizes sum to k are joined

by an edge and decreases when a vertex in a component of size k is chosen for attachment.

The probability that a vertex selected at random will be in a size k component is knk(t)

.

Thus,

t

nk(t + 1) = nk(t) + 

k1

(cid:88)

j=1

jnj(t)

t

(k  j)nkj(t)

t

 2

knk(t)

t

.

To be precise, one needs to consider the actual number of components of various sizes,

rather than the expected numbers. Also, if both vertices at the end of the edge are in the

same k-vertex component, then nk(t) does not go down as claimed. These small inaccu-

racies can be ignored.

Consider solutions of the form nk(t) = akt. Note that nk(t) = akt implies the num-

ber of vertices in a connected component of size k is kakt. Since the total number of

vertices at time t is t, kak is the probability that a random vertex is in a connected

component of size k. The recurrences here are valid only for k xed as t  . So

(cid:80)

k=0 kak may be less than 1, in which case, there are nonnite size components whose

sizes are growing with t. Solving for ak yields a1 = 1

1+2 and ak = 

1+2k

j(k  j)ajakj.

k1

(cid:80)

j=1

Consider the generating function g(x) for the distribution of component sizes where

the coecient of xk is the probability that a vertex chosen at random is in a component

of size k.



(cid:88)

g(x) =

kakxk.

k=1

Now, g(1) = (cid:80)

k=0 kak is the probability that a randomly chosen vertex is in a nite sized

component. For  = 0, this is clearly one, since all vertices are in components of size

one. On the other hand, for  = 1, the vertex created at time one has expected degree

log n (since its expected degree increases by 2/t and (cid:80)n

t=1(2/t) = (log n)); so, it is in a

nonnite size component. This implies that for  = 1, g(1) < 1 and there is a nonnite

size component. Assuming continuity, there is a critical above which g(1) < 1. From the

formula for the a(cid:48)

is, we will derive the dierential equation

g = 2xg(cid:48) + 2xgg(cid:48) + x

and then use the equation for g to determine the value of  at which the phase transition

for the appearance of a nonnite sized component occurs.

Derivation of g(x)

289

From

and

derive the equations

and

a1 =

1

1 + 2

ak =



1 + 2k

k1

(cid:88)

j=1

j(k  j)ajakj

a1 (1 + 2)  1 = 0

ak (1 + 2k) = 

k1

(cid:88)

j=1

j(k  j)ajakj

for k  2. The generating function is formed by multiplying the kth equation by kxk and

summing over all k. This gives

x +



(cid:88)

k=1

kakxk + 2x



(cid:88)

k=1

akk2xk1 = 



(cid:88)

kxk

k1

(cid:88)

k=1

j=1

j(k  j)ajakj.

Note that

Thus,

g(x) =



(cid:80)

k=1

kakxk and g(cid:48)(x) =



(cid:80)

k=1

akk2xk1.

x + g(x) + 2xg(cid:48)(x) = 



(cid:88)

kxk

k1

(cid:88)

k=1

j=1

j(k  j)ajakj.

Working with the right hand side



(cid:88)



kxk

k1

(cid:88)

k=1

j=1

j(k  j)ajakj = x



(cid:88)

k1

(cid:88)

k=1

j=1

j(k  j)(j + k  j)xk1ajakj.

Now breaking the j + k  j into two sums gives



(cid:88)

k1

(cid:88)

x

k=1

j=1

j2ajxj1(k  j)akjxkj + x



(cid:88)

k1

(cid:88)

k=1

j=1

jajxj(k  j)2akjxkj1.

Notice that the second sum is obtained from the rst by substituting k  j for j and that

both terms are xg(cid:48)g. Thus,

x + g(x) + 2xg(cid:48)(x) = 2xg(cid:48)(x)g(x).

Hence,

g(cid:48) =

1

2

1  g

x

1  g

.

290

Phase transition for nonnite components

The generating function g(x) contains information about the nite components of the

graph. A nite component is a component of size 1, 2, . . ., which does not depend on t.

Observe that g(1) =

kak and hence g(1) is the probability that a randomly chosen



(cid:80)

k=0

vertex will belong to a component of nite size. If g(1) = 1 there are no innite compo-

nents. When g(1) (cid:54)= 1, then 1  g(1) is the expected fraction of the vertices that are in

nonnite components. Potentially, there could be many such nonnite components. But

an argument similar to Part 3 of Theorem ?? concludes that two fairly large components

would merge into one. Suppose there are two connected components at time t, each of

size at least t4/5. Consider the earliest created 1

2t4/5 vertices in each part. These vertices

must have lived for at least 1

2t4/5 time after creation. At each time, the probability of an

edge forming between two such vertices, one in each component, is at least (t2/5) and

so the probability that no such edge formed is at most (cid:0)1  t2/5(cid:1)t4/5/2  e(t2/5)  0.

So with high probability, such components would have merged into one. But this still

leaves open the possibility of many components of size t, (ln t)2, or some other slowly

growing function of t.

We now calculate the value of  at which the phase transition for a nonnite component

occurs. Recall that the generating function for g (x) satises

g(cid:48) (x) =

1

2

1  g(x)

x

1  g (x)

.

If  is greater than some critical, then g(1) (cid:54)= 1. In this case the above formula at x = 1

simplies with 1  g(1) canceling from the numerator and denominator, leaving just 1

2 .

Since kak is the probability that a randomly chosen vertex is in a component of size k,

the average size of the nite components is g(cid:48)(1) =

k2ak. Now, g(cid:48)(1) is given by



(cid:80)

k=1

g(cid:48)(1) =

1

2

(8.6)

for all  greater than critical. If  is less than critical, then all vertices are in nite compo-

nents. In this case g(1) = 1 and both the numerator and the denominator approach zero.

Appling LHopitals rule

or

lim

x1

g(cid:48)(x) = 1

2

xg(cid:48)(x)g(x)

x2

g(cid:48)(x)

(cid:12)

(cid:12)

(cid:12)

(cid:12)x=1

(g(cid:48)(1))2 = 1

2

(cid:0)g(cid:48)(1)  g(1)(cid:1).

291

The quadratic (g(cid:48)(1))2  1

2 g(cid:48)(1) + 1

2 g(1) = 0 has solutions

1

2 

g(cid:48)(1) =

(cid:113) 1

42  4

2

2

1 

=



1  8

4

.

(8.7)

The two solutions given by (8.7) become complex for  > 1/8 and thus can be valid only

for 0    1/8. For  > 1/8, the only solution is g(cid:48)(1) = 1

2 and an innite component

exists. As  is decreased, at  = 1/8 there is a singular point where for  < 1/8 there are

three possible solutions, one from (8.6) which implies a giant component and two from

(8.7) which imply no giant component. To determine which one of the three solutions is

valid, consider the limit as   0. In the limit all components are of size one since there

are no edges. Only (8.7) with the minus sign gives the correct solution

1 

g(cid:48) (1) =



1  8

4

=

1  (cid:0)1  1

4642 +   (cid:1)

28  1

4

= 1 + 4 +    = 1.

In the absence of any nonanalytic behavior in the equation for g(cid:48) (x) in the region

0   < 1/8, we conclude that (8.7) with the minus sign is the correct solution for

0   < 1/8 and hence the critical value of  for the phase transition is 1/8. As we shall

see, this is dierent from the static case.

As the value of  is increased, the average size of the nite components increase from

one to

1 



1  8

4

(cid:12)

(cid:12)

(cid:12)

(cid:12)=1/8

= 2

when  reaches the critical value of 1/8. At  = 1/8, the average size of the nite com-

ponents jumps to 1

2 as the giant component swallows

2

up the nite components starting with the larger components.

(cid:12)

(cid:12)=1/8 = 4 and then decreases as 1

Comparison to static random graph

Consider a static random graph with the same degree distribution as the graph in the

growth model. Again let pk be the probability of a vertex being of degree k. From (8.5)

pk =

(2)k

(1 + 2)k+1 .

Recall the Molloy Reed analysis of random graphs with given degree distributions which



(cid:80)

i=0

i(i  2)pi = 0. Using this, it is easy to see

asserts that there is a phase transition at

that a phase transition occurs for  = 1/4. For  = 1/4,

pk = (2)k

(1+2)k+1 =

(cid:17)k

(cid:17)k+1 =

(cid:16) 1

2

1

2

(cid:16)

1+

(cid:17)k

(cid:16) 1

2

(cid:0) 3

2

3

2

(cid:1)k = 2

3

(cid:1)k

(cid:0) 1

3

292

Fractional

size of

innite

component

1

0

grown

static

1/8 1/4



Figure 8.14: Comparison of the static random graph model and the growth model. The

curve for the growth model is obtained by integrating g(cid:48).

and



(cid:88)

i=0

i(i  2) 2

3

(cid:0) 1

3

(cid:1)i = 2

3



(cid:80)

i=0

i2 (cid:0) 1

3

(cid:1)i  4

3



(cid:80)

i=0

i (cid:0) 1

3

(cid:1)i = 2

3  3

2  4

3  3

4 = 0.

Recall that 1 + a + a2 +    = 1

1a , a + 2a2 + 3a3    = a

(1a)2 , and a + 4a2 + 9a3    = a(1+a)

(1a)3 .

See references at end of the chapter for calculating the fractional size Sstatic of the

giant component in the static graph. The result is

Sstatic =

(cid:26) 0

1 



+

1

2+2

  1

4

 > 1

4

8.9.2 Growth Model With Preferential Attachment

Consider a growth model with preferential attachment. At each time unit, a vertex is

added to the graph. Then with probability , an edge is attached to the new vertex and

to a vertex selected at random with probability proportional to its degree. This model

generates a tree with a power law distribution.

Let di(t) be the expected degree of the ith vertex at time t. The sum of the expected

degrees of all vertices at time t is 2t and thus the probability that an edge is connected

to vertex i at time t is di(t)

2t . The degree of vertex i is governed by the equation



t

di(t) = 

di (t)

2t

=

di(t)

2t

where  is the probability that an edge is added at time t and di(t)

2t

the vertex i is selected for the end point of the edge.

is the probability that

293

expected

degree

d

(cid:122)

i = 1, 2, 3

(cid:113) t

i



di(t) > d

(cid:125)(cid:124)

di(t) < d

(cid:125)(cid:124)

(cid:123)

i = t

(cid:123) (cid:122)

i = 2

d2 t

 vertex number 

Figure 8.15: Illustration of degree of ith vertex at time t. At time t, vertices numbered

1 to 2

d2 t have degrees greater than d.

The two in the denominator governs the solution, which is of the form at

1

2 or a = i

of a is determined by the initial condition di (t) =  at t = i. Thus,  = ai

Hence, di(t) = 

(cid:113) t

i .

1

2 . The value

1

2 .

Next, we determine the probability distribution of vertex degrees. Now, di(t) is less

d2 t and thus

d2 . Hence, the probability that a vertex has degree

d2 t. The fraction of the t vertices at time t for which i > 2

than d provided i > 2

that the degree is less than d is 1  2

less than d is 1  2

d2 . The probability density p(d) satises

(cid:90) d

0

p(d)d = Prob(degree < d) = 1 

2

d2

and can be obtained from the derivative of Prob(degree < d).

p(d) =



d

(cid:18)

1 

(cid:19)

2

d2

= 2

2

d3 ,

a power law distribution.

8.10 Small World Graphs

In the 1960s, Stanley Milgram carried out an experiment that indicated that most

pairs of individuals in the United States were connected by a short sequence of acquain-

tances. Milgram would ask a source individual, say in Nebraska, to start a letter on its

journey to a target individual in Massachusetts. The Nebraska individual would be given

basic information about the target including his address and occupation and asked to

send the letter to someone he knew on a rst name basis, who was closer to the target

individual, in order to transmit the letter to the target in as few steps as possible. Each

294

person receiving the letter would be given the same instructions. In successful experi-

ments, it would take on average ve to six steps for a letter to reach its target. This

research generated the phrase six degrees of separation along with substantial research

in social science on the interconnections between people. Surprisingly, there was no work

on how to nd the short paths using only local information.

In many situations, phenomena are modeled by graphs whose edges can be partitioned

into local and long-distance. We adopt a simple model of a directed graph due to Klein-

berg, having local and long-distance edges. Consider a 2-dimensional nn grid where each

vertex is connected to its four adjacent vertices via bidirectional local edges. In addition

to these local edges, there is one long-distance edge out of each vertex. The probability

that the long-distance edge from vertex u terminates at v, v (cid:54)= u, is a function of the

distance d(u, v) from u to v. Here distance is measured by the shortest path consisting

only of local grid edges. The probability is proportional to 1/dr(u, v) for some constant r.

This gives a one parameter family of random graphs. For r equal zero, 1/d0(u, v) = 1 for

all u and v and thus the end of the long-distance edge at u is uniformly distributed over all

vertices independent of distance. As r increases the expected length of the long-distance

edge decreases. As r approaches innity, there are no long-distance edges and thus no

paths shorter than that of the lattice path. What is interesting is that for r less than two,

there are always short paths, but no local algorithm to nd them. A local algorithm is an

algorithm that is only allowed to remember the source, the destination, and its current

location and can query the graph to nd the long-distance edge at the current location.

Based on this information, it decides the next vertex on the path.

The diculty is that for r < 2, the end points of the long-distance edges are too-

uniformly distributed over the vertices of the grid. Although short paths exist, it is

unlikely on a short path to encounter a long-distance edge whose end point is close to

the destination. When r equals two, there are short paths and the simple algorithm that

always selects the edge that ends closest to the destination will nd a short path. For r

greater than two, again there is no local algorithm to nd a short path. Indeed, with high

probability, there are no short paths at all.

The probability that the long-distance edge from u goes to v is proportional to

dr(u, v). Note that the constant of proportionality will vary with the vertex u depend-

ing on where u is relative to the border of the n  n grid. However, the number of

vertices at distance exactly k from u is at most 4k and for k  n/2 is at least k. Let

cr(u) = (cid:80)

It is the inverse of the constant of

proportionality.

v dr(u, v) be the normalizing constant.

For r > 2, cr(u) is lower bounded by

cr(u) =

(cid:88)

v

dr(u, v) 

(k)kr =

n/2

(cid:88)

k=1

k1r  1.

n/2

(cid:88)

k=1

295

Destination

Figure 8.16: For r < 2, on a short path you are unlikely to encounter a long-distance

edge that takes you close to the destination.

r > 2 The lengths of long distance edges tend to be short so the

probability of encountering a suciently long, long-distance edge is

too low.

r = 2 Selecting the edge with end point closest to the destina-

tion nds a short path.

r < 2 The ends of long distance edges tend to be uniformly dis-

tributed.

Short paths exist but a polylog length path is unlikely

to encounter a long distance edge whose end point is close to the

destination.

Figure 8.17: Eects of dierent values of r on the expected length of long-distance edges

and the ability to nd short paths.

296

No matter how large r is the rst term of (cid:80)n/2

k=1 k1r is at least one.

For r = 2 the normalizing constant cr(u) is upper bounded by

cr(u) =

(cid:88)

v

dr(u, v) 

2n

(cid:88)

(4k)k2  4

k=1

2n

(cid:88)

k=1

1

k

= (ln n).

For r < 2, the normalizing constant cr(u) is lower bounded by

cr(u) =

(cid:88)

v

dr(u, v) 

n/2

(cid:88)

(k)kr 

n/2

(cid:88)

k1r.

k=1

k=n/4

The summation

n/2

(cid:80)

k=n/4

k1r has n

4 terms, the smallest of which is (cid:0) n

4

(cid:1)1r or (cid:0) n

2

(cid:1)1r depending

on whether r is greater or less than one. This gives the following lower bound on cr(u).

cr(u) 

n

4

(n1r) = (n2r).

No short paths exist for the r > 2 case.

For r > 2, we rst show that for at least half of the pairs of vertices, there is no short

path between them. We begin by showing that the expected number of edges of length

greater than n r+2

2r goes to zero. The probability of an edge from u to v is dr(u, v)/cr(u)

where cr(u) is lower bounded by a constant. The probability that a particular edge of

2 ) for some

length greater than or equal to n r+2

constant c. Since there are n2 long edges, the expected number of edges of length at least

n r+2

2 , which for r > 2 goes to zero. Thus, by the rst

moment method, almost surely, there are no such edges.

is chosen is upper bounded by cn( r+2

is at most cn2n (r+2)

or cn 2r

2r

2r

2

For at least half of the pairs of vertices, the grid distance, measured by grid edges

between the vertices, is greater than or equal to n/4. Any path between them must have

4n r2

4n/n r+2

at least 1

2r and so there is

no polylog length path.

2r edges since there are no edges longer than n r+2

2r = 1

An algorithm for the r = 2 case

For r = 2, the local algorithm that selects the edge that ends closest to the destination

t nds a path of expected length O(ln n)3. Suppose the algorithm is at a vertex u which

is at distance k from t. Then within an expected O(ln n)2 steps, the algorithm reaches a

point at distance at most k/2. The reason is that there are (k2) vertices at distance at

most k/2 from t. Each of these vertices is at distance at most k + k/2 = O(k) from u. See

Figure 8.18. Recall that the normalizing constant cr is upper bounded by O(ln n), and

297

< 3k/2

k/2

u

k

t

(k2) vertices at

distance k/2 from t

Figure 8.18: Small worlds.

hence, the constant of proportionality is lower bounded by some constant times 1/ ln n.

The probability that the long-distance edge from u goes to one of these vertices is at least

(k2k2/ ln n) = (1/ ln n).

Consider (ln n)2 steps of the path from u. The long-distance edges from the points

visited at these steps are chosen independently and each has probability (1/ ln n) of

reaching within k/2 of t. The probability that none of them does is

(cid:0)1  (1/ ln n)(cid:1)c(ln n)2

= c1e ln n =

c1

n

for a suitable choice of constants. Thus, the distance to t is halved every O(ln n)2 steps

and the algorithm reaches t in an expected O(ln n)3 steps.

A local algorithm cannot nd short paths for the r < 2 case

For r < 2 no local polylog time algorithm exists for nding a short path. To illustrate

the proof, we rst give the proof for the special case r = 0, and then give the proof for

r < 2.

When r = 0, all vertices are equally likely to be the end point of a long-distance edge.

Thus, the probability of a long-distance edge hitting one of the n vertices that are within

distance

n, the probability that

the path does not encounter such an edge is (1  1/n)

n of the destination is 1/n. Along a path of length

n . Now,









(cid:19)

n

(cid:18)

lim

n

1 

1

n

(cid:18)

= lim

n

1 

(cid:19)n 1

n

1

n

= lim

n

e 1

n = 1.

298



Since with probability 1/2 the starting point is at distance at least n/4 from the desti-

n steps, the path will not encounter a long-distance edge ending within

nation and in

n of the destination, for at least half of the starting points the path length will

distance



n and hence not in polylog time.

be at least

n. Thus, the expected time is at least 1

2





For the general r < 2 case, we show that a local algorithm cannot nd paths of length

O(n(2r)/4). Let  = (2  r)/4 and suppose the algorithm nds a path with at most n

edges. There must be a long-distance edge on the path which terminates within distance

n of t; otherwise, the path would end in n grid edges and would be too long. There are

O(n2) vertices within distance n of t and the probability that the long-distance edge from

(cid:1) = n(r2)/2. To

one vertex of the path ends at one of these vertices is at most n2 (cid:0) 1

n2r

see this, recall that the lower bound on the normalizing constant is (n2r) and hence an

upper bound on the probability of a long-distance edge hitting v is  (cid:0) 1

(cid:1) independent

n2r

of where v is. Thus, the probability that the long-distance edge from one of the n vertices

n2r = n r2

on the path hits any one of the n2 vertices within distance n of t is n2

2 .

The probability that this happens for any one of the n vertices on the path is at most

n r2

4 = n(r2)/4 = o(1) as claimed.

2 n = n r2

2 n 2r

1

Short paths exist for r < 2

Finally we show for r < 2 that there are O(ln n) length paths between s and t. The

proof is similar to the proof of Theorem 8.13 showing O(ln n) diameter for G(n, p) when

p is (ln n/n), so we do not give all the details here. We give the proof only for the case

when r = 0.



For a particular vertex v, let Si denote the set of vertices at distance i from v. Using

ln n), then |Si| is (ln n). For later i, we argue a constant

only local edges, if i is O(

factor growth in the size of Si as in Theorem 8.13. As long as |S1|+|S2|+  +|Si|  n2/2,

for each of the n2/2 or more vertices outside, the probability that the vertex is not in

Si+1 is (1  1

2n2 since the long-distance edge from each vertex of Si chooses

a long-distance neighbor at random. So, the expected size of Si+1 is at least |Si|/4 and

using Cherno, we get constant factor growth up to n2/2. Thus, for any two vertices v

and w, the number of vertices at distance O(ln n) from each is at least n2/2. Any two

sets of cardinality at least n2/2 must intersect giving us a O(ln n) length path from v to

w.

n2 )|Si|  1  |Si|

8.11 Bibliographic Notes

The G(n, p) random graph model is from Erdos Renyi [ER60]. Among the books

written on properties of random graphs a reader may wish to consult Frieze and Karonski

[FK15], Jansen, Luczak and Rucinski [JLR00],or Bollobas [Bol01]. Material on phase

transitions can be found in [BT87]. The argument for existence of a giant component is

299

from Krivelevich and Sudakov [KS13]. For additional material on the giant component

consult [Kar90] or [JKLP93].

For further description of ideas used in practical CNF-SAT solvers, see [GKSS08]. A

discussion of solvers used in the 2015 SAT Race appears in [BBIS16]. The work on phase

transitions for CNF was started by Chao and Franco [CF86]. Further work was done in

[FS96], [AP03], [Fri99], and others. The proof here that the SC algorithm produces a

solution when the number of clauses is cn for c < 2

3 is from [Chv92].

Material on branching process can be found in [AN72]. The phase transition for giant

components in random graphs with given degree distributions is from Molloy and Reed

[MR95a].

There are numerous papers on growth models. The material in this chapter was based

primarily on [CHK+01] and [BA]. The material on small world is based on Kleinberg,

[Kle00] which follows earlier work by Watts and Strogatz [WS98].

300

8.12 Exercises

Exercise 8.1 Search the World Wide Web to nd some real world graphs in machine

readable form or data bases that could automatically be converted to graphs.

1. Plot the degree distribution of each graph.

2. Count the number of connected components of each size in each graph.

3. Count the number of cycles in each graph.

4. Describe what you nd.

5. What is the average vertex degree in each graph? If the graph were a G(n, p) graph,

what would the value of p be?

6. Spot dierences between your graphs and G(n, p) for p from Item 5. Look at sizes

of connected components, cycles, size of giant component.

Exercise 8.2 In G(n, p) the probability of a vertex having degree k is (cid:0)n

k

(cid:1)pk(1  p)nk.

1. Show by direct calculation that the expected degree is np.

2. Compute directly the variance of the degree distribution.

3. Where is the mode of the binomial distribution for a given value of p? The mode is

the point at which the probability is maximum.

Exercise 8.3

1. Plot the degree distribution for G(1000, 0.003).

2. Plot the degree distribution for G(1000, 0.030).

Exercise 8.4 To better understand the binomial distribution plot (cid:0)n

(cid:1)pk(1  p)nk as a

function of k for n = 50 and k = 0.05, 0.5, 0.95. For each value of p check the sum over

all k to ensure that the sum is one.

k

(cid:1) , argue that with high probability there is no vertex of degree

Exercise 8.5 In G (cid:0)n, 1

greater than 6 log n

log log n (i.e.,the probability that such a vertex exists goes to zero as n goes

to innity). You may use the Poisson approximation and may wish to use the fact that

k!  ( k

n

e )k.

Exercise 8.6 The example of Section 8.1.1 showed that if the degrees in G(n, 1

n) were

independent there would almost surely be a vertex of degree (log n/ log log n). However,

the degrees are not independent. Show how to overcome this diculty.

301

Exercise 8.7 Let f (n) be a function that is asymptotically less than n. Some such func-

tions are 1/n, a constant d, log n or n

1

3 . Show that

(cid:16)

1 + f (n)

n

(cid:17)n

(cid:39) ef (n)(1o(1)).

(cid:17)n(cid:105)

(cid:104)(cid:16)

ln

1 + f (n)

n

f (n)

lim

n

= 1.

for large n. That is

Exercise 8.8

1. In the limit as n goes to innity, how does (cid:0)1  1

n

(cid:1)n ln n behave.

2. What is lim

n

(cid:0) n+1

n

(cid:1)n?

Exercise 8.9 Consider a random permutation of the integers 1 to n. The integer i is

said to be a xed point of the permutation if i is the integer in the ith position of the

permutation. Use indicator variables to determine the expected number of xed points in

a random permutation.

Exercise 8.10 Generate a graph G (cid:0)n, d

number of triangles in each graph. Try the experiment with n=100.

(cid:1) with n = 1000 and d=2, 3, and 6. Count the

n

Exercise 8.11 What is the expected number of squares (4-cycles) in G (cid:0)n, d

the expected number of 4-cliques in G (cid:0)n, d

(cid:0)4

2

(cid:1)? What is

(cid:1)? A 4-clique consists of four vertices with all

(cid:1) edges present.

n

n

Exercise 8.12 Carry out an argument, similar to the one used for triangles, to show that

p = 1

n2/3 is a threshold for the existence of a 4-clique. A 4-clique consists of four vertices

with all (cid:0)4

(cid:1) edges present.

2

n, and

Exercise 8.13 What is the expected number of simple paths of length 3, log n,

n  1 in G(n, d

n )? A simple path is a path where no vertex appears twice as in a cycle.

The expected number of simple paths of a given length being innite does not imply that a

graph selected at random has such a path.



Exercise 8.14 Let x be an integer chosen uniformly at random from {1, 2, . . . , n}. Count

the number of distinct prime factors of n. The exercise is to show that the number of prime

factors almost surely is (ln ln n). Let p stand for a prime number between 2 and n.

1. For each xed prime p, let Ip be the indicator function of the event that p divides x.

Show that E(Ip) = 1

p + O (cid:0) 1

n

(cid:1).

302

2. The random variable of interest, y = (cid:80)

Ip, is the number of prime divisors of x

p

picked at random. Show that the variance of y is O(ln ln n). For this, assume the

known result that the number of primes p between 2 and n is O(n/ ln n) and that

(cid:80)

1

p = ln ln n. To bound the variance of y, think of what E(IpIq) is for p (cid:54)= q, both

p

primes.

3. Use (1) and (2) to prove that the number of prime factors is almost surely (ln ln n).

(cid:1). I.e., in

Exercise 8.15 Suppose one hides a clique of size k in a random graph G (cid:0)n, 1

the random graph, choose some subset S of k vertices and put in the missing edges to make

S a clique. Presented with the modied graph, the goal is to nd S. The larger S is, the

n ln n, then with high probability

easier it should be to nd. In fact, if k is more than c

the clique leaves a telltale sign identifying S as the k vertices of largest degree. Prove this

statement by appealing to Theorem 8.1. It remains a puzzling open problem to nd such

hidden cliques when k is smaller, say, O(n1/3).



2

Exercise 8.16 The clique problem in a graph is to nd the maximal size clique. This

problem is known to be NP-hard and so a polynomial time algorithm is thought unlikely.

(cid:1)

We can ask the corresponding question about random graphs. For example, in G (cid:0)n, 1

there almost surely is a clique of size (2  ) log n for any  > 0. But it is not known how

to nd one in polynomial time.

2

1. Show that in G(n, 1

2) there almost surely are no cliques of size greater than or equal

to 2 log2 n.

2. Use the second moment method to show that in G(n, 1

2), almost surely there are

cliques of size (2  ) log2 n.

3. Show that for any  > 0, a clique of size (2  ) log n can be found in G (cid:0)n, 1

(cid:1) in

2

time nO(ln n) if one exists.

4. Give an O (n2) algorithm that nds a clique of size  (log n) in G(n, 1

probability. Hint: use a greedy algorithm. Apply your algorithm to G (cid:0)1000, 1

What size clique do you nd?

2

2) with high

(cid:1).

5. An independent set in a graph is a set of vertices such that no two of them are

connected by an edge. Give a polynomial time algorithm for nding an independent

set in G (cid:0)n, 1

(cid:1)of size  (log n) with high probability.

2

Exercise 8.17 Suppose H is a xed graph on cn vertices with 1

4c2(log n)2 edges. Show

that if c  2, with high probability, H does not occur as a vertex-induced subgraph of

G(n, 1/4). In other words, there is no subset of cn vertices of G such that the graph G

restricted to these vertices is isomorphic to H. Or, equivalently, for any subset S of cn

vertices of G and any 1-1 mapping f between these vertices and the vertices of H, there is

either an edge (i, j) within S such that the edge (f (i), f (j)) does not exist in H or there

is a non-edge i, j in S such that (f (i), f (j)) does exist in H.

303

Exercise 8.18 Given two instances, G1 and G2 of G(n, 1

2), consider the size of the largest

vertex-induced subgraph common to both G1 and G2. In other words, consider the largest

k such that for some subset S1 of k vertices of G1 and some subset S2 of k vertices of G2,

the graph G1 restricted to S1 is isomorphic to the graph G2 restricted to S2. Prove that

with high probability, k < 4 log2 n.

Exercise 8.19 (Birthday problem) What is the number of integers that must be drawn

with replacement from a set of n integers so that some integer, almost surely, will be

selected twice?

Exercise 8.20 Suppose you have an algorithm for nding communities in a social net-

work. Assume that the way the algorithm works is that given the graph G for the social

network, it nds a subset of vertices satisfying a desired property P. The specics of prop-

erty P are unimportant for this question. If there are multiple subsets S of vertices that

satisfy property P , assume that the algorithm nds one such set S at random.

In running the algorithm you nd thousands of communities and wonder how many

communities there are in the graph. Finally, when you nd the 10, 000th community, it is

a duplicate. It is the same community as one found earlier. Use the birthday problem to

derive an estimate of the total number of communities in G.

Exercise 8.21 Do a breadth rst search in G(n, d

n ) with d > 1 starting from some vertex.

The number of discovered vertices, zi, after i steps has distribution Binomial(n, pi) where

(cid:1)i . If the connected component containing the start vertex has i vertices,

pi = 1  (cid:0)1  d

then zi = i. Show that as n   (and d is a xed constant), Prob(zi = i) is o(1/n) unless

i  c1 ln n or i  c2n for some constants c1, c2.

n

Exercise 8.22 For f (x) = 1  edx  x, what is the value of xmax = arg max f (x)? What

is the value of f (xmax)? Recall from the text that in a breadth rst search of G(n, d

n), f (x)

is the expected normalized size of the frontier (size of frontier divided by n) at normalized

time x (x = t/n). Where does the maximum expected value of the frontier of a breadth

search in G(n, d

n ) occur as a function of n?

Exercise 8.23 Generate a random graph on 50 vertices by starting with the empty graph

and then adding random edges one at a time. How many edges do you need to add until

cycles rst appear (repeat the experiment a few times and take the average)? How many

edges do you need to add until the graph becomes connected (repeat the experiment a few

times and take the average)?

Exercise 8.24 Consider G(n, p) with p = 1

3n.

1. Use the second moment method to show that with high probability there exists a

simple path of length 10. In a simple path no vertex appears twice.

2. Argue that on the other hand, it is unlikely there exists any cycle of length 10.

304

Exercise 8.25 Complete the second moment argument of Theorem 8.9 to show that for

p = d

n, d > 1, G(n, p) almost surely has a cycle. Hint: If two cycles share one or more

edges, then the union of the two cycles is at least one greater than the union of the vertices.

Exercise 8.26 What is the expected number of isolated vertices in G(n, p) for p = 1

2

as a function of n?

ln n

n

Exercise 8.27 Theorem 8.13 shows that for some c > 0 and p = c ln n/n, G(n, p) has

diameter O (ln n). Tighten the argument to pin down as low a value as possible for c.

Exercise 8.28 What is diameter of G(n,p) for various values of p? Remember that the

graph becomes fully connected at ln n

n and has diameter two at

2 /lnn

n .



Exercise 8.29

1. List ve increasing properties of G (n, p).

2. List ve non increasing properties .

Exercise 8.30 If y and z are independent, nonnegative, integer valued random variables,

then the generating function of the sum y + z is the product of the generating function of

y and z. Show that this follows from E(xy+z) = E(xyxz) = E(xy)E(xz).

Exercise 8.31 Let fj(x) be the jth iterate of the generating function f (x) of a branch-

ing process. When m > 1, limjfj(x) = q for 0  x < 1. In the limit this implies

Prob (zj = 0) = q and Prob (zj = i) = 0 for all nonzero nite values of i. Shouldnt the

probabilities add up to 1? Why is this not a contradiction?

Exercise 8.32 Try to create a probability distribution for a branching process which

varies with the current population in which future generations neither die out, nor grow

to innity.

Exercise 8.33 Consider generating the edges of a random graph by ipping two coins,

one with probability p1 of heads and the other with probability p2 of heads. Add the edge

to the graph if either coin comes down heads. What is the value of p for the generated

G(n, p) graph?

Exercise 8.34 In the proof of Theorem 8.15 that every increasing property has a thresh-

p0(n)

p(n) = 0 that G(n, p0) almost surely did not have

old, we proved for p0(n) such that lim

n

property Q. Give the symmetric argument that for any p1(n) such that

p1(n) = 0,

G(n, p1) almost surely has property Q.

lim

n

p(n)

Exercise 8.35 Consider a model of a random subset N (n, p) of integers {1, 2, . . . n} de-

ned by independently at random including each of {1, 2, . . . n} into the set with probability

p. Dene what an increasing property of N (n, p) means. Prove that every increasing

property of N (n, p) has a threshold.

305

Exercise 8.36 N (n, p) is a model of a random subset of integers {1, 2, . . . n} dened by

independently at random including each of {1, 2, . . . n} into the set with probability p.

What is the threshold for N (n, p) to contain

1. a perfect square,

2. a perfect cube,

3. an even number,

4. three numbers such that x + y = z ?

Exercise 8.37 Explain why the property that N (n, p) contains the integer 1 has a thresh-

old. What is the threshold?

Exercise 8.38 The Sudoku game consists of a 9  9 array of squares. The array is

partitioned into nine 3  3 squares. Each small square should be lled with an integer

between 1 and 9 so that each row, each column, and each 3  3 square contains exactly

one copy of each integer. Initially the board has some of the small squares lled in in such

a way that there is exactly one way to complete the assignments of integers to squares.

Some simple rules can be developed to ll in the remaining squares such as if a row does

not contain a given integer and if every column except one in which the square in the row

is blank contains the integer, then place the integer in the remaining blank square in the

row. Explore phase transitions for the Sudoku game. Some possibilities are:

1. Start with a 9  9 array of squares with each square containing a number between

1 and 9 such that no row, column, or 3  3 square has two copies of any integer.

Develop a set of simple rules for lling in squares such as if a row does not contain

a given integer and if every column except one in which the square in the row is

blank contains the integer, then place the integer in the remaining blank entry in the

row. How many integers can you randomly erase and your rules will still completely

ll in the board?

2. Generalize the Sudoku game for arrays of size n2  n2. Develop a simple set of

rules for completing the game. Start with a legitimate completed array and erase k

entries at random. Experimentally determine the threshold for the integer k such

that if only k entries of the array are erased, your set of rules will nd a solution?

Exercise 8.39 In a square n  n grid, each of the O(n2) edges is randomly chosen to

be present with probability p and absent with probability 1  p. Consider the increasing

property that there is a path from the bottom left corner to the top right corner which

always goes to the right or up. Show that p = 1/2 is a threshold for the property. Is it a

sharp threshold?

Exercise 8.40 The threshold property seems to be related to uniform distributions. What

if we considered other distributions? Consider a model where i is selected from the set

{1, 2, . . . , n} with probability proportional to 1

i . Is there a threshold for perfect squares?

Is there a threshold for arithmetic progressions?

306

Exercise 8.41 Modify the proof that every increasing property of G(n, p) has a threshold

to apply to the 3-CNF satisability problem.

Exercise 8.42 Evaluate (cid:0)1  1

2k

(cid:1)2k

for k=3, 5, and 7. How close is it to 1/e?

Exercise 8.43 For a random 3-CNF formula with n variables and cn clauses for some

constant c, what is the expected number of satisfying assignments?

Exercise 8.44 Which of the following variants of the SC algorithm admit a theorem like

Theorem 8.20?

1. Among all clauses of least length, pick the rst one in the order in which they appear

in the formula.

2. Set the literal appearing in most clauses independent of length to 1.

Exercise 8.45 Suppose we have a queue of jobs serviced by one server. There is a total

of n jobs in the system. At time t, each remaining job independently decides to join the

queue to be serviced with probability p = d/n, where d < 1 is a constant. Each job has a

processing time of 1 and at each time the server services one job, if the queue is nonempty.

Show that with high probability, no job waits more than (ln n) time to be serviced once

it joins the queue.

Exercise 8.46 Consider G (n, p). Show that there is a threshold (not necessarily sharp)

for 2-colorability at p = 1/n. In particular, rst show that for p = d/n with d < 1, with

high probability G(n, p) is acyclic, so it is bipartite and hence 2-colorable. Next, when

pn  , the expected number of triangles goes to innity. Show that in that case, there

is a triangle almost surely and therefore almost surely the graph is not 2-colorable.

Exercise 8.47 A vertex cover of size k for a graph is a set of k vertices such that one end

of each edge is in the set. Experimentally play with the following problem. For G(20, 1

2),

for what value of k is there a vertex cover of size k?

Exercise 8.48 Construct an example of a formula which is satisable, but the SC heuris-

tic fails to nd a satisfying assignment.

Exercise 8.49 In G(n, p), let xk be the number of connected components of size k. Using

xk, write down the probability that a randomly chosen vertex is in a connected component

of size k. Also write down the expected size of the connected component containing a

randomly chosen vertex.

Exercise 8.50 Describe several methods of generating a random graph with a given degree

distribution. Describe dierences in the graphs generated by the dierent methods.

307

Exercise 8.51 Consider generating a random graph adding one edge at a time. Let n(i,t)

be the number of components of size i at time t.

n(1, 1) = n

n(1, t) = 0

t > 1

n(i, t) = n(i, t  1) +

(cid:88) j(i  j)

n2

n (j, t  1) n (i  j, t  1) 

2i

n

n (i)

Compute n(i,t) for a number of values of i and t. What is the behavior? What is the

sum of n(i,t) for xed t and all i? Can you write a generating function for n(i,t)?

Exercise 8.52 The global clustering coecient of a graph is dened as follows. Let dv be

the degree of vertex v and let ev be the number of edges connecting pairs of vertices that

are adjacent to vertex v. The global clustering coecient c is given by

(cid:88)

c =

2ev

dv(dv1).

v

In a social network, for example, it measures what fraction of pairs of friends of each

person are themselves friends. If many are, the clustering coecient is high. What is c

for a random graph with p = d

n in the limit as n goes to innity? For a denser graph?

Compare this value to that for some social network.

Exercise 8.53 Consider a structured graph, such as a grid or cycle, and gradually add

edges or reroute edges at random. Let L be the average distance between all pairs of

vertices in a graph and let C be the ratio of triangles to connected sets of three vertices.

Plot L and C as a function of the randomness introduced.

Exercise 8.54 Consider an n  n grid in the plane.

1. Prove that for any vertex u, there are at least k vertices at distance k for 1  k 

n/2.

2. Prove that for any vertex u, there are at most 4k vertices at distance k.

3. Prove that for one half of the pairs of points, the distance between them is at least

n/4.

Exercise 8.55 Recall the denition of a small-world graph in Section 8.10. Show that

in a small-world graph with r  2, that there exist short paths with high probability. The

proof for r = 0 is in the text.

Exercise 8.56 Change the small worlds graph as follows. Start with a n  n grid where

each vertex has one long-distance edge to a vertex chosen uniformly at random. These are

exactly like the long-distance edges for r = 0. Instead of having grid edges, we have some

other graph with the property that for each vertex, there are (t2) vertices at distance t

from the vertex for t  n. Show that, almost surely, the diameter is O(ln n).

308

Exercise 8.57 Consider an n-node directed graph with two random out-edges from each

node. For two vertices s and t chosen at random, prove that with high probability there

exists a path of length at most O(ln n) from s to t.

Exercise 8.58 Explore the concept of small world by experimentally determining the an-

swers to the following questions:

1. How many edges are needed to disconnect a small world graph? By disconnect we

mean at least two pieces each of reasonable size. Is this connected to the emergence

of a giant component?

2. How does the diameter of a graph consisting of a cycle change as one adds a few

random long-distance edges?

Exercise 8.59 In the small world model with r < 2, would it help if the algorithm could

look at edges at any node at a cost of one for each node looked at?

Exercise 8.60 Make a list of the ten most interesting things you learned about random

graphs.

309

9 Topic Models, Nonnegative Matrix Factorization,

Hidden Markov Models, and Graphical Models

In the chapter on machine learning, we saw many algorithms for tting functions to

data. For example, suppose we want to learn a rule to distinguish spam from nonspam

email and we were able to represent email messages as points in Rd such that the two

categories are linearly separable. Then, we could run the Perceptron algorithm to nd a

linear separator that correctly partitions our training data. Furthermore, we could argue

that if our training sample was large enough, then with high probability, this translates

to high accuracy on future data coming from the same probability distribution. An inter-

esting point to note here is that these algorithms did not aim to explicitly learn a model

of the distribution D+ of spam emails or the distribution D of nonspam emails. Instead,

they aimed to learn a separator to distinguish spam from nonspam. In this chapter, we

look at algorithms that, in contrast, aim to explicitly learn a probabilistic model of the

process used to generate the observed data. This is a more challenging problem, and

typically requires making additional assumptions about the generative process. For ex-

ample, in the chapter on high-dimensional space, we assumed data came from a Gaussian

distribution and we learned the parameters of the distribution. In the chapter on SVD,

we considered the more challenging case that data comes from a mixture of k Gaussian

distributions. For k = 2, this is similar to the spam detection problem, but harder in that

we are not told which training emails are spam and which are nonspam, but easier in that

we assume D+ and D are Gaussian distributions. In this chapter, we examine other

important model-tting problems, where we assume a specic type of process is used to

generate data, and then aim to learn the parameters of this process from observations.

9.1 Topic Models

Topic Modeling is the problem of tting a certain type of stochastic model to a given

collection of documents. The model assumes there exist r topics, that each document is

a mixture of these topics, and that the topic mixture of a given document determines the

probabilities of dierent words appearing in the document. For a collection of news arti-

cles, the topics may be politics, sports, science, etc. A topic is a set of word frequencies.

For the topic of politics, words like president and election may have high frequencies,

whereas for the topic of sports, words like pitcher and goal may have high frequencies.

A document (news item) may be 60% politics and 40% sports. In that case, the word

frequencies in the document are assumed to be convex combinations of word frequencies

for each of these topics with weights 0.6 and 0.4 respectively.

Each document is viewed as a bag of words or terms.37. Namely, we disregard the

order and context in which each word occurs in the document and instead only list the

frequency of occurrences of each word. Frequency is the number of occurrences of the

37In practice, terms are typically words or phrases, and not all words are chosen as terms. For example,

articles and simple verbs, pronouns etc. may not be considered terms.

310

word divided by the total count of all words in the document. Throwing away context

information may seem wasteful, but this approach works fairly well in practice. Each doc-

ument is a vector with d components where d is the total number of dierent terms that

exist; each component of the vector is the frequency of a particular term in the document.

We can represent a collection of n documents by a d  n matrix A called the term-

document matrix, with one column per document and one row per term. The topic model

hypothesizes that there exist r topics (r is typically small) such that each document is

a mixture of these topics. In particular, each document has an associated vector with r

nonnegative components summing to one, telling us the fraction of the document that is

on each of the topics. In the example above, this vector would have 0.6 in the component

for politics and 0.4 in the component for sports. These can be arranged vectors as the

columns of a r  n matrix C, called the topic-document matrix. Finally, there is a third

d  r matrix B for the topics. Each column of B is a vector corresponding to one topic;

it is the vector of expected frequencies of terms in that topic. The vector of expected

frequencies for a document is a convex combination of the expected frequencies for topics,

with the topic weights given by the vector in C for that document. In matrix notation,

let P be a n  d matrix with column P (:, j) denoting the expected frequencies of terms

in document j. Then,

P = BC.

(9.1)

Pictorially, we can represent this as:

D O C U M E N T

T O P I C

T

E

R

M





























P





























T

E

=

R

M





































































T

O

P

I

C

B

D O C U M E N T

C













Topic Models are stochastic models that generate documents according to the fre-

quency matrix P above. pij is viewed as the probability that a random term of document

j is the ith term in the dictionary. We make the assumption that terms in a document are

drawn independently. In general, B is assumed to be a xed matrix, whereas C is random.

So, the process to generate n documents, each containing m terms, is the following:

Denition 9.1 (Document Generation Process) Let D be a distribution over a mix-

ture of topics. Let B be the term-topic matrix. Create a d  n term-document matrix A

as follows:

Intialize aij = 0 for i = 1, 2, . . . , d; j = 1, 2, . . . , n.38

38We will use i to index into the set of all terms, j to index documents and l to index topics.

311

For j = 1, 2, . . . , n

 Pick column j of C from distribution D. This will be the topic mixture for

document j, and induces P (:, j) = BC(:, j).

 For t = 1, 2, . . . , m, do:

 Generate the tth term xt of document j from the multinomial distribution

over {1, 2, . . . , d} with probability vector P (:, j) i.e., Prob(xt = i) = pij.

 Add 1/m to axt,j.

The topic modeling problem is to infer B and C from A. The probability distribution

D, of the columns of C is not yet specied. The most commonly used distribution is the

Dirichlet distribution that we study in detail in Section 9.6.

Often we are given fewer terms of each document than the number of terms or the

number of documents. Even though

E(aij|P ) = pij,

(9.2)

and in expectation A equals P , the variance is high. For example, for the case when



d, A(:, j) is likely to have 1/m in a random

pij = 1/d for all i with m much less than

subset of m coordinates since no term is likely to be picked more than once. Thus

||A(:, j)  P (:, j)||1 = m

(cid:18) 1

m



(cid:19)

1

d

+ (d  m)

(cid:19)

(cid:18) 1

d

 2,

the maximum possible. This says that in l1 norm, which is the right norm when dealing

with probability vectors, the noise a,j  p,j is likely to be larger than p,j. This is one

of the reasons why the model inference problem is hard. Write

A = BC + N,

(9.3)

where, A is the d  n term-document matrix, B is a d  r term-topic matrix and C is

a r  n topic-document matrix. N stands for noise, which can have high norm. The l1

norm of each column of N could be as high as that of BC.

There are two main ways of tackling the computational diculty of nding B and C

from A. One is to make assumptions on the matrices B and C that are both realistic and

also admit ecient computation of B and C. The trade-o between these two desirable

properties is not easy to strike and we will see several approaches beginning with the

strongest assumptions on B and C in Section 9.2. The other way is to restrict N . Here

again, an idealized way would be to assume N = 0 which leads to what is called the Non-

negative Matrix Factorization (NMF) (Section 9.3) problem of factoring the given matrix

A into the product of two nonnegative matrices B and C. With a further restriction on B,

called Anchor terms, (Section 9.4), there is a polynomial time algorithm to do NMF. The

312

strong restriction of N = 0 can be relaxed (Section ??), but at the cost of computational

eciency.

The most common approach to topic modeling makes an assumption on the probabil-

ity distribution of C, namely, that the columns of C are independent Dirichlet distributed

random vectors. This is called the Latent Dirichlet Allocation model (Section 9.6), which

does not admit an ecient computational procedure. We show that the Dirichlet distri-

bution leads to many documents having a primary topic, whose weight is much larger

than average in the document. This motivates a model called the Dominant Admixture

model (Section 9.7) which admits an ecient algorithm.

On top of whatever other assumptions are made, we assume that in each document,

the m terms in it are drawn independently as in Denition 9.1. This is perhaps the biggest

assumption of all.

9.2 An Idealized Model

The Topic Model inference problem is in general computationally hard. But under

certain reasonable assumptions, it can be solved in polynomial time as we will see in this

chapter. We start here with a highly idealized model that was historically the rst for

which a polynomial time algorithm was devised. In this model, we make two assumptions:

The Pure Topic Assumption: Each document is purely on a single topic. I.e., each

column j of C has a single entry equal to 1, and the rest of the entries are 0.

Separability Assumption: The sets of terms occurring in dierent topics are disjoint.

I.e., for each row i of B, there is a unique column l with bil (cid:54)= 0.

Under these assumptions, the data matrix A has a block structure. Let Tl denote the

set of documents on topic l and Sl the set of terms occurring in topic l. After rearranging

columns and rows so that the rows in each Sl occur consecutively and the columns of each

Tl occur consecutively, the matrix A looks like:

A =









































0

0

0

0

0

0.

S1

S2

S3

T1







0

0

0

0

0

0

T O P I C

T2

0

0

0







0

0

0

0.

0

0







0

0

0

0

0

0







0

0

0

0

0

0

0

0

0













0

0

0

0

0

0

T3

0

0

0

0

0

0









































0

0

0

0

0

0







T

E

R

M

313

If we can partition the documents into r clusters, T1, T2, . . . , Tr, one for each topic,

we can take the average of each cluster and that should be a good approximation to the

corresponding column of B. It would also suce to nd the sets Sl of terms, since from

them we could read o the sets Tl of topics. We now formally state the document gen-

eration process under the Pure Topic Assumption and the associated clustering problem.

Note that under the Pure Topics Assumption, the distribution D over columns of C is

specied by the probability that we pick each topic to be the only topic of a document.

Let 1, 2, . . . , r be these probabilities.

Document Generation Process under Pure Topics Assumption:

Intialize all aij to zero.

For each document do

 Select a topic from the distribution given by {1, 2, . . . , r}.

 Select m words according to the distribution for the selected topic.

 For each selected word add 1/m to the document-term entry of the matrix A.

Denition 9.2 (Clustering Problem) Given A generated as above and the number of

topics r, partition the documents {1, 2, . . . , n} into r clusters T1, T2, . . . , Tr, each specied

by a topic.

Approximate Version: Partition the documents into r clusters, where at most n of

the j  {1, 2, . . . , n} are misclustered.

The approximate version of Denition 9.2 suces since we are taking the average of

the document vectors in each cluster j and returning the result as our approximation to

column j of B. Note that even if we clustered perfectly, the average will only approximate

the column of B. We now show how we can nd the term clusters Sl, which then can be

used to solve the Clustering Problem.

Construct a graph G on d vertices, with one vertex per term, and put an edge between

two vertices if they co-occur in any document. By the separability assumption, we know

that there are no edges between vertices belonging to dierent Sl. This means that if each

Sl is a connected component in this graph, then we will be done. Note that we need to

assume m  2 (each document has at least two words) since if all documents have just

one word, there will be no edges in the graph at all and the task is hopeless.

Let us now focus on a specic topic l and ask how many documents nl we need so that

with high probability, Sl is a connected component. One annoyance here is that some

words may have very low probability and not become connected to the rest of Sl. On the

other hand, words of low probability cant cause much harm since they are unlikely to be

the only words in a document, and so it doesnt matter that much if we fail to cluster

them. We make this argument formal here.

314

Let  < 1/3 and dene  = m. Consider a partition of Sl into two subsets of terms W

and W that each have probability mass at least  in the distribution of terms in topic l.

Suppose that for every such partition, there is at least one edge between W and W . This

would imply that the largest connected component Sl in Sl must have probability mass

at least 1  . If Sl had probability mass between  and 1   then using W = Sl would

violate the assumption about partitions with mass greater than  having an edge between

them. If the largest partition Sl had probability mass less than , then one could create a

union of connected components W that violates the assumption. Since Prob( Sl)  1  ,

the probability that a new random document of topic l contains only words not in Sl is

at most m = . Thus, if we can prove the statement about partitions, we will be able to

correctly cluster nearly all new random documents.

To prove the statement about partitions, x some partition of Sl into W and W that

each have probability mass at least . The probability that m words are all in W or W is

at most Prob(W )m + Prob(W )m. Thus the probability that none of nl documents creates

an edge between W and W is

(cid:0)Prob(W )m + Prob(W )m(cid:1)nl  (m + (1  )m)nl

 ((1  /2)m)nl

 emnl/2

where the rst inequality is due to convexity and the second is a calculation. Since there

are at most 2d dierent possible partitions of Sl into W and W , the union bound ensures

at most a  probability of failure by having

This in turn is satised for

2demnl/2  .

mnl 

(cid:18)

2



d ln 2 + ln

(cid:19)

.

1



This proves the following result.

(cid:1), then with probability at least 1  , the largest

Lemma 9.1 If nlm  2



connected component in Sl has probability mass at least 1  . This in turn implies that

the probability to fail to correctly cluster a new random document of topic l is at most

 = 1/m.

(cid:0)d ln 2 + ln 1



9.3 Nonnegative Matrix Factorization - NMF

We saw in Section 9.1, while the expected value E(A|B, C) equals BC, the variance

can be high. Write

A = BC + N,

where, N stands for noise. In topic modeling, N can be high. But it will be useful to rst

look at the problem when there is no noise. This can be thought of as the limiting case

315

as the number of words per document goes to innity.

Suppose we have the exact equations A = BC where A is the given matrix with non-

negative entries and all column sums equal to 1. Given A and the number of topics r,

can we nd B and C such that A = BC where B and C have nonnegative entries? This

is called the Nonnegative Matrix Factorization (NMF) problem and has applications be-

sides topic modeling. If B and C are allowed to have negative entries, we can use Singular

Value Decomposition on A using the top r singular vectors of A.

Before discussing NMF, we will take care of one technical issue. In topic modeling,

besides requiring B and C to be nonnegative, we have additional constraints stemming

from the fact that frequencies of terms in one particular topic are nonnegative reals

summing to one, and that the fractions of each topic that a particular document is on are

also nonnegative reals summing to one. All together, the constraints are:

1. A = BC.

2. The entries of B and C are all nonnegative.

3. Columns of both B and C sums to one.

It will suce to ensure the rst two conditions.

Lemma 9.2 Let A be a matrix with nonnegaitve elements and columns summing to one.

The problem of nding a factorization BC of A satisfying the three conditions above is

reducible to the NMF problem of nding a factorization BC satisfying conditions (1) and

(2).

Proof: Suppose we have a factorization BC that satises (1) and (2) of a matrix A whose

columns each sum to one. We can multiply the lth column of B by a positive real number

and divide the lth row of C by the same real number without violating A = BC. By doing

this, we may assume that each column of B sums to one. Now we have aij = (cid:80)

l bilclj

which implies (cid:80)

i aij = (cid:80)

i aij, is 1.

Thus the columns of C sum to one giving (3).

l clj, the sum of the jth column of C, (cid:80)

i,l bilclj = (cid:80)

Given an d  n matrix A and an integer r, the exact NMF problem is to determine

whether there exists a factorization of A into BC where B is an d  r matrix with non-

negative entries and C is r  n matrix with nonnegative entries and if so, nd such a

factorization.39

Nonnegative matrix factorization is a general problem and there are many heuristic

algorithms to solve the problem. In general, they suer from one of two problems. They

could get stuck at local optima which are not solutions or take exponential time. In fact,

39Bs columns form a basis in which As columns can be expressed as nonnegative linear combinations,

the coecients being given by matrix C.

316

the NMF problem is NP-hard. In practice, often r is much smaller than n and d. We show

rst that while the NMF problem as formulated above is a nonlinear problem in r(n + d)

unknowns (the entries of B and C), it can be reformulated as a nonlinear problem with

just 2r2 unknowns under the simple nondegeneracy assumption that A has rank r. This,

in turn, allows for an algorithm that runs in polynomial time when r is a constant.

Lemma 9.3 If A has rank r, then the NMF problem can be formulated as a problem with

2r2 unknowns. Using this, the exact NMF problem can be solved in polynomial time if r

is constant.

Proof: If A = BC, then each row of A is a linear combination of the rows of C. So the

space spanned by the rows of A is contained in the space spanned by the rows of the r  n

matrix C. The latter space has dimension at most r, while the former has dimension r by

assumption. So they must be equal. Thus every row of C must be a linear combination

of the rows of A. Choose any set of r independent rows of A to form a r  m matrix A1.

Then C = SA1 for some r  r matrix S. By analogous reasoning, if A2 is a n  r matrix

of r independent columns of A, there is a r  r matrix T such that B = A2T . Now we

can easily cast NMF in terms of unknowns S and T :

A = A2T SA1

;

(SA1)ij  0 ;

(A2T )kl  0 i, j, k, l.

It remains to solve the nonlinear problem in 2r2 variables. There is a classical algo-

rithm which solves such problems in time exponential only in r2 (polynomial in the other

parameters). In fact, there is a logical theory, called the Theory of Reals, of which this

is a special case and any problem in this theory can be solved in time exponential in the

number of variables. We do not give details here.

9.4 NMF with Anchor Terms

An important case of NMF, which can be solved eciently, is the case where there are

anchor terms. An anchor term for a topic is a term that occurs in the topic and does not

occur in any other topic. For example, the term batter may be an anchor term for the

topic baseball and election for the topic politics. Consider the case that each topic has

an anchor term. This assumption is weaker than the separability assumption of Section

9.2, which says that all terms are anchor terms.

In matrix notation, the assumption that each topic has an anchor term implies that

for each column of the term-topic matrix B, there is a row whose sole nonzero entry is in

that column.

Denition 9.3 (Anchor Term) For each l = 1, 2, . . . r, there is an index il such that

bil,l (cid:54)= 0 and l(cid:48) (cid:54)= l bil,l(cid:48) = 0 .

317

In this case, it is easy to see that each row of the topic-document matrix C has a

scalar multiple of it occurring as a row of the given term-document matrix A.





















0.3  c4

A

0.2  c2





















=





















election

batter

0

0

0

0.3

B

0

0.2

0

0



















































 .

 c1 

 c2 

 c4 

If there is a NMF of A, there is one in which no row of C is a nonnegative linear

combination of other rows of C. If some row of C is a nonnegative linear combination of

the other rows of C, then eliminate that row of C as well as the corresponding column of

B and suitably modify the other columns of B maintaining A = BC. For example, if

c5 = 4  c3 + 3  c6,

delete row 5 of C, add 4 times column 5 of B to column 3 of B, add 3 times column 5

of B to column 6 of B, and delete column 5 of B. After repeating this, each row of C is

positively independent of the other rows of C, i.e., it cannot be expressed as a nonnegative

linear combination of the other rows.

If A = BC is a NMF of A and there are rows in A that are positive linear combinations

of other rows, the rows can be remove and the corresponding rows of B remove to give a

NMF A = BC where A and B are the matrices A and B with the removed rows. Since

there are no rows in A that are linear combinations of other rows of A, B is a diagonal

matrix and the rows of A are scalar multiples of rows of C. Now set C = A and B = I

and restore the rows to B to get B such that A = BC.

To remove rows of A that are scalar multiples of previous rows in polynomial time

check if there are real numbers x1, x2, . . . xi1, xi+1, . . . xn such that

(cid:88)

j(cid:54)=i

xjaj = ai xj  0.

This is a linear program and can be solved in polynomial time. While the algorithm

runs in polynomial time, it requires solving one linear program per term. An improved

method, not presented here, solves just one linear program.

9.5 Hard and Soft Clustering

In Section 9.2, we saw that under the assumptions that each document is purely on

one topic and each term occurs in only one topic, approximately nding B was reducible

318

Figure 9.1: Geometry of Topic Modeling. The corners of the triangle are the columns

of B. The columns of A for topic 1 are represented by circles, for topic 2 by squares, and

for topic 3 by dark circles. Columns of BC (not shown) are always inside the big triangle,

but not necessarily the columns of A.

to clustering documents according to their topic. Clustering here has the usual meaning

of partitioning the set of documents into clusters. We call this hard clustering, meaning

each data point is to be assigned to a single cluster.

The more general situation is that each document has a mixture of several topics. We

may still view each topic as a cluster and each topic vector, i.e., each column of B, as

a cluster center (Figure 9.1). But now, each document belongs fractionally to several

clusters, the fractions being given by the column of C corresponding to the document. We

may then view P (:, j) = BC(:, j) as the cluster center for document j. The document

vector A(:, j) is its cluster center plus an oset or noise N (:, j).

Barring ties, each column of C has a largest entry. This entry is the primary topic

of document j in topic modeling. Identifying the primary topic of each document is a

hard clustering problem, which intuitively is a useful step in solving the soft cluster-

ing problem of nding the fraction of each cluster each data point belongs to. Soft

Clustering just refers to nding B and C so that N = A  BC is small. In this sense,

soft clustering is equivalent to NMF.

We will see in Sections 9.8 and 9.9 that doing hard clustering to identify the primary

topic and using that to solve the soft clustering problem can be carried out under some

assumptions. The primary topic of each document is used to nd the catchwords of

each topic, the important words in a weaker sense than anchor words, and then using

the catchwords to nd the term-topic matrix B and then C. But as stated earlier, the

319

general NMF problem is NP-hard. So, we make some assumptions before solving the

problem. For this, we rst look at Latent Dirichlet Allocation (LDA), which guides us

towards reasonable assumptions.

9.6 The Latent Dirichlet Allocation Model for Topic Modeling

The most widely used model for topic modeling is the Latent Dirichlet Allocation

(LDA) model. In this model, the topic weight vectors of the documents, the columns of

C, are picked independently from what is known as a Dirichlet distribution. The term-

topic matrix B is xed. It is not random. The Dirichlet distribution has a parameter 

called the concentration parameter, which is a real number in (0, 1), typically set to

1/r. For each vector v with r nonnegative components summing to one,

Prob density ( column j of C = v) =

1

g()

r

(cid:89)

l=1

v1

l

,

where, g() is the normalizing constant so that the total probability mass is one. Since

 < 1, if any vl = 0, then the probability density is innite.

Once C is generated, the Latent Dirichlet Allocation model hypothesizes that the

matrix

acts as the probability matrix for the data matrix A, namely,

P = BC

E(A|P ) = P.

Assume the model picks m terms from each document. Each trial is according to the

multinomial distribution with probability vector P (:, j); so the probability that the rst

term we pick to include in the document j is the ith term in the dictionary is pij. Then,

aij is set equal to the fraction out of m of the number of times term i occurs in document j.

The Dirichlet density favors low vl, but since the vl have to sum to one, there is at

least one component that is high. We show that if  is small, then with high probability,

the highest entry of the column is typically much larger than the average. So, in each

document, one topic, which may be thought as the primary topic of the document, gets

disproportionately high weight. To prove this, we have to work out some properties of

the Dirichlet distribution. The rst Lemma describes the marginal probability density of

each coordinate of a Dirichlet distributed random variable:

Lemma 9.4 Suppose the joint distribution of y = (y1, y2, . . . , yr) is the Dirichlet distri-

bution with concentration parameter . Then, the marginal probability density q(y) of y1

is given by

(r + 1)

()((r  1) + 1)

where,  is the Gamma function (see Appendix for the denition).

y1(1  y)(r1) ,   (0, 1],

q(y) =

320

Proof: By denition of the marginal,

q(y) =

1

g()

y1

(cid:90)

y2,y3,...,yr

y2+y3++yr=1y

(y2 y3    yr)1 dy2 dy3 . . . dyr.

Put zl = yl/(1  y). With this change of variables,

q(y) =

1

g()

y1(1  y)(r1)

(cid:18)(cid:90)

z2,z3,...,zr

z2+z3++zr=1y

(z2z3    zr)1 dz2 dz3 . . . dzr

(cid:19)

.

The quantity inside the parentheses is independent of y, so for some c we have

q(y) = cy1(1  y)(r1).

Since (cid:82) 1

0 q(y) dy = 1, we must have

c =

1

(cid:82) 1

0 y1(1  y)(r1)

=

(r + 1)

()((r  1) + 1)

.

Lemma 9.5 Suppose the joint distribution of y = (y1, y2, . . . , yr) is the Dirichlet distri-

bution with parameter   (0, 1). For   (0, 1),

Prob (y1  1  ) 

0.85 (r1)+1

(r  1) + 1

.

Hence for  = 1/r, we have Prob(y1  1  )  0.4 2/r. If also,  < 0.5, then,

Prob (Maxr

l=1yl  1  )  0.4 2.

Proof: Since  < 1, we have y1 > 1 for y < 1 and so q(y)  c(1  y)(r1), so

(cid:90) 1

1

q(y) dy 

c

(r  1) + 1

 (r1)+1.

To lower bound c, note that ()  1/ for   (0, 1). Also, (x) is an increasing function

for x  1.5, so if (r  1) + 1  1.5, then, (r + 1)  ((r  1) + 1) and in this case,

the rst assertion of the lemma follows. If (r  1) + 1  [1, 1.5], then, ((r  1) + 1)  1

and (r + 1)  min

z[1,2]

(z)  0.85, so again, the rst assertion follows.

If now,  = 1/r, then (r  1) + 1 < 2 and so  (r1)+1/((r  1) + 1)   2/2. So the

second assertion of the lemma follows easily. For the third assertion, note that yl > 1  ,

l = 1, 2, . . . , r are mutually exclusive events for  < 0.5 (since at most one yl can be

(cid:17)

greater than 1/2), so Prob

l=1 Prob(yl > 1  ) = rProb(y1 

1  )  0.4 2.

yl  1  

= (cid:80)r

max

l=1

(cid:16) r

321

For example, from the last lemma, it follows that

1. With high probabilty, a constant fraction of the documents have a primary topic of

weight at least 0.6. In expectation, the fraction of documents for which this holds

is at least 0.4(0.6)2.

2. Also with high probability, a smaller constant fraction of the documents are nearly

pure (weight at least 0.95 on a single topic). Take  = 0.05.

If the total number of documents, n, is large, there will be many nearly pure doc-

uments. Since for nearly pure documents, cl,j  0.95, BC:,j = B(:, j) + , where,

||||1  0.05.

If we could nd the nearly pure documents for a given topic l, then

the average of the A columns corresponding to these documents will be close to the aver-

age of those columns in the matrix BC (though this is not true for individual columns)

and it is intuitively clear that we would be done.

We pursue (1) and (2) in the next section, where we see that under these assumptions,

plus one more assumption, we can indeed nd B.

More generally, the concentration parameter may be dierent for dierent topics. We

then have 1, 2, . . . , r so that

Prob density ( column j of C = v) 

r

(cid:89)

l=1

vl1

l

,

The model tting problem for Latent Dirichlet Allocation given A, nd the B, the

term-topic matrix, is in general NP-hard. There are heuristics, however, which are widely

used. Latent Dirichlet Allocation is known to work well in several application areas.

9.7 The Dominant Admixture Model

In this section, we formulate a model with three key assumptions. The rst two are mo-

tivated by Latent Dirichlet Allocation, respectively by (1) and (2) of the last section. The

third assumption is also natural; it is more realistic than the anchor words assumptions

discussed earlier. This section is self-contained and no familiarity with Latent Dirichlet

Allocation is needed.

We rst recall the notation. A is a d  n data matrix with one document per column,

which is the frequency vector of the d terms in that document. m is the number of words

in each document. r is the inner dimension, i.e., B is d  r and C is r  n. We always

index topics by l and l(cid:48), terms by i, and documents by j.

We give an intuitive description of the model assumptions rst and then make formal

statements.

1. Primary Topic Each document has a primary topic. The weight of the primary

topic in the document is high and the weight of each non-primary topic is low.

322

2. Pure Document Each topic has at least one pure document that is mostly on that

topic.

3. Catchword Each topic has at least one catchword, which has high frequency in

that topic and low frequency in other topics.

In the next section, we state quantitative versions of the assumptions and show that

these assumptions suce to yield a simple polynomial time algorithm to nd the primary

topic of each document. The primary topic classication can then be used to nd B

approximately, but this requires a further assumption (4) in Section (9.9) below, which is

a robust version of the Pure Document assumption.

Lets provide some intuition for how we are able to do the primary topic classication.

By using the primary topic and catchword assumptions, we can show (quantitative version

in Claim 9.1 below) that if i is a catchword for topic l, then there is a threshold i, which

we can compute for each catchword, so that for each document j with primary topic l,

pij is above i and for each document j whose primary topic is not l, pij is substantially

below i. So, if

1. we were given P , and

2. knew a catchword for each topic and the threshold, we can nd the primary topic

of each document.

We illustrate the situation in Equation 9.4, where rows 1, 2, . . . , r of P correspond

to catchwords for topics 1, 2, . . . , r and we have rearranged columns in order of primary

topic. H stands for a high entry and L for a low entry.

























P =

H H H L L L L L L L L L

L L L H H H L L L L L L

L L L L L L H H H L L L

L L L L L L L L L H H H

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

























(9.4)

We are given A, but not P . While E(A|P ) = P , A could be far o from P . In fact,

if in column j of P , there are many entries smaller than c/m in A (since we are doing

only m multinomial trials), they could all be zeros and so are not a good approxima-

tion to the entries of P (:, j). However, if pij > c/m, for a large value c, then aij  pij.

Think of tossing a coin m times whose probability of heads is pij. If pij  c/m, then the

number of heads one gets is close to pijm. We will assume c larger than (log(nd)) for

catchwords so that for every such i and j we have pij  aij. See the formal catchwords

assumption in the next section. This addresses (1), namely, A  B, at least in these rows.

323

One can ask if this is a reasonable assumption. If m is in the hundreds, the assump-

tion is arguably reasonable. But a weaker and more reasonable assumption would be that

there is a set of catchwords, not just one, with total frequency higher than c/m. However,

here we use the stronger assumption of a single high frequency catchword.

(2) is more dicult to address. Let l(i) = arg maxr

l(cid:48)=1 bil(cid:48). Let Tl be the set of j with

primary topic l. Whether or not i is a catchword, the primary topic assumption will imply

that pij does not drop by more than a certain factor  among j  Tl(i). We prove this

formally in Claim 9.1 of Section 9.8. That claim also proves that if i is a catchword for

topic l, that there is a sharp drop in pij between j  Tl and j / Tl.

But for noncatchwords, there is no guarantee of a sharp fall in pij between j  Tl(i)

and j / Tl(i). However, we can identify for each i, where the rst fall of roughly  factor

from the maximum occurs in row i of A. For catchwords, we show below (9.8) that this

happens precisely between Tl(i) and [n] \ Tl(i). For noncatchwords, we show that the fall

does not occur among j  Tl(i). So, the minimal sets where the fall occurs are the Tl and

we use this to identify them. We call this process Pruning.

9.8 Formal Assumptions

Parameters , ,  and  are real numbers in (0, 0.4] satisfying

 +   (1  3).

(1) Primary Topic There is a partition of [n] into T1, T2, . . . , Tk with:

(cid:40)

 

 .

clj

for j  Tl

for j / Tl.

.

(2) Pure Document For each l, there is some j with

clj  1  .

(3) Catchwords For each l, there is at least one catchword i satisfying:

bil(cid:48)  bil

bil   , where,  =

c log(10nd/)

m22

, c constant.

for l(cid:48) (cid:54)= l

Let

l(i) = arg

r

max

l(cid:48)=1

bil(cid:48).

(9.5)

(9.6)

(9.7)

(9.8)

(9.9)

Another way of stating the assumption bil   is that the expected number of times term

i occurs in topic l among m independent trials is at least c log(10nd/)/22 which grows

only logarithmically in n and d. As stated at the end of the last section, the point of

requiring bil   for catchwords is so that using the Hoeding-Cherno inequality, we can

assert that aij  pij. We state the Hoeding-Cherno inequality in the form we use it:

324

Lemma 9.6

Prob(|aij  pij|  Max(pij, )/4) 



10nd

.

So, with probability at least 1  (/10),

|aij  pij|  Max(, pij)/4 i, j

simultaneously. After paying the failure probability of /10, we henceforth assume that

the above holds.

Proof: Since aij is the average of m independent Bernoulli trials, each with expectation

pij, the Hoeding-Cherno inequality asserts that

Prob (|aij  pij|  )  2 exp

cmMin

(cid:18)

(cid:19)(cid:19)

, 

.

(cid:18)2

pij

Plugging in  = Max(pij, )/4, the rst statement of the lemma follows with some

calculation. The second statement is proved by a union bound over the nd possible (i, j)

values.

Algorithm

1. Compute Thresholds: i = (1  ) max

j

aij.

2. Do thresholding: Dene a matrix A by

aij =

(cid:40)

1

0

if aij  i and i   (cid:0)1  5

otherwise .

2

(cid:1) .

.

3. Pruning: Let Ri = {j|aij = 1}. If any Ri strictly contains another, set all

entries of row i of A to zero.

Theorem 9.7 For i = 1, 2, . . . , d, let Ri = {j|aij = 1} at the end of the algorithm. Then,

each nonempty Ri = Tl(i), with l(i) as in (9.9).

Proof: We start with a lemma which proves the theorem for catchwords. This is the

bulk of the work in the proof of the theorem.

Lemma 9.8 If i is a catchword for topic l, then Ri = Tl.

Proof: Assume throughout this proof that i is a catchword for topic l. The proof consists

of three claims. The rst argues that for j  Tl, pij is high and for j / Tl, pij is low. The

second claim argues the same for aij instead of pij. It follows from the Hoeding-Cherno

inequality since aij is just the average of m Bernoulli trials, each with probability pij.

The third claim shows that the threshold computed in the rst step of the algorithm falls

between the high and the low.

325

Claim 9.1 For i, a catchword for topic l,

bil  pij  bil

pij  bil(1  3)

for j  Tl

for j / Tl.

Proof: For j  Tl, using (9.6)

pij =

r

(cid:88)

l(cid:48)=1

bil(cid:48)cl(cid:48),j  [bil, bil]

since bil = max

l(cid:48)

bil(cid:48). For j / Tl,

pij = bilclj +

(cid:88)

l(cid:48)(cid:54)=l

bil(cid:48)cl(cid:48)j  bilclj + bil(1  clj)  bil( + )  bil(1  3),

(9.10)

where, the rst inequality is from (9.7) and the second inequality is because subject to the

constraint clj   imposed by the Primary Topic Assumption (9.6), bilclj + bil(1  clj) is

maximized when clj = . We have also used (9.5).

Claim 9.2 With probability at least 1  /10, for every l and every catchword i of l:

(cid:40)

 bil(1  /4)

 bil(1  (11/4)),

aij

for j  Tl

for j / Tl

Proof: Suppose for some j  Tl, aij < bil(1  /4). Then, since pij  bil by Claim

(9.1), |aij  pij|  bil/4 by Claim (9.1). Since i is a catchword, bil   and so

|aij  pij|  bil/4  (Max(pij, )/4) and we get the rst inequality of the current

claim using Lemma 9.6.

For the second inequality: for j / Tl, pij  bil(1  3) by Claim 9.1 and so if this

inequality is violated, |aij  pij|  bil/4 and we get a contradiction to Lemma (9.6).

Claim 9.3 With probability at least 1, for every topic l and every catchword i of topic l,

the i computed in step 1 of the algorithm satises: i  (cid:0)(1(5/2))bil , bil(1/2)(cid:1).

Proof: If i is a catchword for topic l and j0 a pure document for l, then

pij0 =

k

(cid:88)

l(cid:48)=1

bil(cid:48)cl(cid:48)j0  bilclj0  (1  )bil.

Applying Lemma 9.6, aij0 > (1  (3/2))bil. Thus, i computed in step 1 of the algorithm

satises i > (1  (3/2))(1  )bil  (1  (5/2))bil. Hence, aij is not set to zero for

all j. Now, since pij  bil for all j, aij  (1 + /4)bil by Lemma 9.6 implying

i = Maxjaij(1  )  bil(1 + (/4))(1  )  bil(1  /2).

326

Claims 9.2 and 9.3, complete the proof of Lemma 9.8.

The lemma proves Theorem 9.7 for catchwords. Note that since each topic has at least

one catchword, for each l, there is some i with Ri = Tl.

Suppose i is a noncatchword. Let a = maxj aij. If a < (1  (5/2)) , then i <

(1  (5/2)) and the entire row of A will be set to all zeros by the algorithm, so Ri = 

and there is nothing to prove. Assume that a  (1  (5/2)). Let j0 = arg maxj aij.

Then a = aij0  (1  (5/2)). We claim pi,j0  a(1  /2). If not, pij0 < a(1  /2) and

|aij0  pij0| > max

(cid:18) pij0

4

,



4

(cid:19)

,

which contradicts Lemma 9.6. So,

bil  pij0  (1  3).

(9.11)

Let l = l(i). Then

a(1  /2)  pij0 =

r

(cid:88)

l(cid:48)=1

bil(cid:48)cl(cid:48)j0  bil.

Also, if j1 is a pure document for topic l, cl,j1  (1  ) so, pi,j1  bilcl,j1  bil(1  ).

Now, we claim that

ai,j1  bil(1  (3/2)).

(9.12)

If not,

pij1  aij1 > bil(1  )  bil(1  (3/2)) = bil(/2)  max

(cid:18) 

4

,

pij1

4

(cid:19)

,

contradicting Lemma 9.6. So (9.12) holds and thus,

a  bil(1  (3/2))

(9.13)

Now, for all j  Tl, pij  bilclj  a(1  /2). So, by applying Lemma 9.6 again, for all

j  Tl,

aij  a(1  ).

By step 1 of the algorithm, i = a(1  ), so aij  i for all j  Tl. So, either Ri = Tl

or Tl (cid:40) Ri. In the latter case, the pruning step will set aij = 0 for all j, since topic l has

some catchword i0 for which Ri0 = Tl by Lemma 9.8.

9.9 Finding the Term-Topic Matrix

For this, we need an extra assumption, which we rst motivate. Suppose as in Section

9.8, we assume that there is a single pure document for each topic. In terms of the Figure

327

9.1 of three topics, this says that there is a column of P close to each vertex of the tri-

angle. But the corresponding column of A can be very far from this. So, even if we were

told which document is pure for each topic, we cannot nd the column of B. However, if

we had a large number of nearly pure documents for each topic, since the corresponding

columns of A are independent even conditioned on P , the average of these columns gives

us a good estimate of the column of B. We also note that there is a justication for

assuming the existence of a number of documents which are nearly pure for each topic

based on the Latent Dirichlet Allocation model, (See (2) of Section 9.6). The assumption

is:

Assumption (4): Set of Pure Documents For each l, there is a set Wl of at least

n documents with

clj  1 



4

j  Wl.

If we could nd the set of pure documents for each topic with possibly a small fraction

of errors, we could average them. The major task of this section is to state and prove an

algorithm that does this. For this, we use the primary topic classication, T1, T2, . . . , Tr

from the last section. We know that a for catchword i of topic l, the maximum value of

pij, j = 1, 2, . . . , n occurs for a pure document and indeed if the assumption above holds,

the set of n/4 documents with the top n/4 values of pij should be all pure documents.

But to make use of this, we need to know the catchword, which we are not given. To

discover them, we use another property of catchwords. If i is a catchword for topic l,

then on Tl(cid:48), l(cid:48)

(cid:54)= l, the values of pij are (substantially) lower. So we know that if i is a

catchword of topic l, then it has the property:

Property:

n/4 th maximum value among pij, j  Tl is substantially higher than

than the n/4 th maximum value among pij, j  Tl(cid:48) for any l(cid:48) (cid:54)= l.

We can computationally recognize the property for A (not P ) and on the lines of

Lemma 9.6, we can show that it holds essentially for A if and only if it holds for P .

But then, we need to prove a converse of the statement above, namely we need to

show that if the property holds for i and l, then i is a catchword for topic l. Since catch-

words are not necessarily unique, this is not quite true. But we will prove that any i

satisfying the property for topic l does have bil(cid:48) < bil l(cid:48) (cid:54)= l (Lemma 9.11) and so acts

essentially like a catchword. Using this, we will show that the n/4 documents among all

documents with the highest values of aij for an i satisfying the property, will be nearly

pure documents on topic l in Lemma 9.12 and use this to argue that their average gives

a good approximation to column l of B (Theorem 9.13).

The extra steps in the Algorithm: (By the theorem, the Tl, l = 1, 2, . . . , r are now

known.)

1. For l = 1, 2, . . . , r, and for i = 1, 2, . . . , d, let g(i, l) be the (1  (/4))/;/;th fractile of

328

{Aij : j  Tl}. 40

2. For each l, choose an i(l), (we will prove there is at least 1) such that

g(i(l), l)  (1  (/2)) ; g(i(l), l(cid:48))  (1  2)  g(i(l), l) l(cid:48) (cid:54)= l.

(9.14)

3. Let Rl be the set of n/4 js among j = 1, 2, . . . , n with the highest Ai(l),j.

4. Return B,l = 1

|Rl|

(cid:80)

jRl

A,j as our approximation to B,l.

Lemma 9.9 i(l) satisfying (9.14) exists for each l.

Proof: Let i be a catchword for l. Then, since, j  Wl, pij  bilclj  bil(1  (/4)) and

bil  , we have aij  bil(1  (/2)) and so g(i, l)  (1  (/2))bil  (1  (/2)), by

Lemma 9.6. For j / Tl,

aij  bil(1  (5/2))

by Claim 1.2 and so g(i, l(cid:48))  bil(1  (5/2)). So g(i, l) satises both the requirements

of step 2 of the algorithm.

Fix attention on one l. Let i = i(l). Let

i =

r

max

k=1

bik.

Lemma 9.10 i  (cid:0)1  3

4(cid:1) .

Proof: We have pij = (cid:80)r

9.6. So, either, i   whence the lemma clearly holds or i <  and

k=1 bikckj  i for all i. So, aij  i + 

4 max(, i), from Lemma

j, aij  i +



4

 j = g(i, l)  i + (/4).

By denition of i(l), g(i, l)  (1  (/2)), so

i +



4

  (1  (/2)),

from which the lemma follows.

Lemma 9.11 bik  bil for all k (cid:54)= l.

40The th fractile of a set S of real numbers is the largest real number a so that at least |S| elements

of S are each greter than or equal to a.

329

Proof: Suppose not. Let

We have bil(cid:48) > bil and

l(cid:48) = arg max

k:k(cid:54)=l

bik.

i = Max(bil, bil(cid:48)) <

bil(cid:48)



.

Since for j  Wl(cid:48), at most /4 weight is put on topics other than l(cid:48),

j  Wl(cid:48) , pij  bil(cid:48)(1 



4

)) +



4

(cid:18)

i < bil(cid:48)

1 



4

+

(cid:19)

.



4

Also, for j  Wl(cid:48),

By Lemma 9.6,



4

j  Wl(cid:48), aij  pij 

(cid:18)

 bil(cid:48)

1 

(cid:19)

,

3

4



pij  bil(cid:48)cl(cid:48)j  bil(cid:48)(1 



4

).

max(, pij)  bil(cid:48)(1 



4

) 



4

i

1  (3/4)

by Lemma 9.10

using (9.15) and   0.4. From (9.18), it follows that

g(i, l(cid:48))  bil(cid:48)

(cid:18)

1 

(cid:19)

.

3

4



Since pij  i for all j, using Lemma 9.10,

j, aij  i + (/4)Max(, pij)  i

1 +

(cid:18)

(cid:19)



4

1

1  (3/4)

< bil(cid:48)

1 + (5/6)



,

by (9.15); this implies

g(i, l) 

bil(cid:48)(1 + (5/6))



.

Now, the dention of i(l) implies

g(i, l(cid:48))  g(i, l)(1  2)  bil(cid:48)(1 + (5/6))(1  2)/  bil(cid:48)(1  )

contradicting (9.19) and proving Lemma 9.11.

Lemma 9.12 For each j  Rl of step 3 of the algorithm, we have

clj  1  2.

330

(9.15)

(9.16)

(9.17)

(9.18)

(9.19)

(9.20)

Proof: Let J = {j : clj < 1  2}. Take a j  J. We argue that j / Rl.

pij  bil(1  2) + 2bil  bil(1  1.2),

by Lemma 9.11 using   0.4. So for j  J, we have

aij  bil(1  1.2) +



4

max(, bil) < bil(1  ).

But



4

So for no j  J is aij  g(i, l) and hence no j  J belong sto Rl.

j  Wl, pij  bil(1 

) = aij  bil(1  ) = g(i, l)  bil(1  ).

Theorem 9.13 Assume

n 

cd

m3 ; m 

c

2 .

For all l, 1  l  r, the b,l returned by step 4 of the Algorithm satises

||b,l  b,l||1  6.

Proof: Recall that BC = P . Let V = A  P . From Lemma 9.12, we know that for each

j  Rl, clj  1  2. So

P,j = (1  )B,l + v,

where,   2 and v is a combination of other columns of B with ||v||1    2. Thus,

we have that

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

1

|Rl|

(cid:88)

jRl

p,j  b,l

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)1

 2.

(9.21)

So it suces now to show that

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

1

|Rl|

(cid:88)

jRl

p,j  a,l

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)1

 2.

(9.22)

Note that for an individual j  Rl, ||a,j  p,j||1 can be almost two. For example, if each

pi,j = 1/d, then, Aij would be 1/m for a random subset of m j s and zero for the rest.

What we exploit is that when we average over (n) j s in Rl, the error is small. For this,

the independence of a,j, j  Rl would be useful. But they are not necessarily independent,

there being conditioning on the fact that they all belong to Rl. But there is a simple way

around this conditioning. Namely, we prove (9.22) with very high probability for each

R  [n], |R| = n/4 and then just take the union bound over all (cid:0) n

(cid:1) such subsets.

(n/4)

We know that E (a,j) = p,j. Now consider the random variable x dened by

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

1

|R|

x =

(cid:88)

v,j

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

.

jR

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)1

331

x is a function of m|R| independent random variables, namely, the choice of m|R| terms

in the |R| documents. Changing any one, changes x by at most 1/m|R|. So the Bounded

Dierence Inequality from probability (??? REF ???) implies that

Prob (|x  Ex| > )  2 exp (cid:0)2mn/8(cid:1) .

(9.23)

We also have to bound E(x).

E(x) =

(cid:32)

E

||

1

|R|

(cid:88)

jR

(cid:33)

V,j||1

=

1

|R|

d

(cid:88)

i=1

E

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:88)

jR

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

vij

(cid:118)

(cid:117)

(cid:117)

(cid:117)

(cid:116)E



(cid:32)



1

|R|

d

(cid:88)

i=1

(cid:88)

jR

vij

(cid:33)2

Jensens inequality:E(y)  (cid:112)E(y2)

1

|R|



d

|R|



d

|R|

d

(cid:88)

(cid:115)(cid:88)

i=1

jR

(cid:32) d

(cid:88)

(cid:88)

i=1

jR

(cid:88)

d

(cid:88)

(cid:118)

(cid:117)

(cid:117)

(cid:116)

j

i=1

E(v2

ij) since {vij, j  R}are indep. and var adds up

(cid:33)1/2

E(v2

ij)

Chauchy-Schwartz

E(v2

ij) 





d

mn

 ,



=



=

since E(v2

with (9.23), we see that for a single R  {1, 2, . . . , n} with |R| = n/4,

i pij = 1 and by hypothesis, n  cd/m3. Using this along

ij) = pij/m and (cid:80)

Prob



(cid:12)

(cid:12)

(cid:12)



(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

1

|R|

(cid:88)

jR

v,j

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)1



  2 exp (cid:0)c3mn(cid:1) ,

 2

which implies using the union bound that



Prob

R, |R| =

n

4

:

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

1

|R|

(cid:88)

jR

v,j

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)1



  2 exp (cid:0)c3mn + cn(cid:1)  ,

 2

because the number of R is (cid:0) n

This completes the proof of the theorem.

(n/4)

(cid:1)  (cn/n)n/4  exp(cn) and m  c/2 by hypothesis.

9.10 Hidden Markov Models

A hidden Markov model (HMM) consists of a nite set of states with a transition

between each pair of states. There is an initial probability distribution  on the states

332

and a transition probability aij associated with the transition from state i to state j. Each

state also has a probability distribution p(O, i) giving the probability of outputting the

symbol O in state i. A transition consists of two components. A state transition to a

new state followed by the output of a symbol. The HMM starts by selecting a start state

according to the distribution  and outputting a symbol.

Example: An example of an HMM with two states q and p and two output symbols h

and t is illustrated below.

1

2

q

2h 1

2t

1

3

4

1

2

p

3h 1

3t

2

1

4

The initial distribution is (q) = 1 and (p) = 0. At each step a change of state occurs

followed by the output of heads or tails with probability determined by the new state.

We consider three problems in increasing order of diculty. First, given an HMM

what is the probability of a given output sequence? Second, given an HMM and an out-

put sequence, what is the most likely sequence of states? And third, knowing that the

HMM has at most n states and given an output sequence, what is the most likely HMM?

Only the third problem concerns a hidden Markov model. In the other two problems,

the model is known and the questions can be answered in polynomial time using dynamic

programming. There is no known polynomial time algorithm for the third question.

How probable is an output sequence

Given an HMM, how probable is the output sequence O = O0O1O2    OT of length

T +1? To determine this, calculate for each state i and each initial segment of the sequence

of observations, O0O1O2    Ot of length t + 1, the probability of observing O0O1O2    Ot

ending in state i. This is done by a dynamic programming algorithm starting with t = 0

and increasing t. For t = 0 there have been no transitions. Thus, the probability of

observing O0 ending in state i is the initial probability of starting in state i times the

probability of observing O0 in state i. The probability of observing O0O1O2    Ot ending

in state i is the sum of the probabilities over all states j of observing O0O1O2    Ot1

ending in state j times the probability of going from state j to state i and observing Ot.

The time to compute the probability of a sequence of length T when there are n states is

O(n2T ). The factor n2 comes from the calculation for each time unit of the contribution

from each possible previous state to the probability of each possible current state. The

space complexity is O(n) since one only needs to remember the probability of reaching

333

each state for the most recent value of t.

Algorithm to calculate the probability of the output sequence

The probability, Prob(O0O1    OT , i) of the output sequence O0O1    OT ending in

state i is given by

Prob(O0, i) = (i)p(O0, i)

and for t = 1 to T

Prob(O0O1    Ot, i) = (cid:80)

Prob(O0O1    Ot1, j)aijp(Ot+1, i).

j

Example: What is the probability of the sequence hhht by the HMM in the above two

state example?

1

2

72

1

1

2 + 5

2 + 1

2 = 1

3

4

6

1

8

t = 3

t = 2

t = 1

t = 0

3

32

1

2

1

2

1

8

1

2

1

2

3

4

1

2 = 19

384

3

32

1

2

1

2 = 3

32

1

8

1

2

1

2

1

2

0

72

1

2

3 + 5

3 + 1

3 = 1

1

4

6

6

2

1

4

1

3 = 37

6427

2

3 = 5

72

q

p

For t = 0, the q entry is 1/2 since the probability of being in state q is one and the proba-

bility of outputting heads is 1

2. The entry for p is zero since the probability of starting in

state p is zero. For t = 1, the q entry is 1

8 since for t = 0 the q entry is 1

2 and in state q

the HMM goes to state q with probability 1

2. The

p entry is 1

2 and in state q the HMM goes to state p with

probability 1

3. For t = 2, the q entry is 3

32 which

consists of two terms. The rst term is the probability of ending in state q at t = 1 times

the probability of staying in q and outputting h. The second is the probability of ending

in state p at t = 1 times the probability of going from state p to state q and outputting h.

6 since for t = 0 the q entry is 1

2 and outputs heads with probability 2

2 and outputs heads with probability 1

From the table, the probability of producing the sequence hhht is 19

384 + 37

1728 = 0.0709.

The most likely sequence of states - the Viterbi algorithm

Given an HMM and an observation O = O0O1    OT , what is the most likely sequence

of states? The solution is given by the Viterbi algorithm, which is a slight modication

to the dynamic programming algorithm just given for determining the probability of an

output sequence. For t = 0, 1, 2, . . . , T and for each state i, we calculate the probability

334

of the most likely sequence of states to produce the output O0O1O2    Ot ending in state

i as follows. For each state j, we have already computed the probability of the most

likely sequence producing O0O1O2    Ot1 ending in state j, and we multiply this by the

probability of the transition from j to i producing Ot. We then select the j for which this

product is largest. Note that in the previous example, we added the probabilities of each

possibility together. Now we take the maximum and also record where the maximum

came from. The time complexity is O(n2T ) and the space complexity is O(nT ). The

space complexity bound is argued as follows. In calculating the probability of the most

likely sequence of states that produces O0O1 . . . Ot ending in state i, we remember the

previous state j by putting an arrow with edge label t from i to j. At the end, can nd

the most likely sequence by tracing backwards as is standard for dynamic programming

algorithms.

Example: For the earlier example what is the most likely sequence of states to produce

the output hhht?

t = 3

t = 2

t = 1

t = 0

1

2

24

1

1

2, 1

2, 1

q

3

4

6

max{ 1

48

max{ 1

8

1

2

1

2

1

2

1

2

1

8

2 = 1

q

3

4

1

2} = 1

64

1

2} = 3

48

q or p

p

max{ 3

48

max{ 1

8

1

2

1

4

1

3} = 1

96

2

3} = 1

24

q

q

1

2

24

2

1

3, 1

3, 1

q

1

4

6

1

2

1

2

2

3 = 1

6

0 p

q

p

Note that the two sequences of states, qqpq and qpqq, are tied for the most likely se-

quences of states.

Determining the underlying hidden Markov model

Given an n-state HMM, how do we adjust the transition probabilities and output prob-

abilities to maximize the probability of an output sequence O1O2    OT ? The assumption

is that T is much larger than n.41 There is no known computationally ecient method

for solving this problem. However, there are iterative techniques that converge to a local

optimum.

Let aij be the transition probability from state i to state j and let bj(Ok) be the

probability of output Ok given that the HMM is in state j. Given estimates for the HMM

parameters, aij and bj, and the output sequence O, we can improve the estimates by

calculating for each time step the probability that the HMM goes from state i to state j

and outputs the symbol Ok, conditioned on O being the output sequence.

41If T  n then one can just have the HMM be a linear sequence that outputs O1O2 . . . OT with

probability 1.

335

aij

transition probability from state i to state j

bj(Ot+1) probability of Ot+1 given that the HMM is in state j at time t + 1

t(i)

probability of seeing O0O1    Ot and ending in state i at time t

t+1(j)

probability of seeing the tail of the sequence Ot+2Ot+3    OT given state j

at time t + 1

(i, j)

probability of going from state i to state j at time t given the sequence

of outputs O

st(i)

probability of being in state i at time t given the sequence of outputs O

p(O)

probability of output sequence O

Given estimates for the HMM parameters, aij and bj, and the output sequence O, the

probability t(i, j) of going from state i to state j at time t is given by the probability of

producing the output sequence O and going from state i to state j at time t divided by

the probability of producing the output sequence O.

t(i, j) =

t(i)aijbj(Ot+1)t+1(j)

p(O)

The probability p(O) is the sum over all pairs of states i and j of the numerator in the

above formula for t(i, j). That is,

p(O) =

(cid:88)

(cid:88)

i

j

t(j)aijbj(Ot+1)t+1(j).

The probability of being in state i at time t is given by

st(i) =

n

(cid:88)

j=1

t(i, j).

Summing st(i) over all time periods gives the expected number of times state i is visited

and the sum of t(i, j) over all time periods gives the expected number of times edge i to

j is traversed.

Given estimates of the HMM parameters ai,j and bj(Ok), we can calculate by the above

formulas estimates for

1. (cid:80)T 1

i=1 st(i), the expected number of times state i is visited and departed from

336

2. (cid:80)T 1

i=1 t(i, j), the expected number of transitions from state i to state j

Using these estimates we can obtain new estimates of the HMM parameters

aij =

expected number of transitions from state i to state j

expected number of transitions out of state i

=

(cid:80)T 1

t=1 t(i, j)

t=1 st(i)

(cid:80)T 1

bj(Ok) =

expected number of times in state j observing symbol Ok

expected number of times in state j

=

st(j)

T 1

(cid:80)

t=1

subject to

Ot=Ok

(cid:80)T 1

t=1 st(j)

By iterating the above formulas we can arrive at a local optimum for the HMM parameters

ai,j and bj(Ok).

9.11 Graphical Models and Belief Propagation

A graphical model is a compact representation of a probability distribution over n

variables x1, x2, . . . , xn. It consists of a graph, directed or undirected, whose vertices cor-

respond to variables that take on values from some set. In this chapter, we consider the

case where the set of values the variables take on is nite, although graphical models are

often used to represent probability distributions with continuous variables. The edges of

the graph represent relationships or constraints between the variables.

In the directed model, it is assumed that the directed graph is acyclic. This model

represents a joint probability distribution that factors into a product of conditional prob-

abilities.

p (x1, x2, . . . , xn) =

p (xi|parents of xi)

n

(cid:89)

i=1

The directed graphical model is called a Bayesian or belief network and appears frequently

in the articial intelligence and the statistics literature.

The undirected graphical model, called a Markov random eld, can also represent a

joint probability distribution of the random variables at its vertices. In many applications

the Markov random eld represents a function of the variables at the vertices which is to

be optimized by choosing values for the variables.

A third model called the factor model is akin to the Markov random eld, but here

the dependency sets have a dierent structure. In the following sections we describe all

these models in more detail.

337

C1

D1

S1

C2

causes

D2

diseases

S2

symptoms

Figure 9.2: A Bayesian network

9.12 Bayesian or Belief Networks

A Bayesian network is a directed acyclic graph where vertices correspond to variables

and a directed edge from y to x represents a conditional probability p(x|y). If a vertex x

has edges into it from y1, y2, . . . , yk, then the conditional probability is p (x | y1, y2, . . . , yk).

The variable at a vertex with no in edges has an unconditional probability distribution.

If the value of a variable at some vertex is known, then the variable is called evidence.

An important property of a Bayesian network is that the joint probability is given by the

product over all nodes of the conditional probability of the node conditioned on all its

immediate predecessors.

In the example of Fig. 9.1, a patient is ill and sees a doctor. The doctor ascertains

the symptoms of the patient and the possible causes such as whether the patient was in

contact with farm animals, whether he had eaten certain foods, or whether the patient

has an hereditary predisposition to any diseases. Using the above Bayesian network where

the variables are true or false, the doctor may wish to determine one of two things. What

is the marginal probability of a given disease or what is the most likely set of diseases. In

determining the most likely set of diseases, we are given a T or F assignment to the causes

and symptoms and ask what assignment of T or F to the diseases maximizes the joint

probability. This latter problem is called the maximum a posteriori probability (MAP).

Given the conditional probabilities and the probabilities p (C1) and p (C2) in Figure

9.1, the joint probability p (C1, C2, D1, . . .) can be computed easily for any combination

of values of C1, C2, D1, . . .. However, we might wish to nd the value of the variables of

highest probability (MAP) or we might want one of the marginal probabilities p (D1) or

p (D2). The obvious algorithms for these two problems require evaluating the probabil-

ity p (C1, C2, D1, . . .) over exponentially many input values or summing the probability

p (C1, C2, D1, . . .) over exponentially many values of the variables other than those for

338

which we want the marginal probability. In certain situations, when the joint probability

distribution can be expressed as a product of factors, a belief propagation algorithm can

solve the maximum a posteriori problem or compute all marginal probabilities quickly.

9.13 Markov Random Fields

The Markov random eld model arose rst in statistical mechanics where it was called

the Ising model. It is instructive to start with a description of it. The simplest version

n grid. Each

of the Ising model consists of n particles arranged in a rectangular

particle can have a spin that is denoted 1. The energy of the whole system depends

on interactions between pairs of neighboring particles. Let xi be the spin, 1, of the ith

particle. Denote by i  j the relation that i and j are adjacent in the grid. In the Ising

model, the energy of the system is given by

n 





f (x1, x2, . . . , xn) = exp

c

(cid:32)

(cid:33)

|xi  xj|

.

(cid:88)

ij

The constant c can be positive or negative. If c < 0, then energy is lower if many adjacent

pairs have opposite spins and if c > 0 the reverse holds. The model was rst used to

model probabilities of spin congurations in physical materials.

In most computer science settings, such functions are mainly used as objective func-

tions that are to be optimized subject to some constraints. The problem is to nd the

minimum energy set of spins under some constraints on the spins. Usually the constraints

just specify the spins of some particles. Note that when c > 0, this is the problem of

minimizing (cid:80)

|xi  xj| subject to the constraints. The objective function is convex and

ij

so this can be done eciently. If c < 0, however, we need to minimize a concave function

for which there is no known ecient algorithm. The minimization of a concave func-

tion in general is NP-hard. Intuitively, this is because the set of inputs for which f (x) is

less than some given value can be nonconvex or even consist of many disconnected regions.

A second important motivation comes from the area of vision. It has to to do with

reconstructing images. Suppose we are given noisy observations of the intensity of light at

individual pixels, x1, x2, . . . , xn, and wish to compute the true values, the true intensities,

of these variables y1, y2, . . . , yn. There may be two sets of constraints, the rst stipulating

that the yi should generally be close to the corresponding xi and the second, a term

correcting possible observation errors, stipulating that yi should generally be close to the

values of yj for j  i. This can be formulated as

(cid:32)

(cid:88)

i

min

y

|xi  yi| +

(cid:33)

|yi  yj|

,

(cid:88)

ij

where the values of xi are constrained to be the observed values. The objective function

is convex and polynomial time minimization algorithms exist. Other objective functions

339

x1 + x2 + x3

x1 + x2

x1 + x3

x2 + x3

x1

x2

x3

Figure 9.3: The factor graph for the function

f (x1, x2, x3) = (x1 + x2 + x3)(x1 + x2)(x1 + x3)(x2 + x3).

using say sum of squares instead of sum of absolute values can be used and there are

polynomial time algorithms as long as the function to be minimized is convex.

More generally, the correction term may depend on all grid points within distance

two of each point rather than just immediate neighbors. Even more generally, we may

have n variables y1, y2, . . . yn with the value of some of them already specied and subsets

S1, S2, . . . Sm of these variables constrained in some way. The constraints are accumulated

into one objective function which is a product of functions f1, f2, . . . , fm, where function fi

is evaluated on the variables in subset Si. The problem is to minimize (cid:81)m

i=1 fi(yj, j  Si)

subject to constrained values. Note that the vision example had a sum instead of a prod-

uct, but by taking exponentials we can turn the sum into a product as in the Ising model.

In general, the fi are not convex; indeed they may be discrete. So the minimization

cannot be carried out by a known polynomial time algorithm. The most used forms of the

Markov random eld involve Si which are cliques of a graph. So we make the following

denition.

A Markov Random Field consists of an undirected graph and an associated function

that factorizes into functions associated with the cliques of the graph. The special case

when all the factors correspond to cliques of size one or two is of interest.

9.14 Factor Graphs

Factor graphs arise when we have a function f of a variables x = (x1, x2, . . . , xn) that

f (x) where each factor depends only on some small

can be expressed as f (x) = (cid:81)

number of variables x. The dierence from Markov random elds is that the variables

corresponding to factors do not necessarily form a clique. Associate a bipartite graph

where one set of vertices correspond to the factors and the other set to the variables.



340

Place an edge between a variable and a factor if the factor contains that variable. See

Figure 9.3.

9.15 Tree Algorithms

Let f (x) be a function that is a product of factors. When the factor graph is a tree

there are ecient algorithms for solving certain problems. With slight modications, the

algorithms presented can also solve problems where the function is the sum of terms rather

than a product of factors.

The rst problem is called marginalization and involves evaluating the sum of f over

all variables except one. In the case where f is a probability distribution the algorithm

computes the marginal probabilities and thus the word marginalization. The second prob-

lem involves computing the assignment to the variables that maximizes the function f .

When f is a probability distribution, this problem is the maximum a posteriori probabil-

ity or MAP problem.

If the factor graph is a tree (such as in Figure 9.4), then there exists an ecient al-

gorithm for solving these problems. Note that there are four problems: the function f

is either a product or a sum and we are either marginalizing or nding the maximizing

assignment to the variables. All four problems are solved by essentially the same algo-

rithm and we present the algorithm for the marginalization problem when f is a product.

Assume we want to sum out all the variables except x1, leaving a function of x1.

Call the variable node associated with some variable xi node xi. First, make the node

x1 the root of the tree.

It will be useful to think of the algorithm rst as a recursive

algorithm and then unravel the recursion. We want to compute the product of all factors

occurring in the sub-tree rooted at the root with all variables except the root-variable

summed out. Let gi be the product of all factors occurring in the sub-tree rooted at

node xi with all variables occurring in the subtree except xi summed out. Since this is a

tree, x1 will not reoccur anywhere except the root. Now, the grandchildren of the root

are variable nodes and suppose inductively, each grandchild xi of the root, has already

computed its gi. It is easy to see that we can compute g1 as follows.

Each grandchild xi of the root passes its gi to its parent, which is a factor node. Each

child of x1 collects all its childrens gi, multiplies them together with its own factor and

sends the product to the root. The root multiplies all the products it gets from its children

and sums out all variables except its own variable, namely here x1.

Unraveling the recursion is also simple, with the convention that a leaf node just re-

ceives 1, product of an empty set of factors, from its children. Each node waits until it

receives a message from each of its children. After that, if the node is a variable node,

it computes the product of all incoming messages, and sums this product function over

341

x1

x1

x1 + x2 + x3

x3 + x4 + x5

x2

x3

x4

x4

x5

x5

Figure 9.4: The factor graph for the function f = x1 (x1 + x2 + x3) (x3 + x4 + x5) x4x5.

all assignments to the variables except for the variable of the node. Then, it sends the

resulting function of one variable out along the edge to its parent. If the node is a factor

node, it computes the product of its factor function along with incoming messages from

all the children and sends the resulting function out along the edge to its parent.

The reader should prove that the following invariant holds assuming the graph is a tree:

Invariant The message passed by each variable node to its parent is the product of

all factors in the subtree under the node with all variables in the subtree except its own

summed out.

Consider the following example where

f = x1 (x1 + x2 + x3) (x3 + x4 + x5) x4x5

and the variables take on values 0 or 1. Consider marginalizing f by computing

f (x1) =

(cid:88)

x2x3x4x5

x1 (x1 + x2 + x3) (x3 + x4 + x5) x4x5,

In this case the factor graph is a tree as shown in Figure 9.4. The factor graph as a

rooted tree and the messages passed by each node to its parent are shown in Figure 9.5.

If instead of computing marginals, one wanted the variable assignment that maximizes

the function f , one would modify the above procedure by replacing the summation by a

maximization operation. Obvious modications handle the situation where f (x) is a sum

of products.

f (x) =

g (x)

(cid:88)

x1,...,xn

9.16 Message Passing in General Graphs

The simple message passing algorithm in the last section gives us the one variable

function of x1 when we sum out all the other variables. For a general graph that is not

342

(cid:80)

x2,x3

x1(x1 + x2 + x3)(2 + x3) = 10x2

1 + 11x1

x1

x1 

x1

x1 + x2 + x3

1 

x2

x3

(x1 + x2 + x3)(2 + x3) 

(cid:80)

x4,x5

(x3 + x4 + x5)x4x5

= 2 + x3 

(x3 + x4 + x5)x4x5 

x3 + x4 + x5

x4 

x4 

x4

x4

x5 

x5 

x5

x5

Figure 9.5: Messages.

a tree, we formulate an extension of that algorithm. But unlike the case of trees, there

is no proof that the algorithm will converge and even if it does, there is no guarantee

that the limit is the marginal probability. This has not prevented its usefulness in some

applications.

First, lets ask a more general question, just for trees. Suppose we want to compute

for each i the one-variable function of xi when we sum out all variables xj, j (cid:54)= i. Do we

have to repeat what we did for x1 once for each xi? Luckily, the answer is no. It will

suce to do a second pass from the root to the leaves of essentially the same message

passing algorithm to get all the answers. Recall that in the rst pass, each edge of the

tree has sent a message up, from the child to the parent. In the second pass, each edge

will send a message down from the parent to the child. We start with the root and work

downwards for this pass. Each node waits until its parent has sent it a message before

sending messages to each of its children. The rules for messages are:

343

Rule 1 The message from a factor node v to a child xi, which is the variable node xi,

is the product of all messages received by v in both passes from all nodes other than xi

times the factor at v itself.

Rule 2 The message from a variable node xi to a factor node child, v, is the product

of all messages received by xi in both passes from all nodes except v, with all variables

except xi summed out. The message is a function of xi alone.

At termination, when the graph is a tree, if we take the product of all messages re-

ceived in both passes by a variable node xi and sum out all variables except xi in this

product, what we get is precisely the entire function marginalized to xi. We do not give

the proof here. But the idea is simple. We know from the rst pass that the product of

the messages coming to a variable node xi from its children is the product of all factors in

the sub-tree rooted at xi. In the second pass, we claim that the message from the parent

v to xi is the product of all factors which are not in the sub-tree rooted at xi which one

can show either directly or by induction working from the root downwards.

We can apply the same rules 1 and 2 to any general graph. We do not have child and

parent relationships and it is not possible to have the two synchronous passes as before.

The messages keep owing and one hopes that after some time, the messages will stabilize,

but nothing like that is proven. We state the algorithm for general graphs now:

Rule 1 At each unit of time, each factor node v sends a message to each adjacent

node xi. The message is the product of all messages received by v at the previous step

except for the one from xi multiplied by the factor at v itself.

Rule 2 At each time, each variable node xi sends a message to each adjacent node v.

The message is the product of all messages received by xi at the previous step except the

one from v, with all variables except xi summed out.

9.17 Graphs with a Single Cycle

The message passing algorithm gives the correct answers on trees and on certain other

graphs. One such situation is graphs with a single cycle which we treat here. We switch

from the marginalization problem to the MAP problem as the proof of correctness is

simpler for the MAP problem. Consider the network in Figure 9.6a with a single cycle.

The message passing scheme will count some evidence multiply. The local evidence at A

will get passed around the loop and will come back to A. Thus, A will count the local

evidence multiple times. If all evidence is multiply counted in equal amounts, then there

is a possibility that though the numerical values of the marginal probabilities (beliefs) are

wrong, the algorithm still converges to the correct maximum a posteriori assignment.

Consider the unwrapped version of the graph in Figure 9.6b. The messages that the

344

A

B

C

(a) A graph with a single cycle

A

B

C

A

B

C

A

B

C

(b) Segment of unrolled graph

Figure 9.6: Unwrapping a graph with a single cycle

loopy version will eventually converge to, assuming convergence, are the same messages

that occur in the unwrapped version provided that the nodes are suciently far in from

the ends. The beliefs in the unwrapped version are correct for the unwrapped graph since

it is a tree. The only question is, how similar are they to the true beliefs in the original

network.

Write p (A, B, C) = elog p(A,B,C) = eJ(A,B,C) where J (A, B, C) = log p (A, B, C). Then

the probability for the unwrapped network is of the form ekJ(A,B,C)+J (cid:48) where the J (cid:48) is

associated with vertices at the ends of the network where the beliefs have not yet stabi-

lized and the kJ (A, B, C) comes from k inner copies of the cycle where the beliefs have

stabilized. Note that the last copy of J in the unwrapped network shares an edge with J (cid:48)

and that edge has an associated . Thus, changing a variable in J has an impact on the

value of J (cid:48) through that . Since the algorithm maximizes Jk = kJ (A, B, C) + J (cid:48) in the

unwrapped network for all k, it must maximize J (A, B, C). To see this, set the variables

A, B, C, so that Jk is maximized. If J (A, B, C) is not maximized, then change A, B, and

C to maximize J (A, B, C). This increases Jk by some quantity that is proportional to

k. However, two of the variables that appear in copies of J (A, B, C) also appear in J (cid:48)

345

y2

x2

y3

x3

x4

y4

y1

yn

x1

xn

Figure 9.7: A Markov random eld with a single loop.

and thus J (cid:48) might decrease in value. As long as J (cid:48) decreases by some nite amount, we

can increase Jk by increasing k suciently. As long as all s are nonzero, J (cid:48) which is

proportional to log , can change by at most some nite amount. Hence, for a network

with a single loop, assuming that the message passing algorithm converges, it converges

to the maximum a posteriori assignment.

9.18 Belief Update in Networks with a Single Loop

In the previous section, we showed that when the message passing algorithm converges,

it correctly solves the MAP problem for graphs with a single loop. The message passing

algorithm can also be used to obtain the correct answer for the marginalization problem.

Consider a network consisting of a single loop with variables x1, x2, . . . , xn and evidence

y1, y2, . . . , yn as shown in Figure 9.7. The xi and yi can be represented by vectors having

a component for each value xi can take on. To simplify the discussion assume the xi take

on values 1, 2, . . . , m.

Let mi be the message sent from vertex i to vertex i + 1 mod n. At vertex i + 1

each component of the message mi is multiplied by the evidence yi+1 and the constraint

function . This is done by forming a diagonal matrix Di+1 where the diagonal elements

are the evidence and then forming a matrix Mi whose jkth element is  (xi+1 = j, xi = k).

The message mi+1 is MiDi+1mi. Multiplication by the diagonal matrix Di+1 multiplies

the components of the message mi by the associated evidence. Multiplication by the

matrix Mi multiplies each component of the vector by the appropriate value of  and

sums over the values producing the vector which is the message mi+1. Once the message

346

has travelled around the loop, the new message m(cid:48)

1 is given by

m(cid:48)

1 = MnD1Mn1Dn    M2D3M1D2m1

Let M = MnD1Mn1Dn    M2D3M1D2m1. Assuming that M s principal eigenvalue is

unique, the message passing will converge to the principal vector of M . The rate of con-

vergences depends on the ratio of the rst and second eigenvalues.

An argument analogous to the above concerning the messages going clockwise around

the loop applies to messages moving counter-clockwise around the loop. To obtain the es-

timate of the marginal probability p (x1), one multiples component-wise the two messages

arriving at x1 along with the evidence y1. This estimate does not give the true marginal

probability but the true marginal probability can be computed from the estimate and the

rate of convergences by linear algebra.

9.19 Maximum Weight Matching

We have seen that the belief propagation algorithm converges to the correct solution

in trees and graphs with a single cycle. It also correctly converges for a number of prob-

lems. Here we give one example, the maximum weight matching problem where there is

a unique solution.

We apply the belief propagation algorithm to nd the maximal weight matching

(MWM) in a complete bipartite graph. If the MWM in the bipartite graph is unique,

then the belief propagation algorithm will converge to it.

Let G = (V1, V2, E) be a complete bipartite graph where V1 = {a1, . . . , an} , V2 =

1  i, j  n. Let  = { (1) , . . . ,  (n)} be a per-

{b1, . . . , bn} , and (ai, bj)  E,

mutation of {1, . . . , n}. The collection of edges (cid:8)(cid:0)a1, b(1)

(cid:1)(cid:9)is called a

matching which is denoted by . Let wij be the weight associated with the edge (ai, bj).

wi(i). The maximum weight matching  is

(cid:1) , . . . , (cid:0)an, b(n)

The weight of the matching  is w =

 = arg max

w



n

(cid:80)

i=1

The rst step is to create a factor graph corresponding to the MWM problem. Each

edge of the bipartite graph is represented by a variable cij which takes on the value zero or

one. The value one means that the edge is present in the matching, the value zero means

that the edge is not present in the matching. A set of constraints is used to force the set

of edges to be a matching. The constraints are of the form (cid:80)

cij = 1. Any

cij = 1 and (cid:80)

j

i

0,1 assignment to the variables cij that satises all of the constraints denes a matching.

In addition, we have constraints for the weights of the edges.

We now construct a factor graph, a portion of which is shown in Figure 9.8. Associated

with the factor graph is a function f (c11, c12, . . .) consisting of a set of terms for each cij

347

enforcing the constraints and summing the weights of the edges of the matching. The

terms for c12 are



(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:32)

(cid:88)

(cid:33)

ci2

i

(cid:12)

(cid:12)

(cid:12)

 1

(cid:12)

(cid:12)

 

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:32)

(cid:88)

j

c1j

(cid:33)

(cid:12)

(cid:12)

(cid:12)

 1

(cid:12)

(cid:12)

+ w12c12

where  is a large positive number used to enforce the constraints when we maximize the

function. Finding the values of c11, c12, . . . that maximize f nds the maximum weighted

matching for the bipartite graph.

If the factor graph was a tree, then the message from a variable node x to its parent

is a message g(x) that gives the maximum value for the subtree for each value of x. To

compute g(x), one sums all messages into the node x. For a constraint node, one sums

all messages from subtrees and maximizes the sum over all variables except the variable

of the parent node subject to the constraint. The message from a variable x consists of

two pieces of information, the value p (x = 0) and the value p (x = 1). This information

can be encoded into a linear function of x:

[p (x = 1)  p (x = 0)] x + p (x = 0) .

Thus, the messages are of the form ax + b. To determine the MAP value of x once the

algorithm converges, sum all messages into x and take the maximum over x=1 and x=0

to determine the value for x. Since the arg maximum of a linear form ax +b depends

only on whether a is positive or negative and since maximizing the output of a constraint

depends only on the coecient of the variable, we can send messages consisting of just

the variable coecient.

To calculate the message to c12 from the constraint that node b2 has exactly one

neighbor, add all the messages that ow into the constraint node from the ci2, i (cid:54)= 1

nodes and maximize subject to the constraint that exactly one variable has value one. If

 (i, 2). If

c12 = 0, then one of ci2, i (cid:54)= 1, will have value one and the message is max

i(cid:54)=1

c12 = 1, then the message is zero. Thus, we get

 max

i(cid:54)=1

 (i, 2) x + max

i(cid:54)=1

 (i, 2)

and send the coecient  max

i(cid:54)=1

 (i, 2). This means that the message from c12 to the other

constraint node is (1, 2) = w12  max

i(cid:54)=1

 (i, 2).

The alpha message is calculated in a similar fashion. If c12 = 0, then one of c1j will

 (1, j). If c12 = 1, then the message is zero. Thus,

have value one and the message is max

j(cid:54)=1

the coecient  max

j(cid:54)=1

 (1, j) is sent. This means that (1, 2) = w12  max

j(cid:54)=1

 (1, j).

To prove convergence, we unroll the constraint graph to form a tree with a constraint

node as the root. In the unrolled graph a variable node such as c12 will appear a number

348

w12c12

(cid:80)

j c1j = 1

c12

(cid:80)

i ci2 = 1



(1, 2)



(1, 2)

Constraint forcing

b2 to have exactly

one neighbor

Constraint forcing

a1to have exactly

one neighbor

c32

c42

cn2

Figure 9.8: Portion of factor graph for the maximum weight matching problem.

of times which depends on how deep a tree is built. Each occurrence of a variable such

as c12 is deemed to be a distinct variable.

Lemma 9.14 If the tree obtained by unrolling the graph is of depth k, then the messages

to the root are the same as the messages in the constraint graph after k-iterations.

Proof: Straightforward.

Dene a matching in the tree to be a set of vertices so that there is exactly one variable

node of the match adjacent to each constraint. Let  denote the vertices of the matching.

Heavy circles in Figure 9.9 represent the nodes of the above tree that are in the matching .

Let  be the vertices corresponding to maximum weight matching edges in the bi-

partite graph. Recall that vertices in the above tree correspond to edges in the bipartite

graph. The vertices of  are denoted by dotted circles in the above tree.

Consider a set of trees where each tree has a root that corresponds to one of the con-

straints. If the constraint at each root is satised by the edge of the MWM, then we have

found the MWM. Suppose that the matching at the root in one of the trees disagrees

with the MWM. Then there is an alternating path of vertices of length 2k consisting of

vertices corresponding to edges in  and edges in . Map this path onto the bipartite

graph. In the bipartite graph the path will consist of a number of cycles plus a simple

path. If k is large enough there will be a large number of cycles since no cycle can be of

length more than 2n. Let m be the number of cycles. Then m  2k

2n = k

n .

Let  be the MWM in the bipartite graph. Take one of the cycles and use it as an

alternating path to convert the MWM to another matching. Assuming that the MWM

is unique and that the next closest matching is  less, W  W >  where  is the new

349

(cid:80)

j c1j = 1

c11

c13

c1n

(cid:80)

j ci2 = 1

(cid:80)

i ci2 = 1

c12

c22

c32

cn2

Figure 9.9: Tree for MWM problem.

matching.

Consider the tree matching. Modify the tree matching by using the alternating path

of all cycles and the left over simple path. The simple path is converted to a cycle by

adding two edges. The cost of the two edges is at most 2w* where w* is the weight of the

maximum weight edge. Each time we modify  by an alternating cycle, we increase the

cost of the matching by at least . When we modify  by the left over simple path, we

increase the cost of the tree matching by   2w since the two edges that were used to

create a cycle in the bipartite graph are not used. Thus

weight of  - weight of (cid:48)  k

n   2w

which must be negative since (cid:48) is optimal for the tree. However, if k is large enough this

becomes positive, an impossibility since (cid:48) is the best possible. Since we have a tree, there

can be no cycles, as messages are passed up the tree, each subtree is optimal and hence the

total tree is optimal. Thus the message passing algorithm must nd the maximum weight

matching in the weighted complete bipartite graph assuming that the maximum weight

matching is unique. Note that applying one of the cycles that makes up the alternating

path decreased the bipartite graph match but increases the value of the tree. However,

it does not give a higher tree matching, which is not possible since we already have the

maximum tree matching. The reason for this is that the application of a single cycle does

not result in a valid tree matching. One must apply the entire alternating path to go from

one matching to another.

350

a

b

c

i

j

Figure 9.10: warning propagation

9.20 Warning Propagation

Signicant progress has been made using methods similar to belief propagation in

nding satisfying assignments for 3-CNF formulas. Thus, we include a section on a

version of belief propagation, called warning propagation, that is quite eective in nding

assignments. Consider a factor graph for a SAT problem (Figure 9.10). Index the variables

by i, j, and k and the factors by a, b, and c. Factor a sends a message mai to each variable i

that appears in the factor a called a warning. The warning is 0 or 1 depending on whether

or not factor a believes that the value assigned to i is required for a to be satised. A

factor a determines the warning to send to variable i by examining all warnings received

by other variables in factor a from factors containing them.

For each variable j, sum the warnings from factors containing j that warn j to take

value T and subtract the warnings that warn j to take value F. If the dierence says that

j should take value T or F and this value for variable j does not satisfy a, and this is

true for all j, then a sends a warning to i that the value of variable i is critical for factor a.

Start the warning propagation algorithm by assigning 1 to a warning with probability

1/2. Iteratively update the warnings. If the warning propagation algorithm converges,

then compute for each variable i the local eld hi and the contradiction number ci. The

local eld hi is the number of clauses containing the variable i that sent messages that

i should take value T minus the number that sent messages that i should take value F.

The contradiction number ci is 1 if variable i gets conicting warnings and 0 otherwise.

If the factor graph is a tree, the warning propagation algorithm converges. If one of the

warning messages is one, the problem is unsatisable; otherwise it is satisable.

9.21 Correlation Between Variables

In many situations one is interested in how the correlation between variables drops o

with some measure of distance. Consider a factor graph for a 3-CNF formula. Measure

the distance between two variables by the shortest path in the factor graph. One might

ask if one variable is assigned the value true, what is the percentage of satisfying assign-

ments of the 3-CNF formula in which the second variable also is true. If the percentage

is the same as when the rst variable is assigned false, then we say that the two variables

351

are uncorrelated. How dicult it is to solve a problem is likely to be related to how fast

the correlation decreases with distance.

Another illustration of this concept is in counting the number of perfect matchings

in a graph. One might ask what is the percentage of matching in which some edge is

present and ask how correlated this percentage is with the presences or absence of edges

at some distance d. One is interested in whether the correlation drops o with distance.

To explore this concept we consider the Ising model studied in physics.

As mentioned earlier, the Ising or ferromagnetic model is a pairwise random Markov

eld. The underlying graph, usually a lattice, assigns a value of 1, called spin, to the

variable at each vertex. The probability (Gibbs measure) of a given conguration of spins

is proportional to exp( (cid:80)

exixj where xi = 1 is the value associated

xixj) = (cid:81)

with vertex i. Thus

(i,j)E

(i,j)E

p (x1, x2, . . . , xn) = 1

Z

(cid:81)

(i,j)E

exp(xixj) = 1

Z e

 (cid:80)

(i,j)E

xixj

where Z is a normalization constant.

The value of the summation is simply the dierence in the number of edges whose

vertices have the same spin minus the number of edges whose vertices have opposite spin.

The constant  is viewed as inverse temperature. High temperature corresponds to a low

value of  and low temperature corresponds to a high value of . At high temperature,

low , the spins of adjacent vertices are uncorrelated whereas at low temperature adjacent

vertices have identical spins. The reason for this is that the probability of a conguration

is proportional to e

. As  is increased, for congurations with a large number of

edges whose vertices have identical spins, e

increases more than for congurations

whose edges have vertices with non identical spins. When the normalization constant 1

Z

is adjusted for the new value of , the highest probability congurations are those where

adjacent vertices have identical spins.

 (cid:80)

ij

xixj

 (cid:80)

ij

xixj

Given the above probability distribution, what is the correlation between two variables

xi and xj? To answer this question, consider the probability that xi = +1 as a function

of the probability that xj = +1. If the probability that xi = +1 is 1

2 independent of the

value of the probability that xj = +1, we say the values are uncorrelated.

2 ln d+1

Consider the special case where the graph G is a tree. In this case a phase transition

occurs at 0 = 1

d1 where d is the degree of the tree. For a suciently tall tree and for

 > 0, the probability that the root has value +1 is bounded away from 1/2 and depends

on whether the majority of leaves have value +1 or -1. For  < 0 the probability that

the root has value +1 is 1/2 independent of the values at the leaves of the tree.

352

Consider a height one tree of degree d. If i of the leaves have spin +1 and d  i have

spin -1, then the probability of the root having spin +1 is proportional to

ei(di) = e(2id).

If the probability of a leaf being +1 is p, then the probability of i leaves being +1 and

d  i being -1 is

(cid:19)

(cid:18)d

i

pi (1  p)di

Thus, the probability of the root being +1 is proportional to

A =

d

(cid:88)

i=1

(cid:18)d

i

(cid:19)

pi(1  p)die(2id) = ed

d

(cid:88)

i=1

(cid:19)

(cid:18)d

i

(cid:0)pe2(cid:1)i

(1  p)di = ed (cid:2)pe2 + 1  p(cid:3)d

and the probability of the root being 1 is proportional to

B =

d

(cid:88)

i=1

(cid:18)d

i

(cid:19)

pi(1  p)die(2id)

d

(cid:88)

= ed

(cid:19)

pi (cid:2)(1  p)e2(id)(cid:3)

(cid:18)d

i

i=1

d

(cid:88)

(cid:19)

= ed

(cid:18)d

i

= ed (cid:2)p + (1  p)e2(cid:3)d

i=1

.

pi (cid:2)(1  p)e2(cid:3)di

The probability of the root being +1 is

q = A

A+B =

[pe2 +1p]d

[pe2 +1p]d

+[p+(1p)e2]d = C

D

where

and

C = (cid:2)pe2 + 1  p(cid:3)d

D = (cid:2)pe2 + 1  p(cid:3)d + (cid:2)p + (1  p) e2(cid:3)d.

At high temperature, low , the probability q of the root of the height one tree being

+1 in the limit as  goes to zero is

q =

p + 1  p

[p + 1  p] + [p + 1  p]

=

1

2

independent of p. At low temperature, high ,

q 

pde2d

pde2d + (1  p)de2d =

pd

pd + (1  p)d =

(cid:26) 0 p = 0

1 p = 1

.

353

q goes from a low probability of +1 for p below 1/2 to high probability of +1 for p above

1/2.

Now consider a very tall tree. If the p is the probability that a root has value +1,

we can iterate the formula for the height one tree and observe that at low temperature

the probability of the root being one converges to some value. At high temperature, the

probability of the root being one is 1/2 independent of p. At the phase transition, the slope

of q at p=1/2 is one. See Figure 9.11.

Now the slope of the probability of the root being 1 with respect to the probability of

a leaf being 1 in this height one tree is

q

p

=

D C

p  C D

D2

p

Since the slope of the function q(p) at p=1/2 when the phase transition occurs is one, we

can solve q

p = 1 for the value of  where the phase transition occurs. First, we show that

D

= 0.

p

(cid:12)

(cid:12)

(cid:12)p=

1

2

D = (cid:2)pe2 + 1  p(cid:3)d

p = d (cid:2)pe2 + 1  p(cid:3)d1 (cid:0)e2  1(cid:1) + d (cid:2)p + (1  p) e2(cid:3)d1 (cid:0)1  e2(cid:1)

+ (cid:2)p + (1  p) e2(cid:3)d

D

(cid:12)

(cid:12)

(cid:12)p=

= d

2d1

(cid:2)e2 + 1(cid:3)d1 (cid:0)e2  1(cid:1) + d

2d1

(cid:2)1 + e2(cid:3)d1 (cid:0)1  e2(cid:1) = 0

1

2

=

=

D C

p  C D

D2

p

2e2 + 1

d (cid:2) 1

(cid:2) 1

2e2 + 1

2

2

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)p=

1

2

=

C

p

D

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)p=

(cid:3)d1 (cid:0)e2  1(cid:1)

2 + 1

2e2(cid:3)d =

(cid:3)d + (cid:2) 1

d (cid:0)e2  1(cid:1)

1 + e2

d (cid:2)pe2 + 1  p(cid:3)d1 (cid:0)e2  1(cid:1)

[pe2 + 1  p]d + [p + (1  p) e2]d

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)p=

1

2

=

1

2

D

p

Then

q

p

(cid:12)

(cid:12)

(cid:12)

(cid:12)p=

1

2

Setting

And solving for  yields

d (cid:0)e2  1(cid:1)

1 + e2 = 1

d (cid:0)e2  1(cid:1) = 1 + e2

e2 = d+1

d1

 = 1

2 ln d+1

d1

To complete the argument, we need to show that q is a monotonic function of p. To see

354

1

high

temperature

Probability q(p) of

the root being 1

as a function of p

1/2

at phase transition

slope of q(p) equals 1

at p = 1/2

low

temperature

1/2

1

0

0

Probability p of a leaf being 1

Figure 9.11: Shape of q as a function of p for the height one tree and three values of 

corresponding to low temperature, the phase transition temperature, and high tempera-

ture.

this, write q = 1

1+

B

A

. A is a monotonically increasing function of p and B is monotonically

decreasing. From this it follows that q is monotonically increasing.

In the iteration going from p to q, we do not get the true marginal probabilities at

each level since we ignored the eect of the portion of the tree above. However, when we

get to the root, we do get the true marginal for the root. To get the true marginals for

the interior nodes we need to send messages down from the root.

Note: The joint probability distribution for the tree is of the form e

 (cid:80)

(ij)E)

xixj

= (cid:81)

exixj .

(i,j)E

Suppose x1 has value 1 with probability p. Then dene a function , called evidence, such

that

 (x1) =

(cid:26) p

for x1 = 1

1  p for x1 = 1

= (cid:0)p  1

2

(cid:1) x1 + 1

2

and multiply the joint probability function by . Note, however, that the marginal prob-

ability of x1 is not p. In fact, it may be further from p after multiplying the conditional

probability function by the function .

9.22 Bibliographic Notes

A formal denition of Topic Models described in this chapter as well as the LDA model

are from Blei, Ng and Jordan [BNJ03]; see also [Ble12]. Non-negative Matrix Factoriza-

tion has been used in several contexts, for example [DS03]. Anchor terms were dened

355

and used in [AGKM16]. Sections 9.7 - 9.9 are simplied versions of results from [BBK14].

Good introductions to hidden Markov models, graphical models, Bayesian networks,

and belief propagation appear in [Gha01, Bis06]. The use of Markov Random Fields

for computer vision originated in the work of Boykov, Veksler, and Zabih [BVZ98], and

further discussion appears in [Bis06]. Factor graphs and message-passing algorithms on

them were formalized as a general approach (incorporating a range of existing algorithms)

in [KFL01]. Message-passing in graphs with loops is discussed in [Wei97, WF01].

The use of belief propagation for maximum weighted matching is from [BSS08]. Sur-

vey propagation and warning propagation for nding satisfying assignments to k-CNF

formulas are described and analyzed in [MPZ02, BMZ05, ACORT11]. For additional

relevant papers and surveys, see [FD07, YFW01, YFW03, FK00].

356

9.23 Exercises

Exercise 9.1 Find a nonnegative factorization of the matrix













A =













5

6

4

3

1

2

7

7 10

6

4

8

6 10 11

Indicate the steps in your method and show the intermediate results.

Exercise 9.2 Find a nonnegative factorization of each of the following matrices.

(1)

(3)





















3

10 9 15 14 13

1

2

1

3

7 13 11 11

8

7

5 11 10

7

11

6

5 11

5

3

3

1

1

1

2

2

2

2









































3

3

3

1

4

4

4

3

13 16 13 10 5 13 14 10

15 24 21 12 9 21 18 12

6

7

1

1

4

5

3

3

7 15 10

2

4

2

6

3

7

6

6 12

16 15

4

4

8

7

12 12

6

1

4

3









































(2)

5 5 10 14 17

6

4

2 2

4

2

1 1

3

2

1 1

3 3

10

6

5 5 10 16 18

7

4

2 2

4

4

2

8

6





























(4)



3

9

4

9

1

4

1 1

3

12

9 9





6 6 12 16 15 15 4



1

4

3 3

4

9

3

3

3

Exercise 9.3 Consider the matrix A that is the product of nonnegative matrices B and

C.









12 22 41 35

19 20 13 48

11 14 16 29

14 16 14 36









=











10 1

9

1

4

3

6

2

(cid:18)1 2 4 3

2 2 1 5

(cid:19)







Which rows of A are approximate positive linear combinations of other rows of A?

Find an approxiamte nonnegative factorization of A

Exercise 9.4 Consider a set of vectors S in which no vector is a positive linear combi-

nation of the other vectors in the set. Given a set T containing S along with a number of

elements from the convex hull of S nd the vectors in S. Develop an ecient method to

nd S from T which does not require linear programming.

Hint: The points of S are vertices of the convex hull of T . The Euclidean length of a

vector is a convex function and so its maximum over a polytope is attained at one of the

vertices. Center the set and nd the the maximum length vector in T . This will be one

element of S.

357

Exercise 9.5 Dene the nonnegative column rank of a mn matrix A to be the minimum

number of vectors of in m space with the property that every column of A can be expressed

as a nonnegative linear combination of these vectors.

1. Show that the nonnegative column rank of A is at least the rank of A.

2. Construct a 3  n matrix whose nonnegative column rank is n.

[Hint: Take the

plane x + y = z = 1 in 3 space; draw a circle in the plane and take n points on

the circle.]

3. Show that the nonnegative column rank need not be the same as the nonnegative row

rank.

4. Read/look up a paper of Vavasis showing that the computation of nonnegative rank

is NP-hard.

Exercise 9.6 What happens to the Topic Modeling problem, when m the number of words

in a document goes to innity? Argue that the Idealized Topic Modeling problem of Section

9.2 is easy to solve when m goes to innity.

Exercise 9.7 Suppose y = (y1, y2, . . . , yr) is jointly distributed according to the Dirichlet

distribution with parameter  = 1/r. Show that the expected value of maxr

l=1yl is greater

than 0.1. [Hint: Lemma 9.6]

Exercise 9.8 Suppose there are s documents in a collection which are all nearly pure

I.e., in each of these documents, that topic has weight at least

for a particular topic.

1  . Suppose someone nds and hands to you these documents. Then their average is

an approximation to the topic vector. In terms of s, m and  compute an upper bound on

the error of approximation.

We could suggest at the start of Section 9.7 that they do the following exercise before

reading the section, so they get the intuition.

Exercise 9.9 Two topics and two words. Toy case of the Dominant Admixture Model.

Suppose in a topic model, there are just two words and two topics; word 1 is a key word

of topic 1 and word 2 is a key word of topic 2 in the sense:

b11  2b12

;

b21 

1

2

b22.

Suppose each document has one of the two topics as a dominant topic in the sense:

Max(c1j, c2j)  0.75.

Also suppose

|(A  BC)ij|  0.1 i, j.

Show that there are two real numbers 1 and 2 such that each document j with dominant

topic 1 has a1j  1 and a2j < 2 and each document j(cid:48) with dominant topic 2 has

a2j(cid:48)  2 and a1j(cid:48) <   1.

358

Exercise 9.10 What is the probability of heads occurring after a suciently long sequence

of transitions in Viterbi algorithm example of the most likely sequence of states?

Exercise 9.11 Find optimum parameters for a three state HMM and given output se-

quence. Note the HMM must have a strong signature in the output sequence or we prob-

ably will not be able to nd it. The following example may not be good for that

reason.

1

1

2

1

4

1

3

2

1

4

1

4

1

3

3

1

4

1

2

1

3

1

2

3

A B

1

3

4

4

1

4

1

3

3

4

2

3

1

2

3

Exercise 9.12 In the Ising model for a tree of degree one, a chain of vertices, is there a

phase transition where the correlation between the value at the root and the value at the

leaves becomes independent? Work out mathematical what happens.

Exercise 9.13 For a Boolean function in CNF the marginal probability gives the number

of satisable assignments with x1.

How does one obtain the number of satisfying assignments for a 2-CNF formula? Not

completely related to rst sentence.

359

10 Other Topics

10.1 Ranking and Social Choice

Combining feedback from multiple users to rank a collection of items is an important

task. We rank movies, restaurants, web pages, and many other items. Ranking has be-

come a multi-billion dollar industry as organizations try to raise the position of their web

pages in the results returned by search engines to relevant queries. Developing a method

of ranking that cannot be easily gamed by those involved is an important task.

A ranking of a collection of items is dened as a complete ordering. For every pair of

items a and b, either a is preferred to b or b is preferred to a. Furthermore, a ranking is

transitive in that a > b and b > c implies a > c.

One problem of interest in ranking is that of combining many individual rankings into

one global ranking. However, merging ranked lists in a meaningful way is nontrivial as

the following example illustrates.

Example: Suppose there are three individuals who rank items a, b, and c as illustrated

in the following table.

individual rst item second item third item

1

2

3

a

b

c

b

c

a

c

a

b

Suppose our algorithm tried to rank the items by rst comparing a to b and then

comparing b to c. In comparing a to b, two of the three individuals prefer a to b and thus

we conclude a is preferable to b. In comparing b to c, again two of the three individuals

prefer b to c and we conclude that b is preferable to c. Now by transitivity one would

expect that the individuals would prefer a to c, but such is not the case, only one of the

individuals prefers a to c and thus c is preferable to a. We come to the illogical conclusion

that a is preferable to b, b is preferable to c, and c is preferable to a.

Suppose there are a number of individuals or voters and a set of candidates to be

ranked. Each voter produces a ranked list of the candidates. From the set of ranked lists

can one construct a reasonable single ranking of the candidates? Assume the method of

producing a global ranking is required to satisfy the following three axioms.

Nondictatorship  The algorithm cannot always simply select one individuals ranking

to use as the global ranking.

Unanimity  If every individual prefers a to b, then the global ranking must prefer a to

b.

360

Independent of irrelevant alternatives  If individuals modify their rankings but

keep the order of a and b unchanged, then the global order of a and b should

not change.

Arrow showed that it is not possible to satisfy all three of the above axioms. We begin

with a technical lemma.

Lemma 10.1 For a set of rankings in which each individual ranks an item b either rst

or last (some individuals may rank b rst and others may rank b last), a global ranking

satisfying the above axioms must put b rst or last.

Proof: Let a, b, and c be distinct items. Suppose to the contrary that b is not rst or

last in the global ranking. Then there exist a and c where the global ranking puts a > b

and b > c. By transitivity, the global ranking puts a > c. Note that all individuals can

move c above a without aecting the order of b and a or the order of b and c since b

was rst or last on each list. Thus, by independence of irrelevant alternatives, the global

ranking would continue to rank a > b and b > c even if all individuals moved c above a

since that would not change the individuals relative order of a and b or the individuals

relative order of b and c. But then by unanimity, the global ranking would need to put

c > a, a contradiction. We conclude that the global ranking puts b rst or last.

Theorem 10.2 (Arrow) Any deterministic algorithm for creating a global ranking from

individual rankings of three or more elements in which the global ranking satises una-

nimity and independence of irrelevant alternatives is a dictatorship.

Proof: Let a, b, and c be distinct items. Consider a set of rankings in which every in-

dividual ranks b last. By unanimity, the global ranking must also rank b last. Let the

individuals, one by one, move b from bottom to top leaving the other rankings in place.

By unanimity, the global ranking must eventually move b from the bottom all the way to

the top. When b rst moves, it must move all the way to the top by Lemma 10.1.

Let v be the rst individual whose change causes the global ranking of b to change.

We argue that v is a dictator. First, we argue that v is a dictator for any pair ac not

involving b. We will refer to the three rankings of v in Figure 10.1. The rst ranking

of v is the ranking prior to v moving b from the bottom to the top and the second is

the ranking just after v has moved b to the top. Choose any pair ac where a is above

c in vs ranking. The third ranking of v is obtained by moving a above b in the second

ranking so that a > b > c in vs ranking. By independence of irrelevant alternatives,

the global ranking after v has switched to the third ranking puts a > b since all indi-

vidual ab votes are the same as in the rst ranking, where the global ranking placed

a > b. Similarly b > c in the global ranking since all individual bc votes are the same

as in the second ranking, in which b was at the top of the global ranking. By transitiv-

ity the global ranking must put a > c and thus the global ranking of a and c agrees with v.

361

b

b

...

a

...

c

...

b

v

...

a

...

...

...

b

global

b

b

b

b

b

...

a

...

c

...

v

b

b

b

...

...

...

c

...

global

b

b a

b

...

c

...

...

v

b

b

a

b

...

c

...

...

global

rst ranking

second ranking

third ranking

Figure 10.1: The three rankings that are used in the proof of Theorem 10.2.

Now all individuals except v can modify their rankings arbitrarily while leaving b in its

extreme position and by independence of irrelevant alternatives, this does not aect the

global ranking of a > b or of b > c. Thus, by transitivity this does not aect the global

ranking of a and c. Next, all individuals except v can move b to any position without

aecting the global ranking of a and c.

At this point we have argued that independent of other individuals rankings, the

global ranking of a and c will agree with vs ranking. Now v can change its ranking

arbitrarily, provided it maintains the order of a and c, and by independence of irrelevant

alternatives the global ranking of a and c will not change and hence will agree with v.

Thus, we conclude that for all a and c, the global ranking agrees with v independent of

the other rankings except for the placement of b. But other rankings can move b without

changing the global order of other elements. Thus, v is a dictator for the ranking of any

pair of elements not involving b.

Note that v changed the relative order of a and b in the global ranking when it moved

b from the bottom to the top in the previous argument. We will use this in a moment.

The individual v is also a dictator over every pair ab. Repeat the construction showing

that v is a dictator for every pair ac not involving b only this time place c at the bottom.

There must be an individual vc who is a dictator for any pair such as ab not involving c.

Since both v and vc can aect the global ranking of a and b independent of each other, it

must be that vc is actually v. Thus, the global ranking agrees with v no matter how the

other voters modify their rankings.

10.1.1 Randomization

An interesting randomized algorithm that satises unanimity and independence of irrel-

evant alternatives is to pick a random individual and use that individuals ranking as

362

the output. This is called the random dictator rule because it is a randomization over

dictatorships. An analogous scheme in the context of voting would be to select a winner

with probability proportional to the number of votes for that candidate, because this is

the same as selecting a random voter and telling that voter to determine the winner. Note

that this method has the appealing property that as a voter, there is never any reason

to strategize, e.g., voting for candidate a rather than your preferred candidate b because

you think b is unlikely to win and you dont want to throw away your vote. With this

method, you should always vote for your preferred candidate.

10.1.2 Examples

Borda Count: Suppose we view each individuals ranking as giving each item a score:

putting an item in last place gives it one point, putting it in second-to-last place gives it

two points, third-to-last place is three points, and so on. In this case, one simple way to

combine rankings is to sum up the total number of points received by each item and then

sort by total points. This is called the extended Borda Count method.

Lets examine which axioms are satised by this approach. It is easy to see that it

is a nondictatorship. It also satises unanimity: if every individual prefers a to b, then

every individual gives more points to a than to b, and so a will receive a higher total than

b. By Arrows theorem, the approach must fail independence of irrelevant alternatives,

and indeed this is the case. Here is a simple example with three voters and four items

{a, b, c, d} where the independence of irrelevant alternatives axiom fails:

individual

1

2

3

ranking

abcd

abcd

bacd

In this example, a receives 11 points and is ranked rst, b receives 10 points and is ranked

second, c receives 6 points and is ranked third, and d receives 3 points and is ranked

fourth. However, if individual 3 changes his ranking to bcda, then this reduces the total

number of points received by a to 9, and so b is now ranked rst overall. Thus, even

though individual 3s relative order of b and a did not change, and indeed no individuals

relative order of b and a changed, the global order of b and a did change.

Hare voting: An interesting system for voting is to have everyone vote for their fa-

vorite candidate. If some candidate receives a majority of the votes, he or she is declared

the winner. If no candidate receives a majority of votes, the candidate with the fewest

votes is dropped from the slate and the process is repeated.

The Hare system implements this method by asking each voter to rank all the can-

didates. Then one counts how many voters ranked each candidate as number one. If no

candidate receives a majority, the candidate with the fewest number one votes is dropped

363

from each voters ranking. If the dropped candidate was number one on some voters list,

then the number two candidate becomes that voters number one choice. The process of

counting the number one rankings is then repeated.

We can convert the Hare voting system into a ranking method in the following way.

Whichever candidate is dropped rst is put in last place, whichever is dropped second is

put in second-to-last place, and so on, until the system selects a winner, which is put in

rst place. The candidates remaining, if any, are placed between the rst-place candidate

and the candidates who were dropped, in an order determined by running this procedure

recursively on just those remaining candidates.

As with Borda Count, the Hare system also fails to satisfy independence of irrelevant

alternatives. Consider the following situation in which there are 21 voters that fall into

four categories. Voters within a category rank individuals in the same order.

Category

Number of voters

in category

Preference order

1

2

3

4

7

6

5

3

abcd

bacd

cbad

dcba

The Hare system would rst eliminate d since d gets only three rank one votes. Then

it would eliminate b since b gets only six rank one votes whereas a gets seven and c gets

eight. At this point a is declared the winner since a has thirteen votes to cs eight votes.

So, the nal ranking is acbd.

Now assume that Category 4 voters who prefer b to a move b up to rst place. This

keeps their order of a and b unchanged, but it reverses the global order of a and b. In

particular, d is rst eliminated since it gets no rank one votes. Then c with ve votes is

eliminated. Finally, b is declared the winner with 14 votes, so the nal ranking is bacd.

Interestingly, Category 4 voters who dislike a and have ranked a last could prevent a

from winning by moving a up to rst. Ironically this results in eliminating d, then c, with

ve votes and declaring b the winner with 11 votes. Note that by moving a up, category

4 voters were able to deny a the election and get b to win, whom they prefer over a.

10.2 Compressed Sensing and Sparse Vectors

Dene a signal to be a vector x of length d, and dene a measurement of x to be a dot-

product of x with some known vector ai. If we wish to uniquely reconstruct x without

any assumptions, then d linearly-independent measurements are necessary and sucient.

364

d

A

n

=

b

x

Figure 10.2: Ax = b has a vector space of solutions but possibly only one sparse

solution. If the columns of A are unit length vectors that are pairwise nearly orthogonal,

then the system has a unique sparse solution.

Given b = Ax where A is known and invertible, we can reconstruct x as x = A1b. In

the case where there are fewer than d independent measurements and the rank of A is less

than d, there will be multiple solutions. However, if we knew that x is sparse with s (cid:28) d

nonzero elements, then we might be able to reconstruct x with far fewer measurements

using a matrix A with n (cid:28) d rows. See Figure 10.2.

In particular, it turns out that

a matrix A whose columns are nearly orthogonal, such as a matrix of random Gaussian

entries, will be especially well-suited to this task. This is the idea of compressed sensing.

Note that we cannot make the columns of A be completely orthogonal since A has more

columns than rows.

Compressed sensing has found many applications, including reducing the number of

sensors needed in photography, using the fact that images tend to be sparse in the wavelet

domain, and in speeding up magnetic resonance imaging in medicine.

10.2.1 Unique Reconstruction of a Sparse Vector

A vector is said to be s-sparse if it has at most s nonzero elements. Let x be a d-

dimensional, s-sparse vector with s (cid:28) d. Consider solving Ax = b for x where A is an

n  d matrix with n < d. The set of solutions to Ax = b is a subspace. However, if we

restrict ourselves to sparse solutions, under certain conditions on A there is a unique s-

sparse solution. Suppose that there were two s-sparse solutions, x1 and x2. Then x1  x2

would be a 2s-sparse solution to the homogeneous system Ax = 0. A 2s-sparse solution to

the homogeneous equation Ax = 0 requires that some 2s columns of A be linearly depen-

dent. Unless A has 2s linearly dependent columns there can be only one s-sparse solution.

The solution to the reconstruction problem is simple. If the matrix A has at least 2s

365

1-norm solution

 2-norm solution

Figure 10.3: Illustration of minimum 1-norm and 2-norm solutions.

rows and the entries of A were selected at random from a standard Gaussian, then with

probability one, no set of 2s columns will be linearly dependent. We can see this by not-

ing that if we rst x a subset of 2s columns and then choose the entries at random, the

probability that this specic subset is linearly dependent is the same as the probability

that 2s random Gaussian vectors in a 2s-dimensional space are linearly dependent, which

is zero.42 So, taking the union bound over all (cid:0) d

(cid:1) subsets, the probability that any one

of them is linearly dependent is zero.

2s

The above argument shows that if we choose n = 2s and pick entries of A randomly

from a Gaussian, with probability one there will be a unique s-sparse solution. Thus,

(cid:1) possible locations for the nonzero elements in x and

to solve for x we could try all (cid:0)d

aim to solve Ax = b over just those s columns of A: any one of these that gives a

solution will be the correct answer. However, this takes time (ds) which is exponential

in s. We turn next to the topic of ecient algorithms, describing a polynomial-time

optimization procedure that will nd the desired solution when n is suciently large and

A is constructed appropriately.

s

10.2.2 Eciently Finding the Unique Sparse Solution

To nd a sparse solution to Ax = b, one would like to minimize the zero norm (cid:107)x(cid:107)0

over {x|Ax = b}, i.e., minimize the number of nonzero entries. This is a computationally

hard problem. There are techniques to minimize a convex function over a convex set, but

||x||0 is not a convex function, and with no further assumptions, it is NP-hard. With this

in mind, we use the one-norm as a proxy for the zero-norm and minimize the one-norm

(cid:107)x(cid:107)1 = (cid:80)

i |xi| over {x|Ax = b}. Although this problem appears to be nonlinear, it can

be solved by linear programming by writing x = u  v, u  0, and v  0, and minimizing

the linear function (cid:80)

vi subject to Au-Av=b, u  0, and v  0.

ui + (cid:80)

i

i

42This can be seen by selecting the vectors one at a time. The probability that the ith new vector lies

fully in the lower dimensional subspace spanned by the previous i  1 vectors is zero, and so by the union

bound the overall probability is zero.

366

2s, 1

We now show if the columns of the n by d matrix A are unit length almost orthogo-

nal vectors with pairwise dot products in the range ( 1

2s) that minimizing (cid:107)x(cid:107)1 over

{x|Ax = b} recovers the unique s-sparse solution to Ax=b. The ijth element of the ma-

trix AT A is the cosine of the angle between the ith and jth columns of A. If the columns

of A are unit length and almost orthogonal, AT A will have ones on its diagonal and all

o diagonal elements will be small. By Theorem 2.8, if A has n = s2 log d rows and each

column is a random unit-length n-dimensional vector, with high probability all pairwise

dot-products will have magnitude less than 1

2s as desired.43 Here, we use s2 log d, a larger

value of n compared to the existence argument in Section 10.2.1, but now the algorithm

is computationally ecient.

Let x0 denote the unique s-sparse solution to Ax = b and let x1 be a solution of

smallest possible one-norm. Let z = x1  x0. We now prove that z = 0 implying that

x1 = x0. First, Az = Ax1  Ax0 = b  b = 0. This implies that AT Az = 0. Since each

column of A is unit length, the matrix AT A has ones on its diagonal. Since every pair of

2s , 1

distinct columns of A has dot-product in the range ( 1

2s), each o-diagonal entry in

AT A is in the range ( 1

2s). These two facts imply that unless z = 0, every entry in z

must have absolute value less than 1

2s ||z||1. If the jth entry in z had absolute value greater

than or equal to 1

2s||z||1, it would not be possible for the jth entry of AT Az to equal 0

unless ||z||1 = 0.

2s , 1

jS |zj|  1

Finally let S denote the support of x0, where |S|  s. We now argue that z must

have at least half of its (cid:96)1 norm inside of S, i.e., (cid:80)

2||z||1. This will complete

the argument because it implies that the average value of |zj| for j  S is at least 1

2s ||z||1,

which as shown above is only possible if ||z||1 = 0. Let tin denote the sum of the absolute

values of the entries of x1 in the set S, and let tout denote the sum of the absolute values

of the entries of x1 outside of S. So, tin + tout = ||x1||1. Let t0 be the one-norm of x0.

Since x1 is the minimum one norm solution, t0  tin + tout, or equivalently t0  tin  tout.

But (cid:80)

j(cid:54)S |zj|, or

equivalently, (cid:80)

2||z||1, which as noted above implies that ||z||1 = 0, as desired.

j(cid:54)S |zj| = tout. This implies that (cid:80)

jS |zj|  t0  tin and (cid:80)

jS |zj|  (cid:80)

jS |zj|  1

To summarize, we have shown the following theorem and corollary.

Theorem 10.3 If matrix A has unit-length columns a1, . . . , ad and the property that

|ai  aj| < 1

2s for all i (cid:54)= j, then if the equation Ax = b has a solution with at most s

nonzero coordinates, this solution is the unique minimum 1-norm solution to Ax = b.

Corollary 10.4 For some absolute constant c, if A has n rows for n  cs2 log d and each

column of A is chosen to be a random unit-length n-dimensional vector, then with high

probability A satises the conditions of Theorem 10.3 and therefore if the equation Ax = b

has a solution with at most s nonzero coordinates, this solution is the unique minimum

1-norm solution to Ax = b.

43Note that the roles of n and d are reversed here compared to Theorem 2.8.

367

position on genome

trees

=

Phenotype; outward

manifestation, observables

Genotype:

internal code

Figure 10.4: The system of linear equations used to nd the internal code for some

observable phenomenon.

The condition of Theorem 10.3 is often called incoherence of the matrix A. Other more

involved arguments show that it is possible to recover the sparse solution using one-norm

minimization for a number of rows n as small as O(s log(ds)).

10.3 Applications

10.3.1 Biological

There are many areas where linear systems arise in which a sparse solution is unique.

One is in plant breeding. Consider a breeder who has a number of apple trees and for

each tree observes the strength of some desirable feature. He wishes to determine which

genes are responsible for the feature so he can crossbreed to obtain a tree that better

expresses the desirable feature. This gives rise to a set of equations Ax = b where each

row of the matrix A corresponds to a tree and each column to a position on the genone.

See Figure 10.4. The vector b corresponds to the strength of the desired feature in each

tree. The solution x tells us the position on the genone corresponding to the genes that

account for the feature. It would be surprising if there were two small independent sets

of genes that accounted for the desired feature. Thus, the matrix should have a property

that allows only one sparse solution.

368

10.3.2 Low Rank Matrices

Suppose L is a low rank matrix that has been corrupted by noise. That is, A = L + R.

If the R is Gaussian, then principal component analysis will recover L from A. However,

if L has been corrupted by several missing entries or several entries have a large noise

added to them and they become outliers, then principal component analysis may be far

o. However, if L is low rank and R is sparse, then L can be recovered eectively from

L+R. To do this, nd the L and R that minimize (cid:107)L(cid:107) + (cid:107)R(cid:107)1.44 Here the nuclear norm

(cid:107)L(cid:107) is the 1-norm of the vector of singular values of L and ||R||1 is the entrywise 1-norm

(cid:80)

ij |rij|. A small value of (cid:107)L(cid:107) indicates a sparse vector of singular values and hence a

low rank matrix. Minimizing (cid:107)L(cid:107) + (cid:107)R(cid:107)1 subject to L + R = A is a complex problem

and there has been much work on it. The reader is referred to Add references

Notice that we do not need to know the rank of L or the elements that were corrupted.

All we need is that the low rank matrix L is not sparse and that the sparse matrix R is

not low rank. We leave the proof as an exercise.

If A is a small matrix one method to nd L and R by minimizing ||L|| + ||R||1 is to

nd the singular value decomposition A = U V T and minimize ||||1 + ||R||1 subject to

A = L + R and U V T being the singular value decomposition of A. This can be done

using Lagrange multipliers (??). Write R = R+ + R where R+  0 and R  0. Let

f (i, rij) =

n

(cid:88)

i=1

i +

(cid:88)

r+

ij +

(cid:88)

r

ij.

ij

ij

Write the Lagrange formula

l = f (i, rij) + iigi

where the gi are the required constraints

1. r+

ij  0

2. r

ij  0

3. i  0

4. aij = lij + rij

5. uT

i uj =

6. vT

i vj =

(cid:26) 1 i = j

0 i (cid:54)= j

(cid:26) 1 i = j

0 i (cid:54)= j

7. li = (cid:80) uiivT

j

44To minimize the absolute value of x write x = u  v and using linear programming minimize u + v

subject to u  0 and v  0.

369

Conditions (5) and (6) insure that U V T is the svd of some matrix. The solution is

obtained when (l) = 0 which can be found by gradient descent using 2(l).

An example where low rank matrices that have been corrupted might occur is aerial

photographs of an intersection. Given a long sequence of such photographs, they will be

the same except for cars and people. If each photo is converted to a vector and the vector

used to make a column of a matrix, then the matrix will be low rank corrupted by the

trac. Finding the original low rank matrix will separate the cars and people from the

back ground.

10.4 An Uncertainty Principle

Given a function x(t), one can represent the function by the composition of sinusoidal

functions. Basically one is representing the time function by its frequency components.

The transformation from the time representation of a function to it frequency represen-

tation is accomplished by a Fourier transform. The Fourier transform of a function x(t)

is given by

(cid:90)

f () =

x(t)e2tdt

Converting the frequency representation back to the time representation is done by the

inverse Fourier transformation

(cid:90)

x(t) =

f ()e2td

In the discrete case, x = [x0, x1, . . . , xn1] and f = [f0, f1, . . . , fn1]. The Fourier trans-

n ij where  is the principal nth root of unity. The inverse

form is f = Ax with aij = 1

transform is x = Bf where B = A1 has the simple form bij = 1

n ij.

There are many other transforms such as the Laplace, wavelets, chirplets, etc. In fact,

any nonsingular n  n matrix can be used as a transform.

10.4.1 Sparse Vector in Some Coordinate Basis

Consider Ax = b where A is a square n  n matrix. The vectors x and b can be con-

sidered as two representations of the same quantity. For example, x might be a discrete

time sequence, b the frequency spectrum of x, and the matrix A the Fourier transform.

The quantity x can be represented in the time domain by x and in the frequency domain

by its Fourier transform b.

Any orthonormal matrix can be thought of as a transformation and there are many

important transformations other than the Fourier transformation. Consider a transfor-

mation A and a signal x in some standard representation. Then y = Ax transforms

the signal x to another representation y. If A spreads any sparse signal x out so that

370

the information contained in each coordinate in the standard basis is spread out to all

coordinates in the second basis, then the two representations are said to be incoherent.

A signal and its Fourier transform are one example of incoherent vectors. This suggests

that if x is sparse, only a few randomly selected coordinates of its Fourier transform are

needed to reconstruct x. Below, we show that a signal cannot be too sparse in both its

time domain and its frequency domain.

10.4.2 A Representation Cannot be Sparse in Both Time and Frequency

Domains

There is an uncertainty principle that states that a time signal cannot be sparse in

both the time domain and the frequency domain. If the signal is of length n, then the

product of the number of nonzero coordinates in the time domain and the number of

nonzero coordinates in the frequency domain must be at least n. This is the mathemati-

cal version of Heisenbergs uncertainty principle. Before proving the uncertainty principle

we rst prove a technical lemma.

In dealing with the Fourier transform it is convenient for indices to run from 0 to n  1

rather than from 1 to n. Let x0, x1, . . . , xn1 be a sequence and let f0, f1, . . . , fn1 be its

discrete Fourier transform. Let i =



1. Then fj = 1

n

xke

2i

n jk,

n1

(cid:80)

k=0

j = 0, . . . , n  1.

In matrix form f = Zx where zjk = e

2i

n jk.











f0

f1

...

fn1











=

1



n













1

1

...

1 e

1

2i

e

n

...

2i

n (n  1) e

e

1

2i

n 2

...

2i

n 2 (n  1)

 

 

e

 

e

1

2i

n (n  1)

...

n (n  1)2

2i

































x0

x1

...

xn1

If some of the elements of x are zero, delete the zero elements of x and the corresponding

columns of the matrix. To maintain a square matrix, let nx be the number of nonzero

elements in x and select nx consecutive rows of the matrix. Normalize the columns of the

resulting submatrix by dividing each element in a column by the column element in the

rst row. The resulting submatrix is a Vandermonde matrix that looks like









1

a

a2

a3

1

b

b2

b3

1

1

d

c

c2 d2

c3 d3









and is nonsingular.

Lemma 10.5 If x0, x1, . . . , xn1 has nx nonzero elements, then f0, f1, . . . , fn1 cannot

have nx consecutive zeros.

371





























1

3

1

1

1

1

z3 z6 1

1

1

1

1 1

z2 z3 z4 z5 z6 z7 z8

1 z

z3 z5 z7

1 z2 z4 z6 z8 z

z3 z6

1 z3 z6 1

z5

1 z4 z8 z3 z7 z2 z6 z

z6 z2 z7 z3 z8 z4

1 z5 z

z6 z3

1 z6 z3 1

z8 z6 z4 z2

1 z7 z5 z3 z

1 z8 z7 z6 z5 z4 z3 z2 z

z6 z3 1





















































































1

0

0

1

0

0

1

0

0

=

1

3





























3

1 + z3 + z6

1 + z6 + z3

3

1 + z3 + z6

1 + z6 + z3

3

1 + z3 + z6

1 + z6 + z3





















































































1

0

0

1

0

0

1

0

0

=

Figure 10.5: The transform of the sequence 100100100.

Proof: Let i1, i2, . . . , inx be the indices of the nonzero elements of x. Then the elements

of the Fourier transform in the range k = m + 1, m + 2, . . . , m + nx are

fk = 1

n

2i

n kij

xij e

nx(cid:80)

j=1



1 and the multiplication of the exponent by ij to account for the

Note the use of i as

actual location of the element in the sequence. Normally, if every element in the sequence

was included, we would just multiply by the index of summation.

Convert the equation to matrix form by dening zkj = 1

n kij) and write

f = Zx where now x is the vector consisting of the nonzero elements of the original x. By

its denition, x (cid:54)= 0. To prove the lemma we need to show that f is nonzero. This will be

true provided Z is nonsingular since x = Z 1f . If we rescale Z by dividing each column

by its leading entry we get the Vandermonde determinant which is nonsingular.

n exp( 2i

Theorem 10.6 Let nx be the number of nonzero elements in x and let nf be the number

of nonzero elements in the Fourier transform of x. Let nx divide n. Then nxnf  n.

Proof: If x has nx nonzero elements, f cannot have a consecutive block of nx zeros. Since

nx divides n there are n

blocks each containing at least one nonzero element. Thus, the

nx

product of nonzero elements in x and f is at least n.

The Fourier transform of spikes proves that above bound is tight

To show that the bound in Theorem 10.6 is tight we show that the Fourier transform

of the sequence of length n consisting of

n  1 zeros,

is the sequence itself. For example, the Fourier transform of the sequence 100100100 is

100100100. Thus, for this class of sequences, nxnf = n.

n ones, each one separated by





Theorem 10.7 Let S (

apart. The Fourier transform of S (

n,



n,



n) be the sequence of 1s and 0s with





n) is itself.

372



n 1s spaced



n















Proof: Consider the columns 0,

which S (

k

S (

n, 0  k <





n) equals

n and the 1/

n,

n,



n, . . . , (

n) has value 1. The element of the matrix Z in the row j



n. These are the columns for

n of column

n is znkj = 1. Thus, the product of these rows of Z times the vector

n  1)

n, 2



n normalization yields fj







For rows whose index is not of the form j



n, 2

n1)b(cid:17)

the elements in row b in the columns 0,

(cid:16)

and thus fb = 1

n



1 + zb + z2b    + z(

n, the row b, b (cid:54)= j





n, . . . , (

= 1

z

n

n  1)

nb1

zb1 = 0 since zb





n = 1.







n, j  {0,

n are 1, zb, z2b, . . . , z(

n, . . . ,

n  1},

n1)b

n = 1 and zb (cid:54)= 1.

ms better suited to perhaps a homework question

10.5 Gradient

, f (x0)

x2

The gradient of a function f (x) of d variables, x = (x1, x2, . . . , xd), at a point x0 is

denoted (cid:53)f (x0). It is a d-dimensional vector with components f (x0)

, . . . , f (x0)

,

xd

x1

where f

are partial derivatives. Without explicitly stating, we assume that the deriva-

xi

tives referred to exist. The rate of increase of the function f as we move from x0 in a

direction u is (cid:53)f (x0)  u. So the direction of steepest descent is (cid:53)f (x0); this is a nat-

ural direction to move to minimize f . But by how much should we move? A large move

may overshoot the minimum. See Figure 10.6. A simple x is to minimize f on the line

from x0 in the direction of steepest descent by solving a one dimensional minimization

problem. This gives us the next iterate x1 and we repeat. We do not discuss the issue

of step-size any further. Instead, we focus on innitesimal gradient descent, where, the

algorithm makes innitesimal moves in the (cid:53)f (x0) direction. Whenever (cid:53)f is not the

zero vector, we strictly decrease the function in the direction (cid:53)f , so the current point

is not a minimum of the function f . Conversely, a point x where (cid:53)f = 0 is called a

rst-order local optimum of f . A rst-order local optimum may be a local minimum, local

maximum, or a saddle point. We ignore saddle points since numerical error is likely to

In general, local minima do not

prevent gradient descent from stoping at a saddle point.

have to be global minima, see Figure 10.6, and gradient descent may converge to a local

minimum that is not a global minimum. When the function f is convex, this is not the

case.

A function f of a single variable x is said to be convex if for any two points a and b,

the line joining f (a) and f (b) is above the curve f (). A function of many variables is

convex if on any line segment in its domain, it acts as a convex function of one variable

on the line segment.

Denition 10.1 A function f over a convex domain is a convex function if for any two

points x and y in the domain, and any  in [0, 1] we have

f (x + (1  )y)  f (x) + (1  )f (y).

The function is concave if the inequality is satised with  instead of .

373

Figure 10.6: Gradient descent overshooting minimum

Theorem 10.8 Suppose f is a convex, dierentiable function dened on a closed bounded

convex domain. Then any rst-order local minimum is also a global minimum. Innites-

imal gradient descent always reaches the global minimum.

Proof: We will prove that if x is a local minimum, then it must be a global minimum.

If not, consider a global minimum point y (cid:54)= x. On the line joining x and y, the function

must not go above the line joining f (x) and f (y). This means for an innitesimal  > 0,

moving distance  from x towards y, the function must decrease, so (cid:53)f (x) is not 0,

contradicting the assumption that x is a local minimum.

The second derivatives

2

xixj

form a matrix, called the Hessian, denoted H(f (x)).

The Hessian of f at x is a symmetric d  d matrix with ijth entry 2f

(x). The second

xixj

derivative of f at x in the direction u is the rate of change of the rst derivative in the

direction u from x. It is easy to see that it equals

To see this, note that the second derivative of f along the unit vector u is

uT H(f (x))u.

(cid:88)

uj



xj

((cid:53)f (x)  u) =

(cid:88)

uj

(cid:88)

j

i



xj

(cid:19)

(cid:18)

ui

f (x)

xi

j

=

ujui

2f (x)

xjxi

.

(cid:88)

j,i

Theorem 10.9 Suppose f is a function from a closed convex domain D in Rd to the

reals and the Hessian of f exists everywhere in D. Then f is convex (concave) on D if

and only if the Hessian of f is positive (negative) semi-denite everywhere on D.

Gradient descent requires the gradient to exist. But, even if the gradient is not always

dened, one can minimize a convex function over a convex domain eciently, i.e., in

polynomial time. Technically, one can only nd an approximate minimum and the time

374

depends on the error parameter as well as the presentation of the convex set. We do not

go into these details. But, in principle we can minimize a convex function over a convex

domain. We can also maximize a concave function over a concave domain. However, in

general, we do not have ecient procedures to maximize a convex function over a convex

domain. It is easy to see that at a rst-order local minimum of a possibly non-convex

function, the gradient vanishes. But second-order local decrease of the function may be

possible. The steepest second-order decrease is in the direction of v, where, v is the

eigenvector of the Hessian corresponding to the largest absolute valued eigenvalue.

10.6 Linear Programming

Linear programming is an optimization problem that has been carefully studied and is

immensely useful. We consider linear programming problem in the following form where

A is an m  n matrix, m  n, of rank m, c is 1  n, b is m  1, and x is n  1 :

max c  x

subject to Ax = b, x  0.

Inequality constraints can be converted to this form by adding slack variables. Also, we

can do Gaussian elimination on A and if it does not have rank m, we either nd that

the system of equations has no solution, whence we may stop or we can nd and discard

redundant equations. After this preprocessing, we may assume that A s rows are inde-

pendent.

The simplex algorithm is a classical method to solve linear programming problems. It

is a vast subject and is well discussed in many texts. Here, we will discuss the ellipsoid

algorithm which is in a sense based more on continuous mathematics and is closer to the

spirit of this book.

10.6.1 The Ellipsoid Algorithm

The rst polynomial time algorithm for linear programming45 was developed by Khachiyan

based on work of Iudin, Nemirovsky and Shor and is called the ellipsoid algorithm. The

algorithm is best stated for the seemingly simpler problem of determining whether there

is a solution to Ax  b and if so nding one. The ellipsoid algorithm starts with a large

ball in d-space which is guaranteed to contain the polyhedron Ax  b. Even though we

do not yet know if the polyhedron is empty or non-empty, such a ball can be found. The

algorithm checks if the center of the ball is in the polyhedron, if it is, we have achieved our

objective. If not, we know from the Separating Hyperplane Theorem of convex geometry

that there is a hyperplane called the separating hyperplane through the center of the ball

45Although there are examples where the simplex algorithm requires exponential time, it was shown

by Shanghua Teng and Dan Spielman that the expected running time of the simplex algorithm on an

instance produced by taking an arbitrary instance and then adding small Gaussian perturbations to it is

polynomial.

375

Figure 10.7: Ellipsoid Algorithm

such that the whole polytope lies in one of the half spaces.

We then nd an ellipsoid which contains the ball intersected with this half-space. See

Figure 10.7. The ellipsoid is guaranteed to contain Ax  b as was the ball earlier. If the

center of the ellipsoid does not satisfy the inequalities, then again there is a separating

hyper plane and we repeat the process. After a suitable number of steps, either we nd a

solution to the original Ax  b or we end up with a very small ellipsoid. If the original A

and b had integer entries, one can ensure that the set Ax  b, after a slight perturbation

which preserves its emptiness/non-emptiness, has a volume of at least some (cid:15) > 0. If our

ellipsoid has shrunk to a volume of less than this (cid:15), then there is no solution. Clearly

this must happen within log V0/(cid:15) = O(V0d/(cid:15)) steps, where V0 is an upper bound on the

initial volume and  is the factor by which the volume shrinks in each step. We do not

go into details of how to get a value for V0, but the important points are that (i) only the

logarithm of V0 appears in the bound on the number of steps, and (ii) the dependence on

d is linear. These features ensure a polynomial time algorithm.

The main diculty in proving fast convergence is to show that the volume of the

ellipsoid shrinks by a certain factor in each step. Thus, the question can be phrased as

suppose E is an ellipsoid with center x0 and consider the half-ellipsoid E(cid:48) dened by

E(cid:48) = {x|x  E, a  (x  x0)  0}

where a is some unit length vector. Let E be the smallest volume ellipsoid containing E(cid:48).

376

Ellipsoid containing half-sphereSeparating hyperplanepolytope

Show that

Vol( E)

Vol(E)

 1  

for some  > 0. A sequence of geometric reductions transforms this into a simple problem.

Translate and then rotate the coordinate system so that x0 = 0 and a = (1, 0, 0, . . . , 0).

Finally, apply a nonsingular linear transformation  so that  E = B = {x| |x| = 1}, the

unit sphere. The important point is that a nonsingular linear transformation  multiplies

the volumes of all sets by |det( )|, so that Vol( E)

Vol( (E)) . The following lemma answers

the question raised.

Vol(E) = Vol( ( E))

Lemma 10.10 Consider the half-sphere B(cid:48) = {x|x1  0,

ellipsoid E contains B(cid:48):

|x|  1}. The following

(cid:40)

E =

x

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:18) d + 1

d

(cid:19)2 (cid:18)

x1 

1

d + 1

(cid:19)2

+

(cid:18) d2  1

d2

(cid:19) (cid:18)

2 + x2

x2

3 + . . . + x2

d

(cid:19)

(cid:41)

 1

.

Further,

Vol( E)

Vol(B)

=

(cid:18) d

(cid:19) (cid:18) d2

(cid:19)(d1)/2

d + 1

d2  1

 1 

1

4d

.

The proof is left as an exercise (Exercise 10.27).

10.7 Integer Optimization

The problem of maximizing a linear function subject to linear inequality constraints,

but with the variables constrained to be integers is called integer programming.

Max c  x

subject to Ax  b with xi integers

This problem is NP-hard. One way to handle the hardness is to relax the integer con-

straints, solve the linear program in polynomial time, and round the fractional values to

integers. The simplest rounding, round each variable which is 1/2 or more to 1, the rest

to 0, yields sensible results in some cases. The vertex cover problem is one of them. The

problem is to choose a subset of vertices so that each edge is covered with at least one of

its end points in the subset. The integer program is:

(cid:88)

Min

i

xi

subject to xi + xj  1  edges (i, j); xi integers .

Solve the linear program. At least one variable for each edge must be at least 1/2 and

the simple rounding converts it to one. The integer solution is still feasible. It clearly

at most doubles the objective function from the linear programming solution and since

the LP solution value is at most the optimal integer programming solution value, we are

within a factor of two of the optimal.

377

10.8 Semi-Denite Programming

Semi-denite programs are special cases of convex programs. Recall that an n  n ma-

trix A is positive semi-denite if and only if A is symmetric and for all x  Rn, xT Ax  0.

There are many equivalent characterizations of positive semi-denite matrices. We men-

tion one. A symmetric matrix A is positive semi-denite if and only if it can be expressed

as A = BBT for a possibly rectangular matrix B.

A semi-denite program (SDP) is the problem of minimizing a linear function cT x

subject to a constraint that F = F0 + F1x1 + F2x2 +    + Fdxd is positive semi-denite.

Here F0, F1, . . . , Fd are given symmetric matrices.

This is a convex program since the set of x satisfying the constraint is a convex

set. To see this, note that if F (x) = F0 + F1x1 + F2x2 +    + Fdxd and F (y) =

F0 + F1y1 + F2y2 +    + Fdyd are positive semi-denite, then so is F (cid:0)x + (1  )y(cid:1)

for 0    1.

It turns out

that there are more ecient algorithms for SDPs than general convex programs and that

many interesting problems can be formulated as SDPs. We discuss the latter aspect here.

In principle, SDPs can be solved in polynomial time.

Linear programs are special cases of SDPs. For any vector v, let diag(v) denote a

diagonal matrix with the components of v on the diagonal. Then it is easy to see that

the constraints v  0 are equivalent to the constraint diag(v) is positive semi-denite.

Consider the linear program:

Minimize cT x subject to Ax = b; x  0.

Rewrite Ax = b as Ax  b  0 and b  Ax  0 and use the idea of diagonal matrices

above to formulate this as an SDP.

A second interesting example is that of quadratic programs of the form:

Minimize

(cT x)2

dT x

subject to Ax + b  0.

This is equivalent to

Minimize t subject to Ax + b  0 and t 

(cT x)2

dT x .

This is in turn equivalent to the SDP

Minimize t subject to the following matrix being positive semi-denite:





diag(Ax + b)

0

0

0

t

0

cT x

cT x dT x



 .

378

Application to approximation algorithms.

An exciting area of application of SDP is in nding near-optimal solutions to some

integer problems. The central idea is best illustrated by its early application in a break-

through due to Goemans and Williamson [GW95] for the maximum cut problem which

given a graph G(V, E) asks for the cut S, S maximizing the number of edges going across

the cut from S to S. For each i  V , let xi be an integer variable assuming values 1

depending on whether i  S or i  S respectively. Then the max-cut problem can be

posed as

Maximize (cid:80)

(i,j)E

(1  xixj) subject to the constraints xi  {1, +1}.

The integrality constraint on the xi makes the problem NP-hard. Instead replace the

integer constraints by allowing the xi to be unit length vectors. This enlarges the set of

feasible solutions since 1 are just 1-dimensional vectors of length 1. The relaxed problem

is an SDP and can be solved in polynomial time. To see that it is an SDP, consider xi as

the rows of a matrix X. The variables of our SDP are not X, but actually Y = XX T ,

which is a positive semi-denite matrix. The SDP is

Maximize (cid:80)

(i,j)E

(1  yij) subject to Y positive semi-denite,

which can be solved in polynomial time. From the solution Y , nd X satisfying Y = XX T .

Now, instead of a 1 label on each vertex, we have vector labels, namely the rows of X.

We need to round the vectors to 1 to get an S. One natural way to do this is to pick

a random vector v and if for vertex i, xi  v is positive, put i in S, otherwise put it in

S. Goemans and Wiiliamson showed that this method produces a cut guaranteed to be

at least 0.878 times the maximum. The .878 factor is a big improvement on the previous

best factor of 0.5 which is easy to get by putting each vertex into S with probability 1/2.

Application to machine learning.

As discussed in Chapter 5, kernel functions are a powerful tool in machine learning.

They allow one to apply algorithms that learn linear classiers, such as Perceptron and

Support Vector Machines, to problems where the positive and negative examples might

have a more complicated separating curve.

More specically, a kernel K is a function from pairs of examples to reals such that

for some implicit function  from examples to (cid:60)N , we have K(a, a(cid:48)) = (a)T (a(cid:48)). (We

are using a and a(cid:48) to refer to examples, rather than x and x(cid:48), in order to not conict

with the notation used earlier in this chapter.) Notice that this means that for any set of

379

examples {a1, a2, . . . , an}, the matrix A whose ij entry equals K(ai, aj) is positive semi-

denite. Specically, A = BBT where the ith row of B equals (ai).

Given that a kernel corresponds to a positive semi-denite matrix, it is not surprising

that there is a related use of semi-denite programming in machine learning. In particular,

suppose that one does not want to specify up-front exactly which kernel an algorithm

should use. In that case, a natural idea is instead to specify a space of kernel functions and

allow the algorithm to select the best one from that space for the given data. Specically,

given some labeled training data and some unlabeled test data, one could solve for the

matrix A over the combined data set that is positive semi-denite (so that it is a legal

kernel function) and optimizes some given objective. This objective might correspond

to separating the positive and negative examples in the labeled data while keeping the

kernel simple so that it does not over-t. If this objective is linear in the coecients of

A along with possibly additional linear constraints on A, then this is an SDP. This is the

high-level idea of kernel learning, rst proposed in [LCB+04].

10.9 Bibliographic Notes

Arrows impossibility theorem, stating that any ranking of three or more items satisfying

unanimity and independence of irrelevant alternatives must be a dictatorship, is from

[Arr50]. For extensions to Arrows theorem on the manipulability of voting rules, see

Gibbard [Gib73] and Satterthwaite [Sat75]. A good discussion of issues in social choice

appears in [Lis13]. The results presented in Section 10.2.2 on compressed sensing are due

to Donoho and Elad [DE03] and Gribonval and Nielsen [GN03]. See [Don06] for more

details on issues in compressed sensing. The ellipsoid algorithm for linear programming is

due to Khachiyan [Kha79] based on work of Shor [Sho70] and Iudin and Nemirovski [IN77].

For more information on the ellipsoid algorithm and on semi-denite programming, see

the book of Grotschel, Lovasz, and Schrijver [GLS12]. The use of SDPs for approximating

the max-cut problem is due to Goemans and Williamson[GW95], and the use of SDPs for

learning a kernel function is due to [LCB+04].

380

10.10 Exercises

Exercise 10.1 Select a method that you believe is good for combining individual rankings

into a global ranking. Consider a set of rankings where each individual ranks b last. One

by one move b from the bottom to the top leaving the other rankings in place. Does there

exist a v as in Theorem 10.2 where v is the ranking that causes b to move from the bottom

to the top in the global ranking. If not, does your method of combing individual rankings

satisfy the axioms of unanimity and independence of irrelevant alternatives.

Exercise 10.2 Show that for the three axioms: non dictator, unanimity, and indepen-

dence of irrelevant alternatives, it is possible to satisfy any two of the three.

Exercise 10.3 Does the axiom of independence of irrelevant alternatives make sense?

What if there were three rankings of ve items. In the rst two rankings, A is number one

and B is number two. In the third ranking, B is number one and A is number ve. One

might compute an average score where a low score is good. A gets a score of 1+1+5=7

and B gets a score of 2+2+1=5 and B is ranked number one in the global ranking. Now if

the third ranker moves A up to the second position, As score becomes 1+1+2=4 and the

global ranking of A and B changes even though no individual ranking of A and B changed.

Is there some alternative axiom to replace independence of irrelevant alternatives? Write

a paragraph on your thoughts on this issue.

Exercise 10.4 Prove that in the proof of Theorem 10.2, the global ranking agrees with

column v even if item b is moved down through the column.

Exercise 10.5 Let A be an m by n matrix with elements from a zero mean, unit variance

Gaussian. How large must n be for there to be two or more sparse solutions to Ax = b

with high probability. You will need to dene how small s should be for a solution with at

most s nonzero elements to be sparse.

Exercise 10.6 Section 10.2.1 showed that if A is an n  d matrix with entries selected

at random from a standard Gaussian, and n  2s, then with probability one there will be

a unique s-sparse solution to Ax = b. Show that if n  s, then with probability one there

will not be a unique s-sparse solution. Assume d > s.

Exercise 10.7 Section 10.2.2 used the fact that n = O(s2 log d) rows is sucient so

that if each column of A is a random unit-length n-dimensional vector, then with high

probability all pairwise dot-products of columns will have magnitude less than 1

2s. Here,

we show that n = (log d) rows is necessary as well. To make the notation less confusing

for this argument, we will use m instead of d.

Specically, prove that for m > 3n, it is not possible to have m unit-length n-dimensional

vectors such that all pairwise dot-products of those vectors are less than 1

2.

Some hints: (1) note that if two unit-length vectors u and v have dot-product greater

2 then |u  v|  1 (if their dot-product is equal to 1

than or equal to 1

2 then u, v, and the

origin form an equilateral triangle). So, it is enough to prove that m > 3n unit-length

vectors in (cid:60)n cannot all have distance at least 1 from each other. (2) use the fact that the

volume of a ball of radius r in (cid:60)n is proportional to rn.

381

Exercise 10.8 Create a random 100 by 100 orthonormal matrix A and a sparse 100-

dimensional vector x. Compute Ax = b. Randomly select a few coordinates of b and

reconstruct x from the samples of b using the minimization of 1-norm technique of Section

10.2.2. Did you get x back?

Exercise 10.9 Let A be a low rank n  m matrix. Let r be the rank of A. Let A be A

corrupted by Gaussian noise. Prove that the rank r SVD approximation to A minimizes

(cid:12)

(cid:12)

(cid:12)A  A

(cid:12)

(cid:12)

(cid:12)

.

2

F

Exercise 10.10 Prove that minimizing ||x||0 subject to Ax = b is NP-complete.

Exercise 10.11 When one wants to minimize ||x||0 subject to some constraint the prob-

lem is often NP-hard and one uses the 1-norm as a proxy for the 0-norm. To get an

insite into this issue consider minimizing ||x||0 subject to the constraint that x lies in a

convex region. For simplicity assume the convex region is a sphere with center more than

the radius of the circle from the origin. Explore sparsity of solution when minimizing the

1-norm for values of x in the circular region with regards to location of the center.

Exercise 10.12 Express the matrix

2

2

2

2

13

17 2 2 2

2 2 2

2

2 9 2

2

2 2 2

2

2 2 2

2

as the sum of a low rank matrix plus a sparse matrix. To simplify the computation assume

you want the low rank matrix to be symmetric so that its singular valued decomposition

will be V V T .

Exercise 10.13 Generate 100  100 matrices of rank 20, 40, 60 80, and 100. In each

matrix randomly delete 50, 100, 200, or 400 entries.

In each case try to recover the

original matrix. How well do you do?

Exercise 10.14 Repeat the previous exercise but instead of deleting elements, corrupt the

elements by adding a reasonable size corruption to the randomly selected matrix entries.

End of sparse solutions, start of Uncertainty principle

Exercise 10.15 Compute the Fourier transform of the sequence 1000010000.

Exercise 10.16 What is the Fourier transform of a Gaussian?

Exercise 10.17 What is the Fourier transform of a cyclic shift of a sequence?

382

Exercise 10.18 Let S(i, j) be the sequence of i blocks each of length j where each block

of symbols is a 1 followed by j  1 0s. The number n=6 is factorable but not a perfect

square. What is Fourier transform of S (2, 3)= 100100?

Exercise 10.19 Let Z be the n root of unity. Prove that (cid:8)zbi|0  i < n(cid:9) = {zi|0  i < n}

provide that b does not divide n.

Exercise 10.20 Show that if the elements in the second row of the n  n Vandermonde

matrix















1

a

a2

...

an1

1

b

b2

...

bn1

 

 

 

 















1

c

c2

...

cn1

are distinct, then the Vandermonde matrix is nonsingular by expressing the determinant

of the matrix as an n  1 degree polynomial in a.

Exercise 10.21 Show that the following two statements are equivalent.

1. If the elements in the second row of the n  n Vandermonde matrix















1

a

a2

...

an1

1

b

b2

...

bn1

 

 

 

 















1

c

c2

...

cn1

are distinct, then the Vandermonde matrix is nonsingular.

2. Specifying the value of an nth degree polynomial at n + 1 points uniquely determines

the polynomial.

Exercise 10.22 Many problems can be formulated as nding x satisfying Ax = b where

A has more columns than rows and there is a subspace of solutions. If one knows that the

solution is sparse but some error in the measurement b may prevent nding the sparse

solution, they might add some residual error to b and reformulate the problem as solving

for x and r subject to Ax = b + r where r is the residual error. Discuss the advantages

and disadvantages of each of the following three versions of the problem.

1. Set r=0 and nd x= argmin (cid:107)x(cid:107)1 satisfying Ax = b

2. Lasso: nd x= argmin (cid:0)(cid:107)x(cid:107)1 +  (cid:107)r(cid:107)2

(cid:1) satisfying Ax = b + r

2

3. nd x



=argmin (cid:107)x(cid:107)1 such that (cid:107)r(cid:107)2

2 < 

383

Exercise 10.23 Let M = L+R where L is a low rank matrix corrupted by a sparse noise

matrix R. Why can we not recover L from M if R is low rank or if L is sparse?

Exercise 10.24

1. Suppose for a univariate convex function f and a nite interval D, |f (cid:48)(cid:48)(x)|  |f (cid:48)(x)|

for every x. Then, what is a good step size to choose for gradient descent? Derive a

bound on the number of steps needed to get an approximate minimum of f in terms

of as few parameters as possible.

2. Generalize the statement and proof to convex functions of d variables.

Exercise 10.25 Prove that the maximum of a convex function over a polytope is attained

at one of its vertices.

Exercise 10.26 Create a convex function and a convex region where the maximization

problem has local maximuns.

Exercise 10.27 Prove Lemma 10.10.

Exercise 10.28 Consider the following symmetric matrix A:









1

1

0

1

1 1

1

0

0

1

2

1

2

1 1 0









Find four vectors v1, v2, v3, v4 such that aij = vi

matrix B such that A = BBT .

T vj for all 1  i, j  4. Also, nd a

Exercise 10.29 Prove that if A1 and A2 are positive semi-denite matrices, then so is

A1 + A2.

7. Smoothed Analysis of Algorithms: The Simplex Algorithm Usually Takes a Polyno-

mial Number of Steps, Journal of the Association for Computing Machinery (JACM), 51

(3) pp: 385463, May 2004. Conference Version: the Annual ACM Symposium on Theory

of Computing, pages 296-305, 2001 (with Dan Spielman).

384

11 Wavelets

Given a vector space of functions, one would like an orthonormal set of basis functions

that span the space. The Fourier transform provides a set of basis functions based on

sines and cosines. Often we are dealing with functions that have local structure in which

case we would like the basis vectors to have nite support. Also we would like to have

an ecient algorithm for computing the coecients of the expansion of a function in the

basis.

11.1 Dilation

We begin our development of wavelets by rst introducing dilation. A dilation is a

mapping that scales all distances by the same factor.



A dilation equation is an equation where a function is dened in terms of a linear

combination of scaled, shifted versions of itself. For instance,

f (x) =

d1

(cid:88)

k=0

ckf (2x  k).

An example of this is f (x) = f (2x) + f (2x  1) which has a solution f (x) equal to one

for 0  x < 1 and is zero elsewhere. The equation is illustrated in the gure below. The

solid rectangle is f (x) and the dotted rectangles are f (2x) and f (2x  1).

f (x)

f (2x)

f (2x  1)

0

1

2

1

Another example is f (x) = 1

2f (2x) + f (2x  1) + 1

in the gure below. The function f (x) is indicated by solid lines. The functions 1

f (2x + 1), and 1

2f (2x  2). A solution is illustrated

2f (2x),

2f (2x  2) are indicated by dotted lines.

1

f (x)

1

2f (2x)

f (2x  1)

1

2f (2x  2)

0

1

2

385

If a dilation equation is of the form (cid:80)d1

equation are factor of two reductions.

k=1 ckf (2x  k) then we say that all dilations in the

Lemma 11.1 If a dilation equation in which all the dilations are a factor of two reduction

has a solution, then either the coecients on the right hand side of the equation sum to

two or the integral (cid:82) 

 f (x)dx of the solution is zero.

Proof: Integrate both sides of the dilation equation from  to +.

(cid:90) 



f (x)dx =

(cid:90) 

d1

(cid:88)



k=0

ckf (2x  k)dx =

d1

(cid:88)

k=0

ck

(cid:90) 



f (2x  k)dx

=

d1

(cid:88)

k=0

ck

(cid:90) 



f (2x)dx =

1

2

d1

(cid:88)

k=0

ck

(cid:90) 



f (x)dx

If (cid:82) 

 f (x)dx (cid:54)= 0, then dividing both sides by



(cid:82)



f (x)dx gives

d1

(cid:80)

k=0

ck = 2

The above proof interchanged the order of the summation and the integral. This is valid

provided the 1-norm of the function is nite. Also note that there are nonzero solutions to

dilation equations in which all dilations are a factor of two reduction where the coecients

do not sum to two such as

f (x) = f (2x) + f (2x  1) + f (2x  2) + f (2x  3)

or

f (x) = f (2x) + 2f (2x  1) + 2f (2x  2) + 2f (2x  3) + f (2x  4).

In these examples f (x) takes on both positive and negative values and (cid:82) 

 f (x)dx = 0.

11.2 The Haar Wavelet

Let (x) be a solution to the dilation equation f (x) = f (2x) + f (2x  1). The function

 is called a scale function or scale vector and is used to generate the two dimensional

family of functions, jk(x) = (2jx  k), where j and k are non-negative integers. Other

authors scale jk = (2jx  k) by 2

jk(t)dt, is 1. However, for

educational purposes, simplifying the notation for ease of understanding was preferred.

2 so that the 2-norm, (cid:82) 

 2

j

For a given value of j, the shifted versions, {jk|k  0}, span a space Vj. The spaces

V0, V1, V2, . . . are larger and larger spaces and allow better and better approximations to

a function. The fact that (x) is the solution of a dilation equation implies that for any

xed j, jk is a linear combination of the {j+1,k(cid:48)|k(cid:48)  0} and this ensures that Vj  Vj+1.

It is for this reason that it is desirable in designing a wavelet system for the scale function

to satisfy a dilation equation. For a given value of j, the shifted jk are orthogonal in the

386

1

1

1

1

3

00(x) = (x)

2

1

3

10(x) = (2x)

2

1

3

20(x) = (4x)

2

1

1

1

1

2

3

01(x) = (x  1)

1

2

3

02(x) = (x  2)

1

2

3

4

03(x) = (x  3)

1

1

1

1

2

3

1

2

3

1

2

3

11(x) = (2x  1)

12(x) = (2x  2)

13(x) = (2x  3)

1

1

1

1

2

3

1

2

3

1

2

3

21(x) = (4x  1)

22(x) = (4x  2)

23(x) = (4x  3)

Figure 11.1: Set of scale functions associated with the Haar wavelet.

sense that (cid:82)

x jk(x)jl(x)dx = 0 for k (cid:54)= l.

Note that for each j, the set of functions jk, k = 0, 1, 2 . . . , form a basis for a vector

space Vj and are orthogonal. The set of basis vectors jk, for all j and k, form an over-

complete basis and for dierent values of j are not orthogonal. Since jk, j+1,2k, and

j+1,2k+1 are linearly dependent, for each value of j delete j+1,k for odd values of k to

get a linearly independent set of basis vectors. To get an orthogonal set of basis vectors,

dene

jk(x) =







1

2k

2j  x < 2k+1

2j

1 2k+1

2j  x < 2k+2

2j

0

otherwise

and replace j,2k with j+1,2k. Basically, replace the three functions

1

1

1

1

(x)

11

2

(2x)

11

2

(2x  1)

by the two functions

387

The Haar Wavelet

(x) =







1 0  x < 1

0 otherwise

(x) =







1

0  x < 1

2

1 1

2  x < 1

0

otherwise

1

1

1

(x)

(x)

(x)

1

(x)

1

x

1

1

x

1

1

The basis set becomes

00 10

20 22

30 32 34 36

40 42 44 46 48 4,10 4,12 4,14

To approximate a function that has only nite support, select a scale vector (x)

whose scale is that of the support of the function to be represented. Next approximate

the function by the set of scale functions (2jx  k), k = 0, 1, . . . , for some xed value of

j. The value of j is determined by the desired accuracy of the approximation. Basically

the x axis has been divided into intervals of size 2j and in each interval the function is

approximated by a xed value. It is this approximation of the function that is expressed

as a linear combination of the basis functions.

Once the value of j has been selected, the function is sampled at 2j points, one in each

interval of width 2j. Let the sample values be s0, s1, . . . . The approximation to the func-

tion is (cid:80)2j 1

k=0 sk(2jx  k) and is represented by the vector (s0, s1 . . . , s2j 1). The problem

now is to represent the approximation to the function using the basis vectors rather than

the nonorthogonal set of scale functions jk(x). This is illustrated in the following example.

388

To represent the function corresponding to a vector such as ( 3 1 4 8 3 5 7 9 ),

one needs to nd the ci such that

















































3

1

4

8

3

5

7

9

























=

1

1

1

1

1 1

1 1

1 1

1 1

0

0

0

1

1

1

0

0

0

0 1

1

1

0

0

1

0

0

1 1

0

0

0 1

0

1 1

0

0

0

1

1

0

0

0 1

0

0

1

0

1

0

0

0

0 1

0 1

0

0

0 1









































































.

c1

c2

c3

c4

c5

c6

c7

c8

The rst column represents the scale function (x) and subsequent columns the s.

The tree in Figure 11.2 illustrates an ecient way to nd the coecients representing

the vector ( 3 1 4 8 3 5 7 9 ) in the basis. Each vertex in the tree contains the

average of the quantities of its two children. The root gives the average of the elements in

the vector, which is 5 in this example. This average is the coecient of the basis vector

in the rst column of the above matrix. The second basis vector converts the average

of the eight elements into the average of the rst four elements, which is 4, and the last

four elements, which is 6, with a coecient of -1. Working up the tree determines the

coecients for each basis vector.

1

8

4

6

9

5

7

8

3

4

6

3

2

4

5

Figure 11.2: Tree of function averages

















































3

1

4

8

3

5

7

9

= 5

























1

1

1

1

1

1

1

1



1















































1

1

1

1

1

1

1

1



2















































1

1

1

1

0

0

0

0



2







































































0

0

0

0

1

1

1

1

+1

















































1

1

0

0

0

0

0

0

2

























0

0

1

1

0

0

0

0



1















































0

0

0

0

1

1

0

0



1















































0

0

0

0

0

0

1

1

























389

11.3 Wavelet Systems

So far we have explained wavelets using the simple-to-understand Haar wavelet. We

now consider general wavelet systems. A wavelet system is built from a basic scaling

function (x), which comes from a dilation equation. Scaling and shifting of the basic

scaling function gives a two dimensional set of scaling functions jk where

jk(x) = (2jx  k).

For a xed value of j, the jk span a space Vj. If (x) satises a dilation equation

(x) =

d1

(cid:88)

k=0

ck(2x  k),

then jk is a linear combination of the j+1,ks and this implies that V0  V1  V2  V3    .

11.4 Solving the Dilation Equation

Consider solving a dilation equation

(x) =

d1

(cid:88)

k=0

ck(2x  k)

to obtain the scale function for a wavelet system. Perhaps the easiest way is to assume

a solution and then calculate the scale function by successive approximation as in the

following program for the Daubechies scale function:







(x) = 1+

3

4 (2x) + 3+

3

4 (2x  1) + 3

3

4 (2x  2) + 1



4 (2x  3),

3

The solution will actually be samples of (x) at some desired resolution.

Program Compute-Daubechies:

Set the initial approximation to (x) by generating a vector whose components

approximate the samples of (x) at equally spaced values of x.

Begin with the coecients of the dilation equation.



c1 = 1+

4

3



c2 = 3+

4

3



c3 = 3

4

3



c4 = 1

4

3

Execute the following loop until the values for (x) converge.

begin

Calculate (2x) by averaging successive values of (x) together. Fill

out the remaining half of the vector representing (2x) with zeros.

390

Calculate (2x1), (2x2), and (2x3) by shifting the contents

of (2x) the appropriate distance, discarding the zeros that move

o the right end and adding zeros at the left end.

Calculate the new approximation for (x) using the above values

for (2x  1), (2x  2), and (2x  3) in the dilation equation for

(2x).

end

Figure 11.3: Daubechies scale function and associated wavelet

The convergence of the iterative procedure for computing is fast if the eigenvectors of

a certain matrix are unity.

Another approach to solving the dilation equation

Consider the dilation equation (x) = 1

continuous solutions with support in 0  x < 2.

2f (2x) + f (2x  1) + 1

2f (2x  2) and consider

(0) = 1

(2) = 1

(1) = 1

2(0) + (1) + (2) = 1

2(4) + (3) + (2) = 1

2(2) + 0 + 0

2(2) + (1) + (0) = 0 + (1) + 0

2(0) + 0 + 0

Set (1) = 1. Then

( 1

2) = 1

2(1) + (0) + 1

2(1) = 1

2

( 3

2) = 1

2(3) + (2) + 1

2(1) = 1

2

(0) = 0

(2) = 0

(1) arbitrary

.

( 1

2) + 1

One can continue this process and compute ( i

2j ) for larger values of j until (x) is

approximated to a desired accuracy. If (x) is a simple equation as in this example, one

could conjecture its form and verify that the form satises the dilation equation.

2) + ( 1

2( 3

4) = 1

2) = 1

2( 1

4

391

11.5 Conditions on the Dilation Equation

We would like a basis for a vector space of functions where each basis vector has

nite support and the basis vectors are orthogonal. This is achieved by a wavelet system

consisting of a shifted version of a scale function that satises a dilation equation along

with a set of wavelets of various scales and shifts. For the scale function to have a nonzero

integral, Lemma 11.1 requires that the coecients of the dilation equation sum to two.

Although the scale function (x) for the Haar system has the property that (x) and

(x  k), k > 0, are orthogonal, this is not true for the scale function for the dilation

equation (x) = 1

2(2x  2). The conditions that integer shifts of the

scale function be orthogonal and that the scale function has nite support puts additional

conditions on the coecients of the dilation equation. These conditions are developed in

the next two lemmas.

2(2x) + (2x  1) + 1

Lemma 11.2 Let

(x) =

d1

(cid:88)

k=0

ck(2x  k).

If (x) and (x  k) are orthogonal for k (cid:54)= 0 and (x) has been normalized so that

(cid:82) 

 (x)(x  k)dx = (k), then (cid:80)d1

i=0 cici2k = 2(k).

Proof: Assume (x) has been normalized so that (cid:82) 

 (x)(x  k)dx = (k). Then

(cid:90) 

x=

(x)(x  k)dx =

(cid:90) 

d1

(cid:88)

x=

i=0

ci(2x  i)

d1

(cid:88)

j=0

cj(2x  2k  j)dx

d1

(cid:88)

d1

(cid:88)

=

cicj

(cid:90) 

i=0

j=0

x=

(2x  i)(2x  2k  j)dx

Since

(cid:90) 

x=

(2x  i)(2x  2k  j)dx =

=

=

(cid:90) 

x=

(cid:90) 

x=

(y  i)(y  2k  j)dy

(y)(y + i  2k  j)dy

(2k + j  i),

1

2

1

2

1

2

(cid:90) 

x=

(x)(x  k)dx =

d1

(cid:88)

d1

(cid:88)

i=0

j=0

cicj

1

2

(2k + j  i) =

1

2

d1

(cid:88)

i=0

cici2k. Since (x) was nor-

malized so that

(cid:90) 

(x)(x  k)dx = (k), it follows that



d1

(cid:88)

i=0

cici2k = 2(k).

392

Scale and wavelet coecients equations

(x) = (cid:80)d1

k=0 ck(2x  k)



(cid:82)



d1

(cid:80)

j=0

d1

(cid:80)

j=0

(x)(x  k)dx = (k)

cj = 2

cjcj2k = 2(k)

ck = 0 unless 0  k  d  1

d even

d1

(cid:80)

j=0

c2j =

d1

(cid:80)

j=0

c2j+1

(x) =

d1

(cid:80)

k=0

bk(x  k)



(cid:82)

x=



(cid:82)

x=



(cid:82)

(x)(x  k) = 0

(x)dx = 0

(x)(x  k)dx = (k)

(1)kbibi2k = 2(k)

cjbj2k = 0

x=

d1

(cid:80)

i=0

d1

(cid:80)

j=0

d1

(cid:80)

j=0

bk = (1)kcd1k

bj = 0

One designs wavelet systems so the above conditions are satised.

Lemma 11.2 provides a necessary but not sucient condition on the coecients of

the dilation equation for shifts of the scale function to be orthogonal. One should note

that the conditions of Lemma 11.2 are not true for the triangular or piecewise quadratic

solutions to

(x) =

(2x) + (2x  1) +

(2x  2)

1

2

1

2

and

(x) =

1

4

(2x) +

3

4

(2x  1) +

3

4

(2x  2) +

1

4

(2x  3)

which overlap and are not orthogonal.

For (x) to have nite support the dilation equation can have only a nite number of

terms. This is proved in the following lemma.

Lemma 11.3 If 0  x < d is the support of (x), and the set of integer shifts, {(x 

k)|k  0}, are linearly independent, then ck = 0 unless 0  k  d  1.

Proof: If the support of (x) is 0  x < d, then the support of (2x) is 0  x < d

2 . If

(x) =



(cid:88)

k=

ck(2x  k)

393

the support of both sides of the equation must be the same. Since the (xk) are linearly

independent the limits of the summation are actually k = 0 to d  1 and

(x) =

d1

(cid:88)

k=0

ck(2x  k).

It follows that ck = 0 unless 0  k  d  1.

The condition that the integer shifts are linearly independent is essential to the proof

and the lemma is not true without this condition.

One should also note that

and k = d1

2

d1

(cid:80)

i=0

cici2k = 0 for k (cid:54)= 0 implies that d is even since for d odd

d1

(cid:88)

i=0

cici2k =

d1

(cid:88)

i=0

cicid+1 = cd1c0.

For cd1c0 to be zero either cd1 or c0 must be zero. Since either c0 = 0 or cd1 = 0, there

are only d  1 nonzero coecients. From here on we assume that d is even. If the dilation

equation has d terms and the coecients satisfy the linear equation (cid:80)d1

k=0 ck = 2 and the

2 quadratic equations (cid:80)d1

2  1

coecients that can be used to design the wavelet system to achieve desired properties.

i=0 cici2k = 2(k) for 1  k  d1

2 , then for d > 2 there are d

d

11.6 Derivation of the Wavelets from the Scaling Function

In a wavelet system one develops a mother wavelet as a linear combination of integer

shifts of a scaled version of the scale function (x). Let the mother wavelet (x) be given

d1

(cid:80)

k=0

by (x) =

bk(2x  k). One wants integer shifts of the mother wavelet (x  k) to

be orthogonal and also for integer shifts of the mother wavelet to be orthogonal to the

scaling function (x). These conditions place restrictions on the coecients bk which are

the subject matter of the next two lemmas.

Lemma 11.4 (Orthogonality of (x) and (x  k)) Let (x) =

and (xk) are orthogonal for k (cid:54)= 0 and (x) has been normalized so that (cid:82) 

k)dx = (k), then

bk(2x  k). If (x)

 (x)(x

d1

(cid:80)

k=0

d1

(cid:88)

i=0

(1)kbibi2k = 2(k).

Proof: Analogous to Lemma 11.2.

394

Lemma 11.5 (Orthogonality of (x) and (x  k)) Let (x) =

d1

(cid:80)

k=0

ck(2x  k) and

bk(2x  k). If



(cid:82)

x=

(x)(x  k)dx = (k) and



(cid:82)

x=

(x)(x  k)dx = 0 for

d1

(cid:80)

i=0

cibi2k = 0 for all k.

(x) =

d1

(cid:80)

k=0

all k, then

Proof:

(cid:90) 

x=

(x)(x  k)dx =

(cid:90) 

d1

(cid:88)

x=

i=0

ci(2x  i)

d1

(cid:88)

j=1

bj(2x  2k  j)dx = 0.

Interchanging the order of integration and summation

d1

(cid:88)

d1

(cid:88)

i=0

j=0

cibj

(cid:90) 

x=

(2x  i)(2x  2k  j)dx = 0

Substituting y = 2x  i yields

1

2

d1

(cid:88)

d1

(cid:88)

i=0

j=0

cibj

(cid:90) 

y=

(y)(y  2k  j + i)dy = 0

Thus,

Summing over j gives

d1

(cid:88)

d1

(cid:88)

i=0

j=0

cibj(2k + j  i) = 0

d1

(cid:88)

i=0

cibi2k = 0

Lemma 11.5 gave a condition on the coecients in the equations for (x) and (x) if

integer shifts of the mother wavelet are to be orthogonal to the scale function. In addition,

for integer shifts of the mother wavelet to be orthogonal to the scale function requires

that bk = (1)kcd1k.

Lemma 11.6 Let the scale function (x) equal

d1

(cid:80)

k=0

ck(2xk) and let the wavelet function

(x) equal

d1

(cid:80)

k=0

bk(2x  k). If the scale functions are orthogonal

(cid:90) 



(x)(x  k)dx = (k)

395

and the wavelet functions are orthogonal with the scale function



(cid:90)

x=

(x)(x  k)dx = 0

for all k, then bk = (1)kcd1k.

Proof: By Lemma 11.5,

even indices gives

d1

(cid:80)

j=0

cjbj2k = 0 for all k. Separating

d1

(cid:80)

j=0

cjbj2k = 0 into odd and

d

2 1

(cid:88)

j=0

c2jb2j2k +

d

2 1

(cid:88)

j=0

c2j+1b2j+12k = 0

(11.1)

for all k.

c0b0 + c2b2+c4b4 +    + c1b1 + c3b3 + c5b5 +    = 0

+ c3b1 + c5b3 +    = 0

+ c5b1 +    = 0

c2b0+c4b2 +   

c4b0 +   

k = 0

k = 1

k = 2

By Lemmas 11.2 and 11.4,

d1

(cid:80)

j=0

Separating odd and even terms,

cjcj2k = 2(k) and

d1

(cid:80)

j=0

bjbj2k = 2(k) and for all k.

d

2 1

(cid:88)

j=0

c2jc2j2k +

d

2 1

(cid:88)

j=0

c2j+1c2j+12k = 2(k)

(11.2)

d

2 1

(cid:88)

j=0

b2jb2j2k +

d

2 1

(cid:88)

j=0

(1)jb2j+1b2j+12k = 2(k)

(11.3)

and

for all k.

c0c0 + c2c2+c4c4 +    + c1c1 + c3c3 + c5c5 +    = 2

+ c3c1 + c5c3 +    = 0

+ c5c1 +    = 0

c2c0+c4c2 +   

c4c0 +   

b0b0 + b2b2+b4b4 +    + b1b1  b3b3 + b5b5     = 2

 b3b1 + b5b3     = 0

+ b5b1     = 0

b2b0+b4b2 +   

b4b0 +   

k = 0

k = 1

k = 2

k = 0

k = 1

k = 2

396

Let Ce = (c0, c2, . . . , cd2), Co = (c1, c3, . . . , cd1), Be = (b0, b2, . . . , bd2), and Bo =

(b1, b3, . . . , bd1). Equations 12.1, 12.2, and 11.3 can be expressed as convolutions46 of

these sequences. Equation 12.1 is Ce  BR

e + Co  BR

o = 0, 12.2 is Ce  C R

o = (k),

and 11.3 is Be  BR

o = (k), where the superscript R stands for reversal of the

sequence. These equations can be written in matrix format as

e + Bo  BR

e + Co  C R

(cid:18) Ce Co

Be Bo

(cid:19)



(cid:18) C R

C R

e BR

e

o BR

o

(cid:19)

(cid:18) 2

0

=

(cid:19)

0

2

Taking the Fourier or z-transform yields

(cid:18) F (Ce) F (Co)

F (Be) F (Bo)

(cid:19) (cid:18) F (C R

F (C R

e ) F (BR

e )

o ) F (BR

o )

(cid:19)

(cid:18) 2 0

0 2

(cid:19)

.

=

where F denotes the transform. Taking the determinant yields

(cid:16)

F (Ce)F (Bo)  F (Be)F (Co)

(cid:17)(cid:16)

F (Ce)F (Bo)  F (Co)F (Be)

(cid:17)

= 4

Thus F (Ce)F (Bo)  F (Co)F (Be) = 2 and the inverse transform yields

Ce  Bo  Co  Be = 2(k).

Convolution by C R

e yields

C R

e  Ce  Bo  C R

e  Be  Co = C R

e  2(k)

Now

d1

(cid:80)

j=0

cjbj2k = 0 so C R

e  Be = C R

o  Bo. Thus

C R

e  Ce  Bo + C R

e  Ce + C R

(C R

o  Bo  Co = 2C R

o  Co)  Bo = 2C R

2(k)  Bo = 2C R

Ce = BR

o

e  (k)

e  (k)

e  (k)

Thus, ci = 2bd1i for even i. By a similar argument, convolution by C R

0 yields

C R

0  Ce  B0  C R

0  C0  Be = 2C R

0 (k)

Since C R

)  B0 = C R

0  Be

C R

e  C R

(Ce  C R

e  Be  C R

e + C R

0  C0  Be = 2C R

0  C0)  Be = 2C R

2(k)Be = 2C R

Be = C R

0

0 (k)

0 (k)

0 (k)

Thus, ci = 2bd1i for all odd i and hence ci = (1)i2bd1i for all i.

46The convolution of (a0, a1, . . . , ad1) and (b0, b1, . . . , bd1) denoted

(a0, a1, . . . , ad1)  (b0, b1, . . . , bd1) is the sequence

(a0bd1, a0bd2 + a1bd1, a0bd3 + a1bd2 + a3bd1 . . . , ad1b0).

397

11.7 Sucient Conditions for the Wavelets to be Orthogonal

Section 11.6 gave necessary conditions on the bk and ck in the denitions of the scale

function and wavelets for certain orthogonality properties. In this section we show that

these conditions are also sucient for certain orthogonality conditions. One would like a

wavelet system to satisfy certain conditions.

1. Wavelets, j(2jx  k), at all scales and shifts to be orthogonal to the scale function

(x).

2. All wavelets to be orthogonal. That is

(cid:90) 

j(2jx  k)l(2lx  m)dx = (j  l)(k  m)



3. (x) and jk, j  l and all k, to span Vl, the space spanned by (2lx  k) for all k.

These items are proved in the following lemmas. The rst lemma gives sucient conditions

on the wavelet coecients bk in the denition

(x) =

(cid:88)

k

bk(2x  k)

for the mother wavelet so that the wavelets will be orthogonal to the scale function. The

lemma shows that if the wavelet coecients equal the scale coecients in reverse order

with alternating negative signs, then the wavelets will be orthogonal to the scale function.

Lemma 11.7 If bk = (1)kcd1k, then (cid:82) 

 (x)(2jx  l)dx = 0 for all j and l.

Proof: Assume that bk = (1)kcd1k. We rst show that (x) and (x  k) are orthog-

onal for all values of k. Then we modify the proof to show that (x) and (2jx  k) are

orthogonal for all j and k.

Assume bk = (1)kcd1k. Then

(cid:90) 



(x)(x  k) =

(cid:90) 

d1

(cid:88)



i=0

ci(2x  i)

d1

(cid:88)

j=0

bj(2x  2k  j)dx

=

=

=

d1

(cid:88)

d1

(cid:88)

i=0

j=0

d1

(cid:88)

d1

(cid:88)

i=0

j=0

ci(1)jcd1j

(cid:90) 



(2x  i)(2x  2k  j)dx

(1)jcicd1j(i  2k  j)

d1

(cid:88)

j=0

(1)jc2k+jcd1j

= c2kcd1  c2k+1cd2 +    + cd2c2k1  cd1c2k

= 0

398

The last step requires that d be even which we have assumed for all scale functions.

For the case where the wavelet is (2j  l), rst express (x) as a linear combination

of (2j1x  n). Now for each these terms

(cid:90) 



(2j1x  m)(2jx  k)dx = 0

To see this, substitute y = 2j1x. Then

(cid:90) 



(2jx  m)(2jx  k)dx =

1

2j1

(cid:90) 



(y  m)(2y  k)dy

which by the previous argument is zero.

The next lemma gives conditions on the coecients bk that are sucient for the

wavelets to be orthogonal.

Lemma 11.8 If bk = (1)kcd1k, then

(cid:90) 



1

2j j(2jx  k)

1

2k l(2lx  m)dx = (j  l)(k  m).

Proof: The rst level wavelets are orthogonal.

(cid:90) 



(x)(x  k)dx =

(cid:90) 

d1

(cid:88)



i=0

bi(2x  i)

d1

(cid:88)

j=0

bj(2x  2k  j)dx

d1

(cid:88)

bi

d1

(cid:88)

bj

=

(cid:90) 

i=0

j=0



(2x  i)(2x  2k  j)dx

=

=

=

=

d1

(cid:88)

d1

(cid:88)

i=0

j=0

bibj(i  2k  j)

d1

(cid:88)

i=0

d1

(cid:88)

i=0

d1

(cid:88)

i=0

bibi2k

(1)icd1i(1)i2kcd1i+2k

(1)2i2kcd1icd1i+2k

Substituting j for d  1  i yields

d1

(cid:88)

j=0

cjcj+2k = 2(k)

399

Example of orthogonality when wavelets are of dierent scale.

(cid:90) 



(2x)(x  k)dx =

(cid:90) 

d1

(cid:88)



i=0

bi(4x  i)

d1

(cid:88)

j=0

bj(2x  2k  j)dx

d1

(cid:88)

d1

(cid:88)

=

bibj

i=0

i=0

(cid:90) 



(4x  i)(2x  2k  j)dx

Since (2x  2k  j) =

d1

(cid:80)

l=0

cl(4x  4k  2j  l)

(cid:90) 



(2x)(x  k)dx =

=

=

d1

(cid:88)

d1

(cid:88)

d1

(cid:88)

i=0

j=0

l=0

d1

(cid:88)

d1

(cid:88)

d1

(cid:88)

i=0

j=0

l=0

bibjcl

(cid:90) 



(4x  i)(4x  4k  2j  l)dx

bibjcl(i  4k  2j  l)

d1

(cid:88)

d1

(cid:88)

i=0

j=0

bibjci4k2j

Since

d1

(cid:80)

j=0

cjbj2k = 0,

d1

(cid:80)

i=0

bici4k2j = (j  2k) Thus

(cid:90) 



(2x)(x  k)dx =

d1

(cid:88)

j=0

bj(j  2k) = 0.

Orthogonality of scale function with wavelet of dierent scale.

(cid:90) 



(x)(2x  k)dx =

(cid:90) 

d1

(cid:88)

cj(2x  j)(2x  k)dx



d1

(cid:88)

j=0

cj

j=0

(cid:90) 



(2x  j)(2x  k)dx

1

2

d1

(cid:88)

j=0

cj

(cid:90) 



(y  j)(y  k)dy

=

=

= 0

If  was of scale 2j,  would be expanded as a linear combination of  of scale 2j all of

which would be orthogonal to .

400

11.8 Expressing a Function in Terms of Wavelets

Given a wavelet system with scale function  and mother wavelet  we wish to express

a function f (x) in terms of an orthonormal basis of the wavelet system. First we will ex-

press f (x) in terms of scale functions jk(x) = (2jx  k). To do this we will build a tree

similar to that in Figure 11.2 for the Haar system, except that computing the coecients

will be much more complex. Recall that the coecients at a level in the tree are the

coecients to represent f (x) using scale functions with the precision of the level.

Let f (x) = (cid:80)

k=0 ajkj(x  k) where the ajk are the coecients in the expansion of

f (x) using level j scale functions. Since the j(x  k) are orthogonal

ajk =

(cid:90) 

x=

f (x)j(x  k)dx.

Expanding j in terms of j+1 yields

ajk =

(cid:90) 

x=

f (x)

(cid:90) 

d1

(cid:88)

m=0

cmj+1(2x  2k  m)dx

f (x)j+1(2x  2k  m)dx

=

=

d1

(cid:88)

m=0

d1

(cid:88)

m=0

cm

x=

cmaj+1,2k+m

Let n = 2k + m. Now m = n  2k. Then

ajk =

d1

(cid:88)

n=2k

cn2kaj+1,n

(11.4)

In construction the tree similar to that in Figure 11.2, the values at the leaves are

the values of the function sampled in the intervals of size 2j. Equation 11.4 is used to

compute values as one moves up the tree. The coecients in the tree could be used if we

wanted to represent f (x) using scale functions. However, we want to represent f (x) using

one scale function whose scale is the support of f (x) along with wavelets which gives us

an orthogonal set of basis functions. To do this we need to calculate the coecients for

the wavelets. The value at the root of the tree is the coecient for the scale function. We

then move down the tree calculating the coecients for the wavelets.

Finish by calculating wavelet coecients

maybe add material on jpeg

Example: Add example using D4.

Maybe example using sinc

401

11.9 Designing a Wavelet System

In designing a wavelet system there are a number of parameters in the dilation equa-

tion. If one uses d terms in the dilation equation, one degree of freedom can be used to

satisfy

d1

(cid:88)

i=0

ci = 2

which insures the existence of a solution with a nonzero mean. Another d

freedom are used to satisfy

2 degrees of

d1

(cid:88)

i=0

cici2k = (k)

which insures the orthogonal properties. The remaining d

2  1 degrees of freedom can be

used to obtain some desirable properties such as smoothness. Smoothness appears to be

related to vanishing moments of the scaling function. Material on the design of systems

is beyond the scope of this book and can be found in the literature.

11.10 Applications

Wavelets are widely used in data compression for images and speech, as well as in

computer vision for representing images. Unlike the sines and cosines of the Fourier

transform, wavelets have spatial locality in addition to frequency information, which can

be useful for better understanding the contents of an image and for relating pieces of

dierent images to each other. Wavelets are also being used in power line communication

protocols that send data over highly noisy channels.

11.11 Bibliographic Notes

In 1909 Alfred Haar presented an orthonormal basis for functions with nite support.

Ingrid Daubechies

402

11.12 Exercises

Exercise 11.1 Give a solution to the dilation equation f (x) = f (2x)+f (2xk) satisfying

f (0) = 1. Assume k is an integer.

Exercise 11.2 Are there solutions to f (x) = f (2x) + f (2x  1) other than a constant

multiple of

f (x) =

(cid:26) 1 0  x < 1

0 otherwise

?

Exercise 11.3 Is there a solution to f (x) = 1

f (0) = f (1) = 1 and f (2) = 0?

2f (2x) + f (2x  1) + 1

2f (2x  2) with

Exercise 11.4 What is the solution to the dilation equation

f (x) = f (2x) + f (2x  1) + f (2x  2) + f (2x  3).

Exercise 11.5 Consider the dilation equation

f (x) = f (2x) + 2f (2x  1) + 2f (2x  2) + 2f (2x  3) + f (2x  4)

1. What is the solution to the dilation equation?

2. What is the value of (cid:82) 

 f (x)dx?

Exercise 11.6 What are the solutions to the following families of dilation equations.

1.

2.

f (x) =f (2x) + f (2x  1)

f (2x) +

f (2x) +

f (2x  1) +

f (2x  1) +

1

2

1

4

f (2x  2) +

f (2x  2) +

1

2

1

4

f (2x  3)

f (2x  3) +

1

4

f (2x  4) +

1

4

f (2x  5)

f (x) =

f (x) =

1

2

1

4

f (x) =

1

k

1

4

1

2

1

4

1

k

1

4

+

f (2x  6) +

f (2x  7)

f (2x) +

f (2x) +    +

1

k

f (2x)

f (x) =

f (x) =

f (x) =

f (x) =

1

3

1

4

1

5

1

k

f (2x) +

f (2x) +

f (2x) +

f (2x) +

2

3

3

4

4

5

k  1

k

f (2x  1) +

f (2x  1) +

f (2x  1) +

2

3

3

4

4

5

f (2x  2) +

f (2x  2) +

f (2x  2) +

1

3

1

4

1

5

f (2x  3)

f (2x  3)

f (2x  3)

f (2x  1) +

k  1

k

f (2x  2) +

1

k

f (2x  3)

403

3.

4.

f (x) =

f (x) =

f (x) =

f (x) =

f (x) =

f (x) =

f (x) =

f (x) =

Exercise 11.7

f (2x) +

1

2

3

2

5

2

1 + 2k

2

f (2x) 

f (2x) 

f (2x) +

1

3

4

3

7

3

1 + 3k

3

f (2x) 

f (2x) 

1

2

1

2

3

2

f (2x  1) +

f (2x  1) +

f (2x  1) +

1

2

3

2

5

2

f (2x  2) +

f (2x  3)

f (2x  2) 

f (2x  3)

f (2x  2) 

f (2x  3)

f (2x) 

2k  1

2

f (2x  1) +

f (2x  2) 

2k  1

2

f (2x  3)

2

3

1

3

4

3

f (2x  1) +

f (2x  1) +

f (2x  1) +

2

3

5

3

8

3

f (2x  2) +

f (2x  3)

f (2x  2) 

f (2x  3)

f (2x  2) 

f (2x  3)

f (2x) 

2  3k

3

f (2x  1) +

f (2x  2) 

1  3k

3

f (2x  3)

1

2

1

2

3

2

1 + 2k

2

1

3

2

3

5

3

2 + 3k

3

1. What is the solution to the dilation equation f (x) = 1

Write a program to see what the solution looks like.

2f (2x) + 3

2f (2x  1)? Hint:

2. How does the solution change when the equation is changed to f (x) = 1

3f (2x) +

5

3f (2x  1)?

3. How does the solution change if the coecients no longer sum to two as in f (x) =

f (2x) + 3f (2x  1)?

Exercise 11.8 If f (x) is frequency limited by 2, prove that

f (x) =



(cid:88)

k=0

f (k)

sin((x  k))

(x  k)

.

Hint: Use the Nyquist sampling theorem which states that a function frequency limited by

2 is completely determined by samples spaced one unit apart. Note that this result means

that

f (k) =

f (x)

(cid:90) 



sin((x  k))

(x  k)

dx

Exercise 11.9 Compute an approximation to the scaling function that comes from the

dilation equation

(x) =



1 +

4

3

(2x) +



3

3 +

4

(2x  1) +



3

3 

4

(2x  2) +



1

3

4

(2x  3).

404

Exercise 11.10 Consider f (x) to consist of the semi circle (x  1

for 0  x  1 and 0 otherwise.

2)2 + y2 = 1

4 and y  0

1. Using precision j = 4 nd the coecients for the scale functions and the wavelets

for D4 dened by the dilation equation

(x) =



1 +

4

3

(2x) +



3

3 +

4

(2x  1) +



3 

4

3

(2x  2) +



1

3

4

(2x  3)

2. Graph the approximation to the semi circle for precision j = 4.

Exercise 11.11 What is the set of all solutions to the dilation equation



1

3







3

3

3

(x) =

(2x) +

(2x  1) +

(2x  2) +

(2x  3)

1 +

4

3 +

4

3 

4

4

Exercise 11.12 Prove that if scale functions dened by a dilation equation are orthogo-

nal, then the sum of the even coecients must equal the sum of the odd coecients in the

dilation equation. That is, (cid:80)

c2k = (cid:80)

c2k+1.

k

k

function = wavelets

acc=32; %accuracy of computation

phit=[1:acc zeros(1,3*acc)];

c1=(1+3^0.5)/4; c2=(3+3^0.5)/4; c3=(3-3^0.5)/4; c4=(1-3^0.5)/4;

for i=1:10

temp=(phit(1:2:4*acc)+phit(2:2:4*acc))/2;

phi2t=[temp zeros(1,3*acc)];

phi2tshift1=[ zeros(1,acc) temp zeros(1,2*acc)];

phi2tshift2=[ zeros(1,2*acc) temp zeros(1,acc)];

phi2tshift3=[ zeros(1,3*acc) temp ];

phit=c1*phi2t+c2*phi2tshift1+c3*phi2tshift2+c4*phi2tshift3;

plot(phit)

figure(gcf)

pause

end plot(phit) figure(gcf) end

405

12 Appendix

12.1 Denitions and Notation

. . . , 3, 2, 1,

nonnegative integers

(cid:122)

(cid:123)

(cid:125)(cid:124)

0, 1, 2, 3, . . .

(cid:125)

(cid:123)(cid:122)

positive integers

(cid:125)

(cid:124)

(cid:124)

(cid:123)(cid:122)

integers

substructures

A substring of a string is a continuous string of symbols from the original string. A

subsequence of a sequence is a sequence of elements from the original sequence in order

but not necessarily continuous. With subgraphs there are two possible denitions. We

dene a subgraph of a graph to be a subset of the vertices and a subset of the edges of

the graph induced by the vertices. An induced subgraph is a subset of the vertices and

all the edges of the graph induced by the subset of vertices.

12.2 Asymptotic Notation

We introduce the big O notation here. A motivating example is analyzing the running

time of an algorithm. The running time may be a complicated function of the input length

n such as 5n3 + 25n2 ln n  6n + 22. Asymptotic analysis is concerned with the behavior

as n   where the higher order term 5n3 dominates. Further, the coecient 5 of 5n3

is not of interest since its value varies depending on the machine model. So we say that

the function is O(n3). The big O notation applies to functions on the positive integers

taking on positive real values.

Denition 12.1 For functions f and g from the natural numbers to the positive reals,

f (n) is O(g(n)) if there exists a constant c >0 such that for all n, f (n)  cg(n).

Thus, f (n) = 5n3 + 25n2 ln n  6n + 22 is O(n3). The upper bound need not be tight.

For example, in this case f (n) is also O(n4). Note in our denition we require g(n) to be

strictly greater than 0 for all n.

To say that the function f (n) grows at least as fast as g(n), one uses a notation Omega.

For positive real valued f and g, f (n) is (g(n)) if there exists a constant c > 0 such that

for all n, f (n)  cg(n). If f (n) is both O(g(n)) and (g(n)), then f (n) is (g(n)). Theta

is used when the two functions have the same asymptotic growth rate.

little o is used. We say f (n) is o(g(n)) if

Many times one wishes to bound the low order terms. To do this, a notation called

f (n)

lim

g(n) = 0. Note that f (n) being O(g(n))

n

means that asymptotically f (n) does not grow faster than g(n), whereas f (n) being

o(g(n)) means that asymptotically f (n)/g(n) goes to zero. If f (n) = 2n +

n, then f (n)



406

is O(n) but in bounding the lower order term, we write f (n) = 2n + o(n). Finally, we

f (n)

f (n)

write f (n)  g(n) if lim

g(n) = . The dierence

g(n) = 1 and say f (n) is (g(n)) if lim

n

n

between f (n) being (g(n)) and f (n)  g(n) is that in the rst case f (n) and g(n) may

dier by a multiplicative constant factor. We also note here that formally, O(g(n)) is a

set of functions, namely the set of functions f such that f (n) is O(g(n)); that is, f (n) is

O(g(n)) formally means that f (n)  O(g(n)).

asymptotic upper bound

f (n) is O(g(n)) if for all n, f (n)  cg(n) for some constant c > 0.

asymptotic lower bound

f (n) is (g(n)) if for all n, f (n)  cg(n) for some constant c > 0.

asymptotic equality

f (n) is (g(n)) if it is both O(g(n)) and (g(n)).

f (n) is o(g(n)) if lim

n

f (n)

g(n) = 0 .

f (n)  g(n) if lim

n

f (n)

g(n) = 1.

f (n) is  (g (n)) if

lim

n

f (n)

g(n) = .





=

<

=

>

407

12.3 Useful Relations

Summations

n

(cid:88)

i=0



(cid:88)

i=0



(cid:88)

i=0



(cid:88)

i=0

n

(cid:88)

i=1

n

(cid:88)

i=1



(cid:88)

i=1

ai = 1 + a + a2 +    =

ai = 1 + a + a2 +    =

1  an+1

1  a

,

a (cid:54)= 1

1

1  a

,

|a| < 1

iai = a + 2a2 + 3a3    =

a

(1  a)2 ,

|a| < 1

i2ai = a + 4a2 + 9a3    =

a(1 + a)

(1  a)3 ,

|a| < 1

i =

n(n + 1)

2

i2 =

n(n + 1)(2n + 1)

6

1

i2 =

2

6

We prove one equality:



(cid:88)

i=0

iai = a + 2a2 + 3a3    =

a

(1  a)2 , provided |a| < 1.

Proof: Write S =



(cid:80)

i=0

iai. So,

aS =



(cid:88)

i=0

iai+1 =



(cid:88)

i=1

(i  1)ai.

Thus,

S  aS =



(cid:88)

i=1

iai 



(cid:88)

i=1

(i  1)ai =



(cid:88)

i=1

ai =

a

1  a

,

from which the equality follows. The sum (cid:80)

i2ai can also be done by an extension of this

i

method (left to the reader). Using generating functions, we will see another proof of both

these equalities by derivatives.



(cid:88)

i=1

1

i

= 1 + 1

2 + (cid:0) 1

3 + 1

4

(cid:1) + (cid:0) 1

5 + 1

6 + 1

7 + 1

8

(cid:1) +     1 + 1

2 + 1

2 +    and thus diverges.

408

The summation

n

(cid:80)

i=1

1

i grows as ln n since

n

(cid:80)

i=1

 where  = 0.5772 is Eulers constant. Thus,

x=1

1

i  (cid:82) n

n

(cid:80)

i=1

1

i

1

x dx. In fact, lim

n

(cid:18) n

(cid:80)

i=1

= ln(n) +  for large n.

(cid:19)

1

i  ln(n)

=

Truncated Taylor series

If all the derivatives of a function f (x) exist, then we can write

f (x) = f (0) + f (cid:48)(0)x + f (cid:48)(cid:48)(0)

x2

2

+    .

The series can be truncated. In fact, there exists some y between 0 and x such that

Also, there exists some z between 0 and x such that

f (x) = f (0) + f (cid:48)(y)x.

f (x) = f (0) + f (cid:48)(0)x + f (cid:48)(cid:48)(z)

x2

2

and so on for higher derivatives. This can be used to derive inequalities. For example, if

f (x) = ln(1 + x), then its derivatives are

f (cid:48)(x) =

1

1 + x

; f (cid:48)(cid:48)(x) = 

1

(1 + x)2 ; f (cid:48)(cid:48)(cid:48)(x) =

2

(1 + x)3 .

For any z, f (cid:48)(cid:48)(z) < 0 and thus for any x, f (x)  f (0) + f (cid:48)(0)x, hence ln(1 + x)  x, which

also follows from the inequality 1 + x  ex. Also using

f (x) = f (0) + f (cid:48)(0)x + f (cid:48)(cid:48)(0)

x2

2

+ f (cid:48)(cid:48)(cid:48)(z)

x3

3!

for z > 1, f (cid:48)(cid:48)(cid:48)(z) > 0, and so for x > 1,

Exponentials and logs

ln(1 + x) > x 

x2

2

.

alog b = blog a

ex = 1 + x +

x2

2!

+

x3

3!

+   

e  2.718

1

e  0.3679.

Setting x = 1 in the equation ex = 1 + x + x2

2! + x3

3! +    yields e =



(cid:80)

i=0

1

i! .

409

(cid:0)1 + a

n

(cid:1)n = ea

lim

n

1

2

ln(1 + x) = x 

x2 +

1

3

x3 

1

4

x4   

|x| < 1

The above expression with x substituted for x gives rise to the approximations

ln(1  x) < x

which also follows from 1  x  ex, since ln(1  x) is a monotone function for x  (0, 1).

For 0 < x < 0.69, ln(1  x) > x  x2.

Trigonometric identities

eix = cos(x) + i sin(x)

cos(x) = 1

2 (eix + eix)

sin(x) = 1

2i (eix  eix)

sin(x  y) = sin(x) cos(y)  cos(x) sin(y)

cos(x  y) = cos(x) cos(y)  sin(x) sin(y)

cos (2) = cos2   sin2  = 1  2 sin2 

sin (2) = 2 sin  cos 

sin2 

2 (1  cos )

cos2 

2 (1 + cos )

2 = 1

2 = 1

410

Gaussian and related integrals

(cid:90)

(cid:90)

xeax2dx =

1

2a

eax2

1

a2+x2 dx = 1

a tan1 x

a thus



(cid:90)

1

a2+x2 dx = 

a



(cid:90)

e a2x2

2 dx =



2

a

thus



a



2



(cid:90)



a2x2

2 dx = 1

e

x2eax2dx =

(cid:114) 

a

1

4a

x2ne x2

a2 dx =





1  3  5    (2n  1)

2n+1

a2n1 =





(2n)!

n!

(cid:16) a

2

(cid:17)2n+1





(cid:90)

0



(cid:90)

0

(cid:90) 

0



(cid:90)

x2n+1e x2

a2 dx =

n!

2

a2n+2

ex2dx =





To verify



(cid:82)



ex2dx =





, consider

(cid:19)2

ex2dx

(cid:18) 

(cid:82)



=



(cid:82)



(cid:82)





e(x2+y2)dxdy. Let x =

r cos  and y = r sin . The Jacobian of this transformation of variables is

J (r, ) =

(cid:12)

(cid:12)

(cid:12)

(cid:12)

x

r

y

r

x



y



(cid:12)

(cid:12)

(cid:12)

(cid:12)

=

(cid:12)

(cid:12)

(cid:12)

(cid:12)

cos   r sin 

sin 

r cos 

(cid:12)

(cid:12)

(cid:12)

(cid:12)

= r

Thus,



2

ex2dx



=



(cid:90)









(cid:90)



(cid:90)





e(x2+y2)dxdy =



(cid:90)

2

(cid:90)

0

0

er2J (r, ) drd

= 2

Thus,



(cid:82)



ex2dx =



.

er2rdr

2

(cid:90)

d

=



(cid:90)

0

(cid:104) er2

2

0

(cid:105)

0

= 

411

The integral (cid:82) 

1 xrdx converges if r  1  (cid:15) and diverges if r  1 + (cid:15)

(cid:90) 

1

xrdx =

1

r + 1

xr+1

(cid:12)

(cid:12)

(cid:12)

(cid:12)



1

=

(cid:15)

r = 1  (cid:15)

1 = 1



1 =  r = 1 + (cid:15)



1

x(cid:15)

(cid:15)

 1

(cid:40)  1

(cid:12)

(cid:12)

(cid:15) x(cid:15)(cid:12)

(cid:12)

1 xrdx and (cid:80)

1

i1(cid:15) diverges since (cid:80)

i=1

1

i >

i=1

Thus (cid:80)

(cid:82) 

1 xrdx.

i=1

1

i1+(cid:15) converges since (cid:80)

i=2

1

i < (cid:82) 

Miscellaneous integrals

(cid:90) 1

x=0

x1(1  x)1dx =

()()

( + )

For denition of the gamma function see Section 12.4 Binomial coecients

The binomial coecient (cid:0)n

(nk)!k! is the number of ways of choosing k items from n.

The number of ways of choosing d + 1 items from n + 1 items equals the number of ways

of choosing the d + 1 items from the rst n items plus the number of ways of choosing d

of the items from the rst n items with the other item being the last of the n + 1 items.

(cid:1) = n!

k

(cid:19)

(cid:18)n

d

+

(cid:18) n

(cid:19)

d + 1

=

(cid:18)n + 1

d + 1

(cid:19)

.

The observation that the number of ways of choosing k items from 2n equals the

number of ways of choosing i items from the rst n and choosing k  i items from the

second n summed over all i, 0  i  k yields the identity

k

(cid:88)

i=0

(cid:18)n

i

(cid:19)(cid:18) n

(cid:19)

k  i

=

(cid:19)

.

(cid:18)2n

k

Setting k = n in the above formula and observing that (cid:0)n

(cid:1) = (cid:0) n

(cid:1) yields

i

ni

(cid:19)2

n

(cid:88)

i=0

(cid:18)n

i

=

(cid:19)

.

(cid:18)2n

n

More generally

k

(cid:80)

i=0

(cid:0)n

i

(cid:1)(cid:0) m

ki

(cid:1) = (cid:0)n+m

(cid:1) by a similar derivation.

k

412

12.4 Useful Inequalities

1 + x  ex for all real x.

One often establishes an inequality such as 1 + x  ex by showing that the dif-

ference of the two sides, namely ex  (1 + x), is always positive. This can be done

by taking derivatives. The rst and second derivatives are ex  1 and ex. Since ex

is always positive, ex  1 is monotonic and ex  (1 + x) is convex. Since ex  1 is

monotonic, it can be zero only once and is zero at x = 0. Thus, ex  (1 + x) takes

on its minimum at x = 0 where it is zero establishing the inequality.

(1  x)n  1  nx for 0  x  1

Let g(x) = (1  x)n  (1  nx). We establish g(x)  0 for x in [0, 1] by taking

the derivative.

g(cid:48)(x) = n(1  x)n1 + n = n(cid:0)1  (1  x)n1(cid:1)  0

for 0  x  1. Thus, g takes on its minimum for x in [0, 1] at x = 0 where g(0) = 0

proving the inequality.

(x + y)2  2x2 + 2y2

The inequality follows from (x + y)2 + (x  y)2 = 2x2 + 2y2.

Lemma 12.1 For any nonnegative reals a1, a2, . . . , an and any   [0, 1], (cid:0) (cid:80)n

(cid:80)n

i=1 ai

i=1 a

i .

(cid:1) 

Proof: We will see that we can reduce the proof of the lemma to the case when only one

of the ai is nonzero and the rest are zero. To this end, suppose a1 and a2 are both positive

and without loss of generality, assume a1  a2. Add an innitesimal positive amount (cid:15)

to a1 and subtract the same amount from a2. This does not alter the left hand side. We

claim it does not increase the right hand side. To see this, note that

(a1 + (cid:15)) + (a2  (cid:15))  a

1  a

2 = (a1

1  a1

2

)(cid:15) + O((cid:15)2),

1  a1

and since   1  0, we have a1

2  0, proving the claim. Now by repeating this

process, we can make a2 = 0 (at that time a1 will equal the sum of the original a1 and

a2). Now repeating on all pairs of ai, we can make all but one of them zero and in the

process, we have left the left hand side the same, but have not increased the right hand

side. So it suces to prove the inequality at the end which clearly holds. This method of

proof is called the variational method.

413

1 + x  ex for all real x

(1  x)n  1  nx for 0  x  1

(x + y)2  2x2 + 2y2

Triangle Inequality

|x + y|  |x| + |y|.

Cauchy-Schwartz Inequality

|x||y|  xT y

Youngs Inequality For positive real numbers p and q where 1

positive reals x and y,

p + 1

q = 1 and

xy 

xp +

yq.

1

p

1

q

Holders inequality For positive real numbers p and q with 1

p + 1

q = 1,

n

(cid:88)

i=1

|xiyi| 

(cid:32) n

(cid:88)

i=1

(cid:33)1/p (cid:32) n

(cid:88)

(cid:33)1/q

.

|yi|q

|xi|p

i=1

Jensens inequality For a convex function f , for 1 + . . . + n = 1, i  0,

(cid:32) n

(cid:88)

f

i=1

(cid:33)

ixi



n

(cid:88)

i=1

if (xi),

The Triangle Inequality

For any two vectors x and y, |x + y|  |x| + |y|. This can be seen by viewing x

and y as two sides of a triangle; equality holds i the angle  between x and y is 180

degrees. Formally, by the law of cosines we have |x + y|2 = |x|2 + |y|2  2|x||y| cos() 

|x|2 + |y|2 + 2|x||y| = (|x| + |y|)2. The inequality follows by taking square roots.

Stirling approximation

n! =

(cid:16)n

e

(cid:17)n 

2n

(cid:19)

(cid:18)2n

n

=



2n

nn

en < n! <



2n

nn

en

(cid:18)

1 +

1

12n  1

22n

1



n

(cid:19)

We prove the inequalities, except for constant factors. Namely, we prove that

1.4

(cid:16) n

e

(cid:17)n 

n  n!  e

(cid:17)n 

n.

(cid:16) n

e

414

Write ln(n!) = ln 1 + ln 2 +    + ln n. This sum is approximately (cid:82) n

indenite integral (cid:82) ln x dx = (x ln x  x) gives an approximation, but without the

term. To get the

means that for any positive x0,

x=1 ln x dx. The



n

n, dierentiate twice and note that ln x is a concave function. This



ln x0 + ln(x0 + 1)

2



(cid:90) x0+1

x=x0

ln x dx,

since for x  [x0, x0 + 1], the curve ln x is always above the spline joining (x0, ln x0) and

(x0 + 1, ln(x0 + 1)). Thus,

ln(n!) =



ln 1

2

(cid:90) n

x=1

+

ln 1 + ln 2

2

+

ln 2 + ln 3

2

+    +

ln(n  1) + ln n

2

+

ln n

2

ln x dx +

= [x ln x  x]n

1 +

ln n

2

ln n

2

ln n

2

.

= n ln n  n + 1 +

Thus, n!  nnen

x0  1/2 and any real 

ne. For the lower bound on n!, start with the fact that for any

ln x0 

1

2

Thus,

(ln(x0 + ) + ln(x0  ))

implies

ln x0 

(cid:90) x0+.5

x=x00.5

ln x dx.

ln(n!) = ln 2 + ln 3 +    + ln n 

(cid:90) n+.5

ln x dx,

x=1.5

from which one can derive a lower bound with a calculation.

Stirling approximation for the binomial coecient

(cid:19)

(cid:18)n

k



(cid:17)k

(cid:16) en

k

Using the Stirling approximation for k!,

(cid:19)

(cid:18)n

k

=

n!

(n  k)!k!



nk

k!

=

(cid:17)k

.

(cid:16) en

k

The gamma function

For a > 0



(cid:90)

 (a) =

xa1exdx

0



 (cid:0) 1

2

(cid:1) =

,  (1) =  (2) = 1, and for n  2,  (a) = (a  1) (a  1) .

415

To prove  (a) = (a  1) (a  1) use integration by parts.

(cid:90)

f (x) g(cid:48) (x) dx = f (x) g (x) 

(cid:90)

f (cid:48) (x) g (x) dx

Write (a) = (cid:82) 

x=0 f (x)g(cid:48)(x) dx, where, f (x) = xa1 and g(cid:48)(x) = ex. Thus,



(cid:90)

(a) =

xa1exdx = [f (x)g(x)]

x=0 +

(cid:90) 

x=0

(a  1)xa2ex dx

0

= lim

x

xa1ex + (a  1)(a  1) = (a  1)(a  1),

as claimed.

Cauchy-Schwartz Inequality

(cid:32) n

(cid:88)

(cid:33) (cid:32) n

(cid:88)

(cid:33)

y2

i



(cid:32) n

(cid:88)

xiyi

(cid:33)2

x2

i

i=1

i=1

i=1

In vector form, |x||y|  xT y, the inequality states that the dot product of two vectors

is at most the product of their lengths. The Cauchy-Schwartz inequality is a special case

of Holders inequality with p = q = 2.

Youngs inequality

For positive real numbers p and q where 1

p + 1

q = 1 and positive reals x and y,

1

p

xp +

1

q

yq  xy.

The left hand side of Youngs inequality, 1

q yq, is a convex combination of xp and yq

since 1

q sum to 1. ln(x) is a concave function for x > 0 and so the ln of the convex

combination of the two elements is greater than or equal to the convex combination of

the ln of the two elements

p and 1

p xp + 1

ln(

1

p

xp +

1

q

yp) 

1

p

ln(xp) +

1

q

ln(yq) = ln(xy).

Since for x  0, ln x is a monotone increasing function, 1

p xp + 1

q yq  xy..

Holders inequality

416

For positive real numbers p and q with 1

p + 1

q = 1,

n

(cid:88)

i=1

|xiyi| 

(cid:32) n

(cid:88)

i=1

(cid:33)1/p (cid:32) n

(cid:88)

(cid:33)1/q

.

|yi|q

|xi|p

i=1

i = xi / ((cid:80)n

i=1 |xi|p)1/p and y(cid:48)

Let x(cid:48)

i does not change the inequality. Now (cid:80)n

y(cid:48)

(cid:80)n

i=1 |x(cid:48)

i|  1. Apply Youngs inequality to get |x(cid:48)

i = yi / ((cid:80)n

i=1 |x(cid:48)

i=1 |yi|q)1/q. Replacing xi by x(cid:48)

i and yi by

i|p = (cid:80)n

i|q = 1, so it suces to prove

i=1 |y(cid:48)

i|p

i|  |x(cid:48)

i|q

p + |y(cid:48)

iy(cid:48)

q . Summing over i, the

q = 1 nishing the proof.

right hand side sums to 1

p + 1

iy(cid:48)

For a1, a2, . . . , an real and k a positive integer,

(a1 + a2 +    + an)k  nk1(|a1|k + |a2|k +    + |an|k).

Using Holders inequality with p = k and q = k/(k  1),

|a1 + a2 +    + an|  |a1  1| + |a2  1| +    + |an  1|

(cid:32) n

(cid:88)



|ai|k

(cid:33)1/k

i=1

(1 + 1 +    + 1)(k1)/k ,

from which the current inequality follows.

Arithmetic and geometric means

The arithmetic mean of a set of nonnegative reals is at least their geometric mean.

For a1, a2, . . . , an > 0,

1

n

n

(cid:88)

i=1



ai  n

a1a2    an.

Assume that a1  a2  . . .  an. We reduce the proof to the case when all the ai are equal

using the variational method. In this case the inequality holds with equality. Suppose

a1 > a2. Let  be a positive innitesimal. Add  to a2 and subtract  from a1 to get closer

to the case when they are equal. The left hand side 1

n

i=1 ai does not change.

(cid:80)n

(a1  )(a2 + )a3a4    an = a1a2    an + (a1  a2)a3a4    an + O(2)

> a1a2    an



a1a2    an. So if the inequality

for small enough  > 0. Thus, the change has increased n

holds after the change, it must hold before. By continuing this process, one can make all

the ai equal.

Approximating sums by integrals

417

n+1

(cid:82)

x=m

f (x)dx 

n

(cid:80)

i=m

f (i) 

n

(cid:82)

x=m1

f (x)dx

m  1 m

n n + 1

Figure 12.1: Approximating sums by integrals

For monotonic decreasing f (x),

n+1

(cid:90)

x=m

f (x)dx 

n

(cid:88)

i=m

f (i) 

n

(cid:90)

x=m1

f (x)dx.

See Fig. 12.1. Thus,

n+1

(cid:90)

x=2

1

x2 dx 

n

(cid:88)

i=2

and hence 3

2  1

n+1 

n

(cid:80)

i=1

1

i2  2  1

n .

1

i2 = 1

4 + 1

9 +    + 1

n2 

n

(cid:82)

x=1

1

x2 dx

Jensens Inequality

For a convex function f ,

(cid:18) 1

2

f

(cid:19)

(x1 + x2)



1

2

(f (x1) + f (x2)) .

More generally for any convex function f ,

(cid:32) n

(cid:88)

f

i=1

(cid:33)

ixi



n

(cid:88)

i=1

if (xi),

where 0  i  1 and

random variable x,

n

(cid:80)

i=1

i = 1. From this, it follows that for any convex function f and

E (f (x))  f (E (x)) .

418

We prove this for a discrete random variable x taking on values a1, a2, . . . with Prob(x =

ai) = i:

E(f (x)) =

(cid:88)

i

if (ai)  f

(cid:33)

(cid:32)

(cid:88)

i

iai

= f (E(x)).

f (x1)

f (x2)

x1

x2

Figure 12.2: For a convex function f , f (cid:0) x1+x2

2

(cid:1)  1

2 (f (x1) + f (x2)) .

Example: Let f (x) = xk for k an even positive integer. Then, f (cid:48)(cid:48)(x) = k(k  1)xk2

which since k  2 is even is nonnegative for all x implying that f is convex. Thus,

E (x)  k(cid:112)E (xk),

k is a monotone function of t, t > 0. It is easy to see that this inequality does not

since t 1

necessarily hold when k is odd; indeed for odd k, xk is not a convex function.

Tails of Gaussians

For bounding the tails of Gaussian densities, the following inequality is useful. The

proof uses a technique useful in many contexts. For t > 0,

(cid:90) 

x=t

ex2 dx 

et2

2t

.

In proof, rst write: (cid:82) 

t ex2 dx, using the fact that x  t in the range of

integration. The latter expression is integrable in closed form since d(ex2) = (2x)ex2

yielding the claimed bound.

x=t ex2 dx  (cid:82) 

x=t

x

A similar technique yields an upper bound on

(cid:90) 1

x=

(1  x2) dx,

419

for   [0, 1] and  > 0. Just use (1  x2)  x

last expression.

 (1  x2) over the range and integrate the

(cid:90) 1

x=

(1  x2)dx 

=

(1  x2)dx =

(cid:90) 1

x



x=

(1  2)+1

2( + 1)

1

2( + 1)

(1  x2)+1

(cid:12)

(cid:12)

(cid:12)

(cid:12)

1

x=

12.5 Probability

Consider an experiment such as ipping a coin whose outcome is determined by chance.

To talk about the outcome of a particular experiment, we introduce the notion of a ran-

dom variable whose value is the outcome of the experiment. The set of possible outcomes

is called the sample space. If the sample space is nite, we can assign a probability of

occurrence to each outcome. In some situations where the sample space is innite, we can

1

assign a probability of occurrence. The probability p (i) = 6

i2 for i an integer greater

2

than or equal to one is such an example. The function assigning the probabilities is called

a probability distribution function.

In many situations, a probability distribution function does not exist. For example,

for the uniform probability on the interval [0,1], the probability of any specic value is

zero. What we can do is dene a probability density function p(x) such that

Prob(a < x < b) =

b

(cid:90)

a

p(x)dx

If x is a continuous random variable for which a density function exists, then the cumu-

lative distribution function f (a) is dened by

f (a) =

(cid:90) a



p(x)dx

which gives the probability that x  a.

12.5.1 Sample Space, Events, and Independence

There may be more than one relevant random variable in a situation. For example, if

one tosses n coins, there are n random variables, x1, x2, . . . , xn, taking on values 0 and 1,

a 1 for heads and a 0 for tails. The set of possible outcomes, the sample space, is {0, 1}n.

An event is a subset of the sample space. The event of an odd number of heads, consists

of all elements of {0, 1}n with an odd number of 1s.

420

Let A and B be two events. The joint occurrence of the two events is denoted by

(AB). The conditional probability of event A given that event B has occurred is denoted

by Prob(A|B)and is given by

Prob(A|B) =

Prob(A  B)

Prob(B)

.

Events A and B are independent if the occurrence of one event has no inuence on the

probability of the other. That is, Prob(A|B) = Prob(A) or equivalently, Prob(A  B) =

Prob(A)Prob(B). Two random variables x and y are independent if for every possible set

A of values for x and every possible set B of values for y, the events x in A and y in B

are independent.

A collection of n random variables x1, x2, . . . , xn is mutually independent if for all

possible sets A1, A2, . . . , An of values of x1, x2, . . . , xn,

Prob(x1  A1, x2  A2, . . . , xn  An) = Prob(x1  A1)Prob(x2  A2)    Prob(xn  An).

If the random variables are discrete, it would suce to say that for any real numbers

a1, a2, . . . , an

Prob(x1 = a1, x2 = a2, . . . , xn = an) = Prob(x1 = a1)Prob(x2 = a2)    Prob(xn = an).

Random variables x1, x2, . . . , xn are pairwise independent if for any ai and aj, i (cid:54)= j,

Prob(xi = ai, xj = aj) = Prob(xi = ai)Prob(xj = aj). Mutual independence is much

stronger than requiring that the variables are pairwise independent. Consider the exam-

ple of 2-universal hash functions discussed in Chapter ??.

If (x, y) is a random vector and one normalizes it to a unit vector

(cid:18)

x

x2+y2

,

y

x2+y2

(cid:19)

the coordinates are no longer independent since knowing the value of one coordinate

uniquely determines the absolute value of the other.

12.5.2 Linearity of Expectation

An important concept is that of the expectation of a random variable. The expected

xp(x) in the discrete case and E(x) =

value, E(x), of a random variable x is E(x) = (cid:80)



(cid:82)

xp(x)dx in the continuous case. The expectation of a sum of random variables is equal

x



to the sum of their expectations. The linearity of expectation follows directly from the

denition and does not require independence.

421

12.5.3 Union Bound

Let A1, A2, . . . , An be events. The actual probability of the union of events is given

by Booles formula.

Prob(A1  A2     An) =

n

(cid:88)

i=1

Prob(Ai) 

(cid:88)

ij

Prob(Ai  Aj) +

(cid:88)

ijk

Prob(Ai  Aj  Ak)    

Often we only need an upper bound on the probability of the union and use

Prob(A1  A2     An) 

n

(cid:88)

i=1

Prob(Ai)

This upper bound is called the union bound.

12.5.4 Indicator Variables

A useful tool is that of an indicator variable that takes on value 0 or 1 to indicate

whether some quantity is present or not. The indicator variable is useful in determining

the expected size of a subset. Given a random subset of the integers {1, 2, . . . , n}, the

expected size of the subset is the expected value of x1 + x2 +    + xn where xi is the

indicator variable that takes on value 1 if i is in the subset.

Example: Consider a random permutation of n integers. Dene the indicator function

xi = 1 if the ith integer in the permutation is i. The expected number of xed points is

given by

(cid:32) n

(cid:88)

E

(cid:33)

xi

=

i=1

n

(cid:88)

i=1

E(xi) = n

1

n

= 1.

Note that the xi are not independent. But, linearity of expectation still applies.

Example: Consider the expected number of vertices of degree d in a random graph

G(n, p). The number of vertices of degree d is the sum of n indicator random variables, one

for each vertex, with value one if the vertex has degree d. The expectation is the sum of the

expectations of the n indicator random variables and this is just n times the expectation

of one of them. Thus, the expected number of degree d vertices is n(cid:0)n

(cid:1)pd(1  p)nd.

d

12.5.5 Variance

In addition to the expected value of a random variable, another important parameter

is the variance. The variance of a random variable x, denoted var(x) or often 2(x) is

E (x  E (x))2 and measures how close to the expected value the random variable is likely

to be. The standard deviation  is the square root of the variance. The units of  are the

same as those of x.

422

By linearity of expectation

2 = E (x  E (x))2 = E(x2)  2E(x)E(x) + E2(x) = E (cid:0)x2(cid:1)  E2 (x) .

12.5.6 Variance of the Sum of Independent Random Variables

In general, the variance of the sum is not equal to the sum of the variances. However,

if x and y are independent, then E (xy) = E (x) E (y) and

var(x + y) = var (x) + var (y) .

To see this

var(x + y) = E (cid:0)(x + y)2(cid:1)  E2(x + y)

= E(x2) + 2E(xy) + E(y2)  E2(x)  2E(x)E(y)  E2(y).

From independence, 2E(xy)  2E(x)E(y) = 0 and

var(x + y) = E(x2)  E2(x) + E(y2)  E2(y)

= var(x) + var(y).

More generally, if x1, x2, . . . , xn are pairwise independent random variables, then

var(x1 + x2 +    + xn) = var(x1) + var(x2) +    + var(xn).

For the variance of the sum to be the sum of the variances only requires pairwise inde-

pendence not full independence.

12.5.7 Median

One often calculates the average value of a random variable to get a feeling for the

magnitude of the variable. This is reasonable when the probability distribution of the

variable is Gaussian, or has a small variance. However, if there are outliers, then the

average may be distorted by outliers. An alternative to calculating the expected value is

to calculate the median, the value for which half of the probability is above and half is

below.

12.5.8 The Central Limit Theorem

Let s = x1 + x2 +    + xn be a sum of n independent random variables where each xi

has probability distribution

xi =

(cid:26) 0 with probability 0.5

1 with probability 0.5

.

The expected value of each xi is 1/2 with variance

(cid:18) 1

2

(cid:19)2 1

2

(cid:18) 1

2

2

i =

 0

+

 1

(cid:19)2 1

2

=

1

4

.

423

The expected value of s is n/2 and since the variables are independent, the variance of

the sum is the sum of the variances and hence is n/4. How concentrated s is around its



n

mean depends on the standard deviation of s which is

2 . For n equal 100 the expected

value of s is 50 with a standard deviation of 5 which is 10% of the mean. For n = 10, 000

the expected value of s is 5,000 with a standard deviation of 50 which is 1% of the

mean. Note that as n increases, the standard deviation increases, but the ratio of the

standard deviation to the mean goes to zero. More generally, if xi are independent and

identically distributed, each with standard deviation , then the standard deviation of

has standard deviation . The central limit

x1 + x2 +    + xn is

theorem makes a stronger assertion that in fact x1+x2++xn

has Gaussian distribution

with standard deviation .

n. So, x1+x2++xn







n

n

Theorem 12.2 Suppose x1, x2, . . . , xn is a sequence of identically distributed independent

random variables, each with mean  and variance 2. The distribution of the random

variable

1



n

(x1 + x2 +    + xn  n)

converges to the distribution of the Gaussian with mean 0 and variance 2.

12.5.9 Probability Distributions

The Gaussian or normal distribution

The normal distribution is



1

2

e 1

2

(xm)2

2

where m is the mean and 2 is the variance. The coecient

makes the integral of

the distribution be one. If we measure distance in units of the standard deviation  from

the mean, then

2

1

Standard tables give values of the integral

(x) =

1



2

e 1

2 x2.

t

(cid:90)

0

(x)dx

and from these values one can compute probability integrals for a normal distribution

with mean m and variance 2.

General Gaussians

So far we have seen spherical Gaussian densities in Rd. The word spherical indicates

that the level curves of the density are spheres. If a random vector y in Rd has a spherical

424

Gaussian density with zero mean, then yi and yj, i (cid:54)= j, are independent. However, in

many situations the variables are correlated. To model these Gaussians, level curves that

are ellipsoids rather than spheres are used.

For a random vector x, the covariance of xi and xj is E((xi  i)(xj  j)). We list

the covariances in a matrix called the covariance matrix, denoted .47 Since x and  are

column vectors, (x  )(x  )T is a d  d matrix. Expectation of a matrix or vector

means componentwise expectation.

 = E(cid:0)(x  )(x  )T (cid:1).

The general Gaussian density with mean  and positive denite covariance matrix  is

f (x) =

1

(cid:112)(2)d det()

(cid:18)

exp



1

2

(x  )T 1(x  )

(cid:19)

.

To compute the covariance matrix of the Gaussian, substitute y = 1/2(x  ). Noting

that a positive denite symmetric matrix has a square root:

E((x  )(x  )T = E(1/2yyT 1/2)

= 1/2 (cid:0)E(yyT )(cid:1) 1/2 = .

The density of y is the unit variance, zero mean Gaussian, thus E(yyT ) = I.

Bernoulli trials and the binomial distribution

A Bernoulli trial has two possible outcomes, called success or failure, with probabilities

p and 1  p, respectively. If there are n independent Bernoulli trials, the probability of

exactly k successes is given by the binomial distribution

B (n, p) =

(cid:19)

(cid:18)n

k

pk(1  p)nk

The mean and variance of the binomial distribution B(n, p) are np and np(1  p), respec-

tively. The mean of the binomial distribution is np, by linearity of expectations. The

variance is np(1  p) since the variance of a sum of independent random variables is the

sum of their variances.

Let x1 be the number of successes in n1 trials and let x2 be the number of successes

in n2 trials. The probability distribution of the sum of the successes, x1 + x2, is the same

as the distribution of x1 + x2 successes in n1 + n2 trials. Thus, B (n1, p) + B (n2, p) =

B (n1 + n2, p).

47 is the standard notation for the covariance matrix. We will use it sparingly so as not to confuse

with the summation sign.

425

When p is a constant, the expected degree of vertices in G (n, p) increases with n. For

(cid:1), the expected degree of a vertex is (n  1)/2. In many applications,

example, in G (cid:0)n, 1

we will be concerned with G (n, p) where p = d/n, for d a constant; i.e., graphs whose

expected degree is a constant d independent of n. Holding d = np constant as n goes to

innity, the binomial distribution

2

Prob (k) =

(cid:19)

(cid:18)n

k

pk (1  p)nk

approaches the Poisson distribution

Prob(k) =

(np)k

k!

enp =

dk

k!

ed.

To see this, assume k = o(n) and use the approximations n  k = n, (cid:0)n

(cid:0)1  1

(cid:1)nk = e1 to approximate the binomial distribution by

k

n

(cid:1) = nk

k! , and

(cid:19)

(cid:18)n

k

lim

n

pk(1  p)nk =

(cid:19)k

nk

k!

(cid:18) d

n

(1 

d

n

)n =

dk

k!

ed.

Note that for p = d

n, where d is a constant independent of n, the probability of the bi-

nomial distribution falls o rapidly for k > d, and is essentially zero for all but some

nite number of values of k. This justies the k = o(n) assumption. Thus, the Poisson

distribution is a good approximation.

Poisson distribution

The Poisson distribution describes the probability of k events happening in a unit of

time when the average rate per unit of time is . Divide the unit of time into n segments.

When n is large enough, each segment is suciently small so that the probability of two

events happening in the same segment is negligible. The Poisson distribution gives the

probability of k events happening in a unit of time and can be derived from the binomial

distribution by taking the limit as n  .

Let p = 

n . Then

(cid:19)k (cid:18)

(cid:19)nk

(cid:18)n

(cid:19) (cid:18) 

n

k

(cid:19)k (cid:18)

(cid:18) 

n

1 



n

1 



n

(cid:19)n (cid:18)

1 

(cid:19)k



n

Prob(k successes in a unit of time) = lim

n

= lim

n

n (n  1)    (n  k + 1)

k!

= lim

n

k

k!

e

426

In the limit as n goes to innity the binomial distribution p (k) = (cid:0)n

(cid:1)pk (1  p)nk be-

comes the Poisson distribution p (k) = e k

k! . The mean and the variance of the Poisson

distribution have value . If x and y are both Poisson random variables from distributions

with means 1 and 2 respectively, then x + y is Poisson with mean m1 + m2. For large

n and small p the binomial distribution can be approximated with the Poisson distribution.

k

The binomial distribution with mean np and variance np(1  p) can be approximated

by the normal distribution with mean np and variance np(1p). The central limit theorem

tells us that there is such an approximation in the limit. The approximation is good if

both np and n(1  p) are greater than 10 provided k is not extreme. Thus,

(cid:18)n

k

(cid:19) (cid:18)1

2

(cid:19)k (cid:18) 1

2

(cid:19)nk

=

1

(cid:112)n/2

e

 (n/2k)2

1

2 n

.

This approximation is excellent provided k is (n). The Poisson approximation

(cid:19)

(cid:18)n

k

pk (1  p)k = enp (np)k

k!

is o for central values and tail values even for p = 1/2. The approximation

(cid:19)

(cid:18) n

k

pk (1  p)nk =



e (pnk)2

pn

1

pn

is good for p = 1/2 but is o for other values of p.

Generation of random numbers according to a given probability distribution

Suppose one wanted to generate a random variable with probability density p(x) where

p(x) is continuous. Let P (x) be the cumulative distribution function for x and let u be

a random variable with uniform probability density over the interval [0,1]. Then the ran-

dom variable x = P 1 (u) has probability density p(x).

Example: For a Cauchy density function the cumulative distribution function is

P (x) =

x

(cid:90)

t=

1



1

1 + t2 dt =

1

2

+

1



tan1 (x) .

(cid:1)(cid:1). Thus, to generate a

Setting u = P (x) and solving for x yields x = tan (cid:0) (cid:0)u  1

random number x  0 using the Cauchy distribution, generate u, 0  u  1, uniformly

(cid:1)(cid:1) . The value of x varies from  to  with P (0) = 1/2.

and calculate x = tan (cid:0) (cid:0)u  1

2

2

For the probability distribution Prob(x = i) = 6

2

butions Prob(x = i) = c 1

variance.

i3 and Prob(xi = 2i) = 1

1

i2 E(x) = . The probability distri-

4i have nite expectation but innite

427

12.5.10 Bayes Rule and Estimators

Bayes rule

Bayes rule relates the conditional probability of A given B to the conditional proba-

bility of B given A.

Prob (A|B) =

Prob (B|A) Prob (A)

Prob (B)

Suppose one knows the probability of A and wants to know how this probability changes

if we know that B has occurred. Prob(A) is called the prior probability. The conditional

probability Prob(A|B) is called the posterior probability because it is the probability of

A after we know that B has occurred.

The example below illustrates that if a situation is rare, a highly accurate test will

often give the wrong answer.

Example: Let A be the event that a product is defective and let B be the event that a

test says a product is defective. Let Prob(B|A) be the probability that the test says a

product is defective assuming the product is defective and let Prob (cid:0)B| A(cid:1) be the proba-

bility that the test says a product is defective if it is not actually defective.

What is the probability Prob(A|B) that the product is defective if the test says it is

defective? Suppose Prob(A) = 0.001, Prob(B|A) = 0.99, and Prob (cid:0)B| A(cid:1) = 0.02. Then

Prob (B) = Prob (B|A) Prob (A) + Prob (cid:0)B| A(cid:1) Prob (cid:0) A(cid:1)

= 0.99  0.001 + 0.02  0.999

= 0.02087

and

Prob (A|B) =

Prob (B|A) Prob (A)

Prob (B)



0.99  0.001

0.0210

= 0.0471

Even though the test fails to detect a defective product only 1% of the time when it

is defective and claims that it is defective when it is not only 2% of the time, the test

is correct only 4.7% of the time when it says a product is defective. This comes about

because of the low frequencies of defective products.

The words prior, a posteriori, and likelihood come from Bayes theorem.

a posteriori =

likelihood  prior

normalizing constant

Prob (A|B) =

Prob (B|A) Prob (A)

Prob (B)

The a posteriori probability is the conditional probability of A given B. The likelihood

is the conditional probability Prob(B|A).

428

Unbiased Estimators

2. For this distribution, m = x1+x2++xn

Consider n samples x1, x2, . . . , xn from a Gaussian distribution of mean  and variance

is an unbiased estimator of , which means

(xi  )2 is an unbiased estimator of 2. However, if  is not

that E(m) =  and 1

n

n

n

(cid:80)

i=1

known and is approximated by m, then 1

n1

n

(cid:80)

i=1

(xi  m)2 is an unbiased estimator of 2.

Maximum Likelihood Estimation MLE

Suppose the probability distribution of a random variable x depends on a parameter

r. With slight abuse of notation, since r is a parameter rather than a random variable, we

denote the probability distribution of x as p (x|r) . This is the likelihood of observing x if

r was in fact the parameter value. The job of the maximum likelihood estimator, MLE,

is to nd the best r after observing values of the random variable x. The likelihood of r

being the parameter value given that we have observed x is denoted L(r|x). This is again

not a probability since r is a parameter, not a random variable. However, if we were to

apply Bayes rule as if this was a conditional probability, we get

L(r|x) =

Prob(x|r)Prob(r)

Prob(x)

.

Now, assume Prob(r) is the same for all r. The denominator Prob(x) is the absolute

probability of observing x and is independent of r. So to maximize L(r|x), we just maxi-

mize Prob(x|r). In some situations, one has a prior guess as to the distribution Prob(r).

This is then called the prior and in that case, we call Prob(x|r) the posterior which we

try to maximize.

Example: Consider ipping a coin 100 times. Suppose 62 heads and 38 tails occur.

What is the maximum likelihood value of the probability of the coin to come down heads

when the coin is ipped? In this case, it is r = 0.62. The probability that we get 62 heads

if the unknown probability of heads in one trial is r is

Prob (62 heads|r) =

(cid:19)

(cid:18)100

62

r62(1  r)38.

This quantity is maximized when r = 0.62. To see this take the derivative with respect

to r of r62(1  r)38 The derivative is zero at r = 0.62 and the second derivative is negative

indicating a maximum. Thus, r = 0.62 is the maximum likelihood estimator of the

probability of heads in a trial.

429

12.6 Bounds on Tail Probability

12.6.1 Cherno Bounds

Markovs inequality bounds the probability that a nonnegative random variable exceeds

a value a.

or

p (cid:0)x  aE(x)(cid:1) 

p(x  a) 

E(x)

a

.

1

a

If one also knows the variance, 2, then using Chebyshevs inequality one can bound the

probability that a random variable diers from its expected value by more than a standard

deviations. Let m = E(x). Then Chebyshevs inequality states that

p(|x  m|  a) 

1

a2

If a random variable s is the sum of n independent random variables x1, x2, . . . , xn of

nite variance, then better bounds are possible. Here we focus on the case where the

n independent variables are binomial. In the next section we consider the more general

case where we have independent random variables from any distribution that has a nite

variance.

Let x1, x2, . . . , xn be independent random variables where

xi =

(cid:26) 0 Prob 1  p

1 Prob

p

.

n

(cid:80)

i=1

Consider the sum s =

xi. Here the expected value of each xi is p and by linearity

of expectation, the expected value of the sum is m=np. Cherno bounds bound the

probability that the sum s exceeds (1 + ) m or is less than (1  ) m. We state these

bounds as Theorems 12.3 and 12.4 below and give their proofs.

Theorem 12.3 For any  > 0, Prob (cid:0)s > (1 + )m(cid:1) <

(cid:16)

e

(1+)(1+)

(cid:17)m

Theorem 12.4 Let 0 <   1, then Prob (cid:0)s < (1  )m(cid:1) <

(cid:16)

e

(1+)(1+)

(cid:17)m

< e 2m

2

.

Proof (Theorem 12.3): For any  > 0, the function ex is monotone. Thus,

Prob (cid:0)s > (1 + )m(cid:1) = Prob (cid:0)es > e(1+)m(cid:1) .

ex is nonnegative for all x, so we can apply Markovs inequality to get

Prob (cid:0)es > e(1+)m(cid:1)  e(1+)mE (cid:0)es(cid:1) .

430

Since the xi are independent,

E (cid:0)es(cid:1) = E

(cid:33)

(cid:32)

e



n

(cid:80)

i=1

xi

= E

(cid:32) n

(cid:89)

(cid:33)

exi

=

E (cid:0)exi(cid:1)

n

(cid:89)

i=1

=

n

(cid:89)

i=1

(cid:0)ep + 1  p(cid:1) =

i=1

n

(cid:89)

i=1

(cid:0)p(e  1) + 1(cid:1).

Using the inequality 1 + x < ex with x = p(e  1) yields

E (cid:0)es(cid:1) <

n

(cid:89)

i=1

ep(e1).

Thus, for all  > 0

Prob (cid:0)s > (1 + )m(cid:1)  Prob (cid:0)es > e(1+)m(cid:1)

 e(1+)mE (cid:0)es(cid:1)

 e(1+)m

n

(cid:89)

i=1

ep(e1).

Setting  = ln(1 + )

Prob (cid:0)s > (1 + )m(cid:1)  (cid:0)e ln(1+)(cid:1)(1+)m

n

(cid:89)

i=1

ep(eln(1+)1)







(cid:18) 1

(cid:19)(1+)m n

(cid:89)

ep

1 + 

(cid:18) 1

(1 + )

i=1

(cid:19)(1+)m

enp

(cid:32)

e

(1 + )(1+)

(cid:33)m

.

To simplify the bound of Theorem 12.3, observe that

(1 + ) ln (1 + ) =  +

2

2



3

6

+

4

12

    .

Therefore

and hence

(1 + )(1+) = e+ 2

2  3

6 + 4

12 

431

e

(1+)(1+) = e 2

2 + 3

6 .

Thus, the bound simplies to

Prob (cid:0)s < (1 + ) m(cid:1)  e 2

2 m+ 3

6 m.

For small  the probability drops exponentially with 2.

When  is large another simplication is possible. First

Prob (cid:0)s > (1 + ) m(cid:1) 

(cid:32)

e

(1 + )(1+)

(cid:33)m



(cid:18) e

(cid:19)(1+)m

1 + 

If  > 2e  1, substituting 2e  1 for  in the denominator yields

Prob(s > (1 + ) m)  2(1+)m.

Theorem 12.3 gives a bound on the probability of the sum being signicantly greater

than the mean. We now prove Theorem 12.4, bounding the probability that the sum will

be signicantly less than its mean.

Proof (Theorem 12.4): For any  > 0

Prob (cid:0)s < (1  )m(cid:1) = Prob (cid:0)  s > (1  )m(cid:1) = Prob (cid:0)es > e(1)m(cid:1) .

Applying Markovs inequality

Now

Thus,

Since 1 + x < ex

Setting  = ln 1

1

Prob (cid:0)s < (1  )m(cid:1) <

E(ex)

e(1)m <

n

(cid:81)

E(eXi)

i=1

e(1)m .

E(exi) = pe + 1  p = 1 + p(e  1) + 1.

Prob(s < (1  )m) <

Prob (cid:0)s < (1  )m(cid:1) <

n

(cid:81)

i=1

[1 + p(e  1)]

.

e(1)m

enp(e1)

e(1)m .

Prob (cid:0)s < (1  )m(cid:1) <

<

432

enp(11)

(1  )(1)m

(cid:18)

e

(1  )(1)

(cid:19)m

.

But for 0 <   1, (1  )(1) > e+ 2

2 . To see this note that

(1  ) ln (1  ) = (1  )

 

(cid:18)

2

2



3

3

(cid:19)

   

    + 2 +

(cid:18) 3

(cid:19)

2



+

+

(cid:19)

3

2

3

3

4

3

+   

+   

=  

=  +

=  +

  +

3

3

2

2

3

6



2

2

(cid:18)

2 

2

2

2

2

+

.

+   

It then follows that

Prob (cid:0)s < (1  )m(cid:1) <

(cid:18)

e

(1  )(1)

(cid:19)m

< e m2

2

.

12.6.2 More General Tail Bounds

The main purpose of this section is to state the Master Tail bounds theorem of Chapter

2 (with more detail), give a proof of it, and derive the other tail inequalities mentioned

in the table in that chapter. Recall that Markovs inequality bounds the tail probability

of a nonnegative random variable x based only on its expectation. For a > 0,

Pr(x > a) 

E(x)

a

.

As a grows, the bound drops o as 1/a. Given the second moment of x, recall that

Chebyshevs inequality, which does not assume x is a nonnegative random variable, gives

a tail bound falling o as 1/a2

Pr(|x  E(x)|  a) 

E

(cid:16)(cid:0)x  E(x)(cid:1)2(cid:17)

a2

.

Higher moments yield bounds by applying either of these two theorems. For example,

if r is a nonnegative even integer, then xr is a nonnegative random variable even if x takes

on negative values. Applying Markovs inequality to xr,

Pr(|x|  a) = Pr(xr  ar) 

E(xr)

ar

,

433

a bound that falls o as 1/ar. The larger the r, the greater the rate of fall, but a bound

on E(xr) is needed to apply this technique.

For a random variable x that is the sum of a large number of independent random

variables, x1, x2, . . . , xn, one can derive bounds on E(xr) for high even r. There are many

situations where the sum of a large number of independent random variables arises. For

example, xi may be the amount of a good that the ith consumer buys, the length of the ith

message sent over a network, or the indicator random variable of whether the ith record

in a large database has a certain property. Each xi is modeled by a simple probability

distribution. Gaussian, exponential probability density (at any t > 0 is et), or binomial

distributions are typically used, in fact, respectively in the three examples here. If the

xi have 0-1 distributions, then the Cherno bounds described in Section 12.6.1 can be

used to bound the tails of x = x1 + x2 +    + xn. But exponential and Gaussian random

variables are not bounded so the proof technique used in Section 12.6.1 does not apply.

However, good bounds on the moments of these two distributions are known. Indeed, for

any integer s > 0, the sth moment for the unit variance Gaussian and the exponential are

both at most s!.

Given bounds on the moments of individual xi the following theorem proves moment

bounds on their sum. We use this theorem to derive tail bounds not only for sums of 0-1

random variables, but also Gaussians, exponentials, Poisson, etc.

The gold standard for tail bounds is the central limit theorem for independent, iden-

tically distributed random variables x1, x2,    , xn with zero mean and Var(xi) = 2 that

n tends to the Gaus-

states as n   the distribution of x = (x1 + x2 +    + xn)/

sian density with zero mean and variance 2. Loosely, this says that in the limit, the

tails of x = (x1 + x2 +    + xn)/

n are bounded by that of a Gaussian with variance

2. But this theorem is only in the limit, whereas, we prove a bound that applies for all n.





In the following theorem, x is the sum of n independent, not necessarily identically

distributed, random variables x1, x2, . . . , xn, each of zero mean and variance at most 2.

By the central limit theorem, in the limit the probability density of x goes to that of

the Gaussian with variance at most n2. In a limit sense, this implies an upper bound

of cea2/(2n2) for the tail probability Pr(|x| > a) for some constant c. The following

theorem assumes bounds on higher moments, and asserts a quantitative upper bound of

3ea2/(12n2) on the tail probability, not just in the limit, but for every n. We will apply

this theorem to get tail bounds on sums of Gaussian, binomial, and power law distributed

random variables.

Theorem 12.5 Let x = x1 + x2 +    + xn, where x1, x2, . . . , xn are mutually independent

2n2] and

random variables with zero mean and variance at most 2. Suppose a  [0,



434

s  n2/2 is a positive even integer and |E(xr

i )|  2r!, for r = 3, 4, . . . , s. Then,

Pr (|x1 + x2 +    xn|  a) 

(cid:18) 2sn2

a2

(cid:19)s/2

.

If further, s  a2/(4n2), then we also have:

Pr (|x1 + x2 +    xn|  a)  3ea2/(12n2).

Proof: We rst prove an upper bound on E(xr) for any even positive integer r and then

use Markovs inequality as discussed earlier. Expand (x1 + x2 +    + xn)r.

(x1 + x2 +    + xn)r =

(cid:88) (cid:18)

r

r1, r2, . . . , rn

(cid:19)

1 xr2

xr1

2    xrn

n

=

(cid:88) r!

r1!r2!    rn!

1 xr2

xr1

2    xrn

n

where the ri range over all nonnegative integers summing to r. By independence

E(xr) =

(cid:88) r!

r1!r2!    rn!

E(xr1

1 )E(xr2

2 )    E(xrn

n ).

If in a term, any ri = 1, the term is zero since E(xi) = 0. Assume henceforth that

(r1, r2, . . . , rn) runs over sets of nonzero ri summing to r where each nonzero ri is at least

two. There are at most r/2 nonzero ri in each set. Since |E(xri

i )|  2ri!,

E(xr)  r!

(cid:88)

2( number of nonzero ri in set).

(r1,r2,...,rn)

(cid:1)

Collect terms of the summation with t nonzero ri for t = 1, 2, . . . , r/2. There are (cid:0)n

subsets of {1, 2, . . . , n} of cardinality t. Once a subset is xed as the set of t values of i

with nonzero ri, set each of the ri  2. That is, allocate two to each of the ri and then

allocate the remaining r  2t to the t ri arbitrarily. The number of such allocations is just

(cid:0)r2t+t1

t1

(cid:1) = (cid:0)rt1

(cid:1). So,

t1

t

E(xr)  r!

r/2

(cid:88)

t=1

f (t), where

f (t) =

(cid:18)n

t

(cid:19)(cid:18)r  t  1

(cid:19)

2t.

t  1

Thus f (t)  h(t), where h(t) = (n2)t

t! 2rt1. Since t  r/2  n2/4, we have

h(t)

h(t  1)

=

n2

2t

 2.

So, we get

E(xr) = r!

r/2

(cid:88)

t=1

f (t)  r!h(r/2)(1 +

1

2

+

1

4

+    ) 

r!

(r/2)!

2r/2(n2)r/2.

435

Applying Markov inequality,

Pr(|x| > a) = Pr(|x|r > ar) 

r!(n2)r/22r/2

(r/2)!ar

= g(r) 

(cid:18) 2rn2

a2

(cid:19)r/2

.

This holds for all r  s, r even and applying it with r = s, we get the rst inequality of

the theorem.

We now prove the second inequality. For even r, g(r)/g(r  2) = 4(r1)n2

and so

g(r) decreases as long as r  1  a2/(4n2). Taking r to be the largest even integer

less than or equal to a2/(6n2), the tail probability is at most er/2, which is at most

e  ea2/(12n2)  3  ea2/(12n2), proving the theorem.

a2

12.7 Applications of the Tail Bound

Cherno Bounds

Cherno bounds deal with sums of Bernoulli random variables. Here we apply Theo-

rem 12.5 to derive these.

Theorem 12.6 Suppose y1, y2, . . . , yn are independent 0-1 random variables with E(yi) =

p for all i. Let y = y1 + y2 +    + yn. Then for any c  [0, 1],

Prob(cid:0)|y  E(y)|  cnp(cid:1)  3enpc2/8.

Proof: Let xi = yi  p. Then, E(xi) = 0 and E(x2

i ) = E(y  p)2 = p. For s  3,

|E(xs

i )| = |E(yi  p)s|

= |p(1  p)s + (1  p)(0  p)s|

= (cid:12)

 p.

(cid:12)p(1  p) (cid:0)(1  p)s1 + (p)s1(cid:1)(cid:12)

(cid:12)

Apply Theorem 12.5 with a = cnp. Noting that a <



2 np, completes the proof.

Section (12.6.1) contains a dierent proof that uses a standard method based on

moment-generating functions and gives a better constant in the exponent.

Power Law Distributions

The power law distribution of order k where k is a positive integer is

f (x) =

k  1

xk

for x  1.

If a random variable x has this distribution for k  4, then

 = E(x) =

k  1

k  2

and Var(x) =

k  1

(k  2)2(k  3)

.

436

Theorem 12.7 Suppose x1, x2, . . . , xn are i.i.d, each distributed according to the Power

Law of order k  4 (with n > 10k2). Then, for x = x1 + x2 +    + xn, and any

  (1/(2

nk), 1/k2), we have



Pr (|x  E(x)|  E(x)) 

(cid:18)

4

2(k  1)n

(cid:19)(k3)/2

.

Proof: For integer s, the sth moment of xi  E(xi), namely, E((xi  )s), exists if and

only if s  k  2. For s  k  2,

E((xi  )s) = (k  1)

(cid:90) 

1

(y  )s

yk

dy

Using the substitution of variable z = /y

(y  )s

yk

As y goes from 1 to , z goes from  to 0, and dz =  

= ysk(1  z)s =

zks

ks (1  z)s

y2 dy. Thus

E((xi  )s) =(k  1)

(cid:90) 

(y  )s

yk

dy

1

k  1

ks1

=

(cid:90) 1

0

(1  z)szks2dz +

k  1

ks1

(cid:90) 

1

(1  z)szks2dz.

The rst integral is just the standard integral of the beta function and its value is s!(k2s)!

To bound the second integral, note that for z  [1, ], |z  1|  1

(k1)!

k2 and

.

zks2  (cid:0)1 + (cid:0)1/(k  2)(cid:1)(cid:1)ks2

 e(ks2)/(k2)  e.

(cid:18) 1

So, |E((xi  )s)| 

(k  1)s!(k  2  s)!

(k  1)!

+

e(k  1)

(k  2)s+1  s!Var(y)

+

(cid:19)

e

3!

 s!Var(x).

k  4

Now, apply the rst inequality of Theorem 12.5 with s of that theorem set to k  2 or

2n2 (since   1/k2). The present

k  3 whichever is even. Note that a = E(x) 

theorem follows by a calculation.



12.8 Eigenvalues and Eigenvectors

Let A be an nn real matrix. The scalar  is called an eigenvalue of A if there exists a

nonzero vector x satisfying the equation Ax = x. The vector x is called the eigenvector

of A associated with . The set of all eigenvectors associated with a given eigenvalue form

a subspace as seen from the fact that if Ax = x and Ay = y, then for any scalars c

and d, A(cx + dy) = (cx + dy). The equation Ax = x has a nontrivial solution only if

det(A  I) = 0. The equation det(A  I) = 0 is called the characteristic equation and

has n not necessarily distinct roots.

Matrices A and B are similar if there is an invertible matrix P such that A = P 1BP .

437

Theorem 12.8 If A and B are similar, then they have the same eigenvalues.

Proof: Let A and B be similar matrices. Then there exists an invertible matrix P

such that A = P 1BP . For an eigenvector x of A with eigenvalue , Ax = x, which

implies P 1BP x = x or B(P x) = (P x). So, P x is an eigenvector of B with the same

eigenvalue . Since the reverse also holds, the theorem follows.

Even though two similar matrices, A and B, have the same eigenvalues, their eigen-

vectors are in general dierent.

The matrix A is diagonalizable if A is similar to a diagonal matrix.

Theorem 12.9 A is diagonalizable if and only if A has n linearly independent eigenvec-

tors.

Proof:

(only if ) Assume A is diagonalizable. Then there exists an invertible matrix P

and a diagonal matrix D such that D = P 1AP . Thus, P D = AP . Let the diago-

nal elements of D be 1, 2, . . . , n and let p1, p2, . . . , pn be the columns of P . Then

AP = [Ap1, Ap2, . . . , Apn] and P D = [1p1, 2p2, . . . , npn] . Hence Api = ipi. That

is, the i are the eigenvalues of A and the pi are the corresponding eigenvectors. Since P

is invertible, the pi are linearly independent.

(if ) Assume that A has n linearly independent eigenvectors p1, p2, . . . , pn with cor-

responding eigenvalues 1, 2, . . . , n. Then Api = ipi and reversing the above steps

AP = [Ap1, Ap2, . . . , Apn] = [1p1, 2p2, . . . npn] = P D.

Thus, AP = P D. Since the pi are linearly independent, P is invertible and hence A =

P DP 1. Thus, A is diagonalizable.

It follows from the proof of the theorem that if A is diagonalizable and has eigenvalue

 with multiplicity k, then there are k linearly independent eigenvectors associated with .

A matrix P is orthogonal if it is invertible and P 1 = P T . A matrix A is orthogonally

diagonalizable if there exists an orthogonal matrix P such that P 1AP = D is diagonal.

If A is orthogonally diagonalizable, then A = P DP T and AP = P D. Thus, the columns

of P are the eigenvectors of A and the diagonal elements of D are the corresponding

eigenvalues.

If P is an orthogonal matrix, then P T AP and A are both representations of the same

linear transformation with respect to dierent bases. To see this, note that if e1, e2, . . . , en

is the standard basis, then aij is the component of Aej along the direction ei, namely,

T Aej. Thus, A denes a linear transformation by specifying the image under the

aij = ei

438

transformation of each basis vector. Denote by pj the jth column of P . It is easy to see that

T Apj.

(P T AP )ij is the component of Apj along the direction pi, namely, (P T AP )ij = pi

Since P is orthogonal, the pj form a basis of the space and so P T AP represents the same

linear transformation as A, but in the basis p1, p2, . . . , pn.

Another remark is in order. Check that

A = P DP T =

n

(cid:88)

i=1

diipipi

T .

Compare this with the singular value decomposition where

A =

n

(cid:88)

i=1

iuivi

T ,

the only dierence being that ui and vi can be dierent and indeed if A is not square,

they will certainly be.

12.8.1 Symmetric Matrices

For an arbitrary matrix, some of the eigenvalues may be complex. However, for a

symmetric matrix with real entries, all eigenvalues are real. The number of eigenvalues

of a symmetric matrix, counting multiplicities, equals the dimension of the matrix. The

set of eigenvectors associated with a given eigenvalue form a vector space. For a non-

symmetric matrix, the dimension of this space may be less than the multiplicity of the

eigenvalue. Thus, a nonsymmetric matrix may not be diagonalizable. However, for a

symmetric matrix the eigenvectors associated with a given eigenvalue form a vector space

of dimension equal to the multiplicity of the eigenvalue. Thus, all symmetric matrices are

diagonalizable. The above facts for symmetric matrices are summarized in the following

theorem.

Theorem 12.10 (Real Spectral Theorem) Let A be a real symmetric matrix. Then

1. The eigenvalues, 1, 2, . . . , n, are real, as are the components of the corresponding

eigenvectors, v1, v2, . . . , vn.

2. (Spectral Decomposition) A is orthogonally diagonalizable and indeed

A = V DV T =

n

(cid:88)

i=1

ivivi

T ,

where V is the matrix with columns v1, v2, . . . , vn, |vi| = 1 and D is a diagonal

matrix with entries 1, 2, . . . , n.

439

Proof: Avi = ivi and vi

pose. Then

cAvi = ivi

cvi. Here the c superscript means conjugate trans-

i = vi

cAvi = (vi

cAvi)cc = (vi

cAcvi)c = (vi

cAvi)c = c

i

and hence i is real.

Since i is real, a nontrivial solution to (A  iI) x = 0 has real components.

Let P be a real symmetric matrix such that P v1 = e1 where e1 = (1, 0, 0, . . . , 0)T and

P 1 = P T . We will construct such a P shortly. Since Av1 = 1v1,

P AP T e1 = P Av1 = P v1 = 1e1.

The condition P AP T e1 = 1e1 plus symmetry implies that P AP T =

(cid:19)

(cid:18) 1 0

0 A(cid:48)

where

A(cid:48) is n  1 by n  1 and symmetric. By induction, A(cid:48) is orthogonally diagonalizable. Let

Q be the orthogonal matrix with QA(cid:48)QT = D(cid:48), a diagonal matrix. Q is (n  1)  (n  1).

Augment Q to an n  n matrix by putting 1 in the (1, 1) position and 0 elsewhere in the

rst row and column. Call the resulting matrix R. R is orthogonal too.

(cid:18) 1 0

0 A(cid:48)

R

(cid:19)

RT =

(cid:19)

(cid:18) 1 0

0 D(cid:48)

= RP AP T RT =

(cid:18) 1 0

0 D(cid:48)

(cid:19)

.

Since the product of two orthogonal matrices is orthogonal, this nishes the proof of (2)

except it remains to construct P . For this, take an orthonormal basis of space containing

v1. Suppose the basis is {v1, w2, w3, . . .} and V is the matrix with these basis vectors as

its columns. Then P = V T will do.

Theorem 12.11 (The fundamental theorem of symmetric matrices) A real ma-

trix A is orthogonally diagonalizable if and only if A is symmetric.

Proof: (if ) Assume A is orthogonally diagonalizable. Then there exists P such that

D = P 1AP . Since P 1 = P T , we get

A = P DP 1 = P DP T

AT = (P DP T )T = P DP T = A

which implies

and hence A is symmetric.

(only if ) Already proved.

Note that a nonsymmetric matrix may not be diagonalizable, it may have eigenvalues

that are not real, and the number of linearly independent eigenvectors corresponding to

an eigenvalue may be less than its multiplicity. For example, the matrix









1 1 0

0 1 1

1 0 1

440

has eigenvalues 2, 1



3

2 . The matrix

(1  )2 = 0 and thus has eigenvalue 1 with multiplicity 2 but has only one linearly

has characteristic equation

2 , and 1

2  i

2 + i



3

(cid:19)

(cid:18) 1 2

0 1

independent eigenvector associated with the eigenvalue 1, namely x = c

Neither of these situations is possible for a symmetric matrix.

(cid:19)

(cid:18) 1

0

c (cid:54)= 0.

12.8.2 Relationship between SVD and Eigen Decomposition

The singular value decomposition exists for any n  d matrix whereas the eigenvalue

decomposition exists only for certain square matrices. For symmetric matrices the de-

compositions are essentially the same.

E and singular value decomposition A = USDSV T

The singular values of a matrix are always positive since they are the sum of squares

of the projection of a row of a matrix onto a singular vector. Given a symmetric matrix,

the eigenvalues can be positive or negative. If A is a symmetric matrix with eigenvalue

decomposition A = VEDEV T

S , what is

the relationship between DE and DS, and between VE and VS, and between US and VE?

Observe that if A can be expressed as QDQT where Q is orthonormal and D is diagonal,

then AQ = QD. That is, each column of Q is an eigenvector and the elements of D

are the eigenvalues. Thus, if the eigenvalues of A are distinct, then Q is unique up to

a permutation of columns. If an eigenvalue has multiplicity k, then the space spanned

the k columns is unique.

In the following we will use the term essentially unique to

SU T

capture this situation. Now AAT = USD2

S . By an argument

similar to the one above, US and VS are essentially unique and are the eigenvectors or

negatives of the eigenvectors of A and AT . The eigenvalues of AAT or AT A are the squares

of the eigenvalues of A. If A is not positive semi denite and has negative eigenvalues,

then in the singular value decomposition A = USDSVS, some of the left singular vectors

are the negatives of the eigenvectors. Let S be a diagonal matrix with 1(cid:48)s on the

diagonal depending on whether the corresponding eigenvalue is positive or negative. Then

A = (USS)(SDS)VS where USS = VE and SDS = DE.

S and AT A = VSD2

SV T

12.8.3 Extremal Properties of Eigenvalues

In this section we derive a min max characterization of eigenvalues that implies that

the largest eigenvalue of a symmetric matrix A has a value equal to the maximum of

xT Ax over all vectors x of unit length. That is, the largest eigenvalue of A equals the

2-norm of A. If A is a real symmetric matrix there exists an orthogonal matrix P that

diagonalizes A. Thus

P T AP = D

where D is a diagonal matrix with the eigenvalues of A, 1  2      n, on its

diagonal. Rather than working with A, it is easier to work with the diagonal matrix D.

This will be an important technique that will simplify many proofs.

441

Consider maximizing xT Ax subject to the conditions

1.

n

(cid:80)

i=1

x2

i = 1

2. rT

i x = 0,

1  i  s

where the ri are any set of nonzero vectors. We ask over all possible sets {ri|1  i  s}

of s vectors, what is the minimum value assumed by this maximum.

Theorem 12.12 (Min max theorem) For a symmetric matrix A, min

r1,...,rs

(xtAx) =

max

x

rix

s+1 where the minimum is over all sets {r1, r2, . . . , rs} of s nonzero vectors and the

maximum is over all unit vectors x orthogonal to the s nonzero vectors.

Proof: A is orthogonally diagonalizable. Let P satisfy P T P = I and P T AP = D, D

diagonal. Let y = P T x. Then x = P y and

xT Ax = yT P T AP y = yT Dy =

n

(cid:88)

i=1

iy2

i

i = 1 is equivalent to maximizing

Since there is a one-to-one correspondence between unit vectors x and y, maximizing

xT Ax subject to (cid:80) x2

n

(cid:80)

i=1

iy2

i at 1. Then x = P y is the rst

column of P and is the rst eigenvector of A. Similarly n is the minimum value of xT Ax

subject to the same conditions.

1  i, 2  i  n, y = (1, 0, . . . , 0) maximizes

i subject to (cid:80) y2

i = 1. Since

n

(cid:80)

i=1

iy2

Now consider maximizing xT Ax subject to the conditions

1. (cid:80) x2

i = 1

2. rT

i x = 0

where the ri are any set of nonzero vectors. We ask over all possible choices of s vectors

what is the minimum value assumed by this maximum.

min

r1,...,rs

max

x

rT

i x=0

xT Ax

As above, we may work with y. The conditions are

1. (cid:80) y2

i = 1

2. qT

i y = 0 where, qT

i = rT

i P

442

Consider any choice for the vectors r1, r2, . . . , rs. This gives a corresponding set of qi. The

yi therefore satisfy s linear homogeneous equations. If we add ys+2 = ys+3 =    yn = 0

we have n  1 homogeneous equations in n unknowns y1, . . . , yn. There is at least one

solution that can be normalized so that (cid:80) y2

i = 1. With this choice of y

yT Dy =

(cid:88)

iy2

i s+1

since coecients greater than or equal to s + 1 are zero. Thus, for any choice of ri there

will be a y such that

and hence

(yT P T AP y)  s+1

max

y

rT

i y=0

min

r1,r2,...,rs

(yT P T AP y)  s+1.

max

y

rT

i y=0

However, there is a set of s constraints for which the minimum is less than or equal to

s+1. Fix the relations to be yi = 0, 1  i  s. There are s equations in n unknowns

and for any y subject to these relations

yT Dy =

n

(cid:88)

s+1

iy2

i  s+1.

Combining the two inequalities, min max yT Dy = s+1.

The above theorem tells us that the maximum of xT Ax subject to the constraint that

|x|2 = 1 is 1. Consider the problem of maximizing xT Ax subject to the additional re-

striction that x is orthogonal to the rst eigenvector. This is equivalent to maximizing

ytP tAP y subject to y being orthogonal to (1,0,. . . ,0), i.e. the rst component of y being

0. This maximum is clearly 2 and occurs for y = (0, 1, 0, . . . , 0). The corresponding x is

the second column of P or the second eigenvector of A.

Similarly the maximum of xT Ax for p1

T x = p2

T x =    ps

T x = 0 is s+1 and is

obtained for x = ps+1.

12.8.4 Eigenvalues of the Sum of Two Symmetric Matrices

The min max theorem is useful in proving many other results. The following theorem

shows how adding a matrix B to a matrix A changes the eigenvalues of A. The theorem

is useful for determining the eect of a small perturbation on the eigenvalues of A.

Theorem 12.13 Let A and B be n  n symmetric matrices. Let C=A+B. Let i, i,

and i denote the eigenvalues of A, B, and C respectively, where 1  2  . . . n and

similarly for i, i. Then s + 1  s  s + n.

443

Proof: By the min max theorem we have

s = min

r1,...,rs1

(xT Ax).

max

x

rix

Suppose r1, r2, . . . , rs1 attain the minimum in the expression. Then using the min max

theorem on C,

(cid:0)xT (A + B)x(cid:1)

s 



max

xr1,r2,...rs1

max

xr1,r2,...rs1

(xT Ax) +

max

xr1,r2,...rs1

(xT Bx)

 s + max

x

(xT Bx)  s + 1.

Therefore, s  s + 1.

An application of the result to A = C + (B), gives s  s  n. The eigenvalues

of -B are minus the eigenvalues of B and thus n is the largest eigenvalue. Hence

s  s + n and combining inequalities yields s + 1  s  s + n.

Lemma 12.14 Let A and B be n  n symmetric matrices. Let C=A+B. Let i, i,

and i denote the eigenvalues of A, B, and C respectively, where 1  2  . . . n and

similarly for i, i. Then r+s1  r + s.

Proof: There is a set of r1 relations such that over all x satisfying the r1 relationships

And a set of s  1 relations such that over all x satisfying the s  1 relationships

max(xT Ax) = r.

max(xT Bx) = s.

Consider x satisfying all these r + s  2 relations. For any such x

xT Cx = xT Ax + xT Bxx  r + s

and hence over all the x

max(xT Cx)  s + r

Taking the minimum over all sets of r + s  2 relations

r+s1 = min max(xT Cx)  r + s

444

12.8.5 Norms

T xj = 0 for i (cid:54)= j and is orthonormal if

A set of vectors {x1, . . . , xn} is orthogonal if xi

in addition |xi| = 1 for all i. A matrix A is orthonormal if AT A = I. If A is a square

orthonormal matrix, then rows as well as columns are orthogonal. In other words, if A

is square orthonormal, then AT is also. In the case of matrices over the complexes, the

concept of an orthonormal matrix is replaced by that of a unitary matrix. A is the con-

jugate transpose of A if a

ij is the complex

conjugate of the ijth element of A. A matrix A over the eld of complex numbers is

unitary if AA = I.

ij is the ijth entry of A and a

ij = aji where a

Norms

A norm on Rn is a function f : Rn  R satisfying the following three axioms:

1. f (x)  0,

2. f (x + y)  f (x) + f (y), and

3. f (x) = ||f (x).

A norm on a vector space provides a distance function where

distance(x, y) = norm(x  y).

An important class of norms for vectors is the p-norms dened for p > 0 by

|x|p = (|x1|p +    + |xn|p)

1

p .

Important special cases are

|x|0 = the number of non zero entries (not quite a norm: fails #3)

|x|1 = |x1| +    + |xn|

|x|2 = (cid:112)|x1|2 +    + |xn|2

|x| = max |xi|.

Lemma 12.15 For any 1  p < q, |x|q  |x|p.

Proof:

|x|q

q =

(cid:88)

|xi|q.

i

Let ai = |xi|q and  = p/q. Using Jensens inequality (see Section 12.4) that for any

nonnegative reals a1, a2, . . . , an and any   (0, 1), we have ((cid:80)n

i , the

lemma is proved.

i=1 ai)  (cid:80)n

i=1 a

445

There are two important matrix norms, the matrix p-norm

and the Frobenius norm

||A||p = max

|x|=1

(cid:107)Ax(cid:107)p

||A||F =

(cid:115)(cid:88)

a2

ij.

ij

Let ai be the ith column of A. Then (cid:107)A(cid:107)2

F = (cid:80)

F = tr (cid:0)AAT (cid:1). Thus, (cid:107)A(cid:107)2

i

T ai = tr (cid:0)AT A(cid:1). A similar argument

ai

F = tr (cid:0)AT A(cid:1) = tr (cid:0)AAT (cid:1).

on the rows yields (cid:107)A(cid:107)2

If A is symmetric and rank k

||A||2

2  ||A||2

F  k ||A||2

2 .

12.8.6 Important Norms and Their Properties

Lemma 12.16 ||AB||2  ||A||2 ||B||2

Proof: ||AB||2 = max

|x|=1

let z = By. Then

|ABx|. Let y be the value of x that achieves the maximum and

||AB||2 = |ABy| = |Az| =

(cid:12)

(cid:12)

(cid:12)

(cid:12)

A

z

|z|

(cid:12)

(cid:12)

(cid:12)

(cid:12)

|z|

But

(cid:12)

(cid:12)

(cid:12)A z

|z|

(cid:12)

(cid:12)

(cid:12)  max

|x|=1

|Ax| = ||A||2 and |z|  max

|x|=1

|Bx| = ||B||2. Thus ||AB||2  ||A||2 ||B||2.

Let Q be an orthonormal matrix.

Lemma 12.17 For all x,

|Qx| = |x|.

Proof: |Qx|2

2 = xT QT Qx = xT x = |x|2

2.

Lemma 12.18 ||QA||2 = ||A||2

Proof: For all x,

|Qx| = |x|. Replacing x by Ax, |QAx| = |Ax| and thus max

|x|=1

|QAx| =

max

|x|=1

|Ax|

Lemma 12.19 ||AB||2

F  ||A||2

F ||B||2

F

Proof: Let ai be the ith column of A and let bj be the jth column of B. By the

Cauchy-Schwartz inequality (cid:13)

2 

(cid:13)

(cid:13)  (cid:107)ai(cid:107) (cid:107)bj(cid:107). Thus ||AB||2

F = (cid:80)

(cid:12)

(cid:12)ai

T bj

T bj

(cid:13)ai

(cid:12)

(cid:12)

(cid:80)

j

i

(cid:80)

i

(cid:80)

j

(cid:107)ai(cid:107)2 (cid:107)bj(cid:107)2 = (cid:80)

(cid:107)ai(cid:107)2 (cid:80)

i

j

(cid:107)bj(cid:107)2 = ||A||2

F ||B||2

F

446

Lemma 12.20 ||QA||F = ||A||F

Proof: ||QA||2

F = Tr(AT QT QA) = Tr(AT A) = ||A||2

F .

Lemma 12.21 For real, symmetric matrix A with eigenvalues 1  2  . . ., (cid:107)A(cid:107)2

2 +    + 2

max(2

n

n) and (cid:107)A(cid:107)2

1 + 2

1, 2

F = 2

2 =

Proof: Suppose the spectral decomposition of A is P DP T , where P is an orthogo-

nal matrix and D is diagonal. We saw that ||P T A||2 = ||A||2. Applying this again,

||P T AP ||2 = ||A||2. But, P T AP = D and clearly for a diagonal matrix D, ||D||2 is the

largest absolute value diagonal entry from which the rst equation follows. The proof of

the second is analogous.

If A is real and symmetric and of rank k then ||A||2

2  ||A||2

F  k ||A||2

2

Theorem 12.22 ||A||2

2  ||A||2

Proof: It is obvious for diagonal matrices that ||D||2

2. Let D =

QtAQ where Q is orthonormal. The result follows immediately since for Q orthonormal,

||QA||2 = ||A||2 and ||QA||F = ||A||F .

F  k ||D||2

F  k ||A||2

2  ||D||2

2

Real and symmetric are necessary for some of these theorems. This condition was

needed to express  = QT AQ. For example, in Theorem 12.22 suppose A is the n  n

matrix



A =









1 1

1 1

...

...

1 1



0

.









||A||2 = 2 and ||A||F =

2n. But A is rank 2 and ||A||F > 2 ||A||2 for n > 8.



Lemma 12.23 Let A be a symmetric matrix. Then (cid:107)A(cid:107)2 = max

|x|=1

(cid:12)xT Ax(cid:12)

(cid:12)

(cid:12).

Proof: By denition, the 2-norm of A is (cid:107)A(cid:107)2 = max

|x|=1

|Ax|. Thus,

(cid:107)A(cid:107)2 = max

|x|=1

|Ax| = max

|x|=1



xT AT Ax = (cid:112)2

1 = 1 = max

|x|=1

(cid:12)xT Ax(cid:12)

(cid:12)

(cid:12)

The two norm of a matrix A is greater than or equal to the 2-norm of any of its

columns. Let au be a column of A.

Lemma 12.24 |au|  (cid:107)A(cid:107)2

Proof: Let eu be the unit vector with a 1 in position u and all other entries zero. Note

 = max

|x|=1

|Ax|. Let x = eu where au is row u. Then |au| = |Aeu|  max

|x|=1

|Ax| = 

447

12.8.7 Additional Linear Algebra

Lemma 12.25 Let A be an n  n symmetric matrix. Then det(A) = 12    n.

Proof: The det (A  I) is a polynomial in  of degree n. The coecient of n will be 1

depending on whether n is odd or even. Let the roots of this polynomial be 1, 2, . . . , n.

Then det(A  I) = (1)n

(  i). Thus

n

(cid:81)

i=1

det(A) = det(A  I)|=0 = (1)n

n

(cid:89)

i=1

(cid:12)

(cid:12)

(cid:12)

(  i)

(cid:12)

(cid:12)=0

= 12    n

The trace of a matrix is dened to be the sum of its diagonal elements. That is,

tr (A) = a11 + a22 +    + ann.

Lemma 12.26 tr(A) = 1 + 2 +    + n.

Proof: Consider the coecient of n1 in det(A  I) = (1)n

n

(cid:81)

i=1

(  i). Write

A  I =







a11   a12

a21

...

 

a22     

...

...





 .

Calculate det(A  I) by expanding along the rst row. Each term in the expansion

involves a determinant of size n  1 which is a polynomial in  of deg n  2 except for

the principal minor which is of deg n  1. Thus the term of deg n  1 comes from

and has coecient (1)n1 (a11 + a22 +    + ann). Now

(a11  ) (a22  )    (ann  )

(1)n

n

(cid:89)

i=1

(  i) = (1)n (  1)(  2)    (  n)

= (1)n (cid:16)

n  (1 + 2 +    + n)n1 +   

(cid:17)

Therefore equating coecients 1 + 2 +    + n = a11 + a22 +    + ann = tr(A)

Note that (tr(A))2 (cid:54)= tr(A2). For example A =

(cid:18) 1 0

0 2

has trace 5 (cid:54)=9. However tr(A2) = 2

n. To see this, observe that A2 =

(V T DV )2 = V T D2V . Thus, the eigenvalues of A2 are the squares of the eigenvalues for

A.

(cid:18) 1 0

0 4

has trace 3, A2 =

2 +    + 2

1 + 2

(cid:19)

(cid:19)

448

Alternative proof that tr(A) = 1+ 2+   + n : Suppose the spectral decomposition

of A is A = P DP T . We have

tr (A) = tr (cid:0)P DP T (cid:1) = tr (cid:0)DP T P (cid:1) = tr (D) = 1 + 2 +    + n.

Lemma 12.27 If A is n  m and B is a m  n matrix, then tr(AB)=tr(BA).

tr(AB) =

n

(cid:88)

m

(cid:88)

aijbji =

m

(cid:88)

n

(cid:88)

i=1

j=1

j=1

i=1

bjiaij = tr (BA)

Pseudo inverse

Let A be an n  m rank r matrix and let A = U V T be the singular value decompo-

(cid:17)

where 1, . . . , r are the nonzero singular

It is the unique X that

sition of A. Let (cid:48) = diag

values of A. Then A(cid:48) = V (cid:48)U T is the pseudo inverse of A.

minimizes (cid:107)AX  I(cid:107)F .

, . . . , 1

r

, 0, . . . , 0

(cid:16) 1

1

Second eigenvector

Suppose the eigenvalues of a matrix are 1  2     . The second eigenvalue,

2, plays an important role for matrices representing graphs. It may be the case that

|n| > |2|.

Why is the second eigenvalue so important? Consider partitioning the vertices of a

regular degree d graph G = (V, E) into two blocks of equal size so as to minimize the

number of edges between the two blocks. Assign value +1 to the vertices in one block and

1 to the vertices in the other block. Let x be the vector whose components are the 1

values assigned to the vertices. If two vertices, i and j, are in the same block, then xi and

xj are both +1 or both 1 and (xi xj)2 = 0. If vertices i and j are in dierent blocks then

(xi  xj)2 = 4. Thus, partitioning the vertices into two blocks so as to minimize the edges

between vertices in dierent blocks is equivalent to nding a vector x with coordinates

1 of which half of its coordinates are +1 and half of which are 1 that minimizes

Ecut =

1

4

(cid:88)

(i,j)E

(xi  xj)2

Let A be the adjacency matrix of G. Then

xT Ax = (cid:80)

aijxixj = 2 (cid:80)

(cid:18) number of edges

ij

edges

xixj

(cid:19)

= 2 

= 2 

within components

(cid:18) total number

of edges

(cid:19)

 4 

(cid:18) number of edges

(cid:19)

 2 

(cid:18) number of edges

between components

(cid:19)

between components

449

Maximizing xT Ax over all x whose coordinates are 1 and half of whose coordinates are

+1 is equivalent to minimizing the number of edges between components.

Since nding such an x is computationally dicult, one thing we can try to do is

replace the integer condition on the components of x and the condition that half of the

components are positive and half of the components are negative with the conditions

n

(cid:80)

i=1

it is easy to see that the rst eigenvector is along 1

xi = 0. Then nding the optimal x gives us the second eigenvalue since

x2

i = 1 and

n

(cid:80)

i=1

2 = max

xv1

xT Ax

(cid:80) x2

i

Actually we should use

(cid:18) total number

of edges

2 

(cid:19)

n

(cid:80)

i=1

 4 

n

(cid:80)

i=1

x2

x2

i = 1. Thus n2 must be greater than

i = n not

(cid:18) number of edges

(cid:19)

since the maximum is taken over

between components

a larger set of x. The fact that 2 gives us a bound on the minimum number of cross

edges is what makes it so important.

12.8.8 Distance between subspaces

Suppose S1 and S2 are two subspaces. Choose a basis of S1 and arrange the basis

vectors as the columns of a matrix X1; similarly choose a basis of S2 and arrange the

basis vectors as the columns of a matrix X2. Note that S1 and S2 can have dierent

dimensions. Dene the square of the distance between two subspaces by

dist2(S1, S2) = dist2(X1, X2) = ||X1  X2X T

2 X1||2

F

Since X1  X2X T

2 X1 and X2X T

F = (cid:13)

(cid:107)X1(cid:107)2

2 X1 are orthogonal

(cid:13)

F + (cid:13)

2

(cid:13)

(cid:13)X1  X2X T

2 X1

(cid:13)X2X T

2 X1

(cid:13)

2

(cid:13)

F

and hence

dist2 (X1, X2) = (cid:107)X1(cid:107)2

F  (cid:13)

(cid:13)X2X T

2 X1

(cid:13)

2

F .

(cid:13)

Intuitively, the distance between X1 and X2 is the Frobenius norm of the component of

X1 not in the space spanned by the columns of X2.

If X1 and X2 are 1-dimensional unit length vectors, dist2 (X1, X2) is the sine squared

of the angle between the spaces.

Example: Consider two subspaces in four dimensions











X1 =

1

2

0

1

2

0

X2 =









1 0

0 1

0 0

0 0



















0

1

3

1

3

1

3

450

Here

dist2 (X1, X2) =

=



















(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

1

2

0

1

2

0

0

0

1

2

0

1

2

0

1

2

0

0

1

3

1

3

1

3











(cid:13)

2

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

F









1 0

0 1

0 0

0 0









(cid:18) 1 0 0 0

0 1 0 0

(cid:19)











=

7

6

0

1

3

1

3

1

3

0

0

1

3

1

3





















2

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

(cid:13)

F

In essence, we projected each column vector of X1 onto X2 and computed the Frobenius

norm of X1 minus the projection. The Frobenius norm of each column is the sin squared

of the angle between the original column of X1 and the space spanned by the columns of

X2.

12.8.9 Positive semidenite matrix

A square symmetric matrix is positive semidenite if for all x, xTAx  0. There are

actually three equivalent denitions of positive semidenite.

1. for all x, xTAx  0

2. all eigenvalues are nonnegative

3. A = BT B

We will prove (1) implies (2), (2) implies (3), and (3) implies (1).

1. (1) implies (2) If i were negative, select x = vi. Let ei be the vector of all zeros

except for a one in position i. Then xT Ax = vi

T V DV T vi = ei

T Dei = i < 0.

2. (2) implies (3) A = V DV T = V D 1

2 D 1

2 V T = BT B

3. (3) implies (1) xTAx = (xB)T Bx  0

12.9 Generating Functions

A sequence a0, a1, . . ., can be represented by a generating function g(x) =



(cid:80)

i=0

aixi. The

advantage of the generating function is that it captures the entire sequence in a closed

form that can be manipulated as an entity. For example, if g(x) is the generating func-

tion for the sequence a0, a1, . . ., then x d

dx g(x) is the generating function for the sequence

0, a1, 2a2, 3a3, . . . and x2g(cid:48)(cid:48)(x) + xg(cid:48)(x) is the generating function for the sequence for

0, a1, 4a2, 9a3, . . .

451

Example: The generating function for the sequence 1, 1, . . . is

ating function for the sequence 0, 1, 2, 3, . . . is



(cid:80)

i=0

xi = 1

1x . The gener-



(cid:80)

i=0

ixi =



(cid:80)

i=0

dx xi = x d

x d

dx



(cid:80)

i=0

xi = x d

dx

1

1x = x

(1x)2 .

Example: If A can be selected 0 or 1 times and B can be selected 0, 1, or 2 times and C

can be selected 0, 1, 2, or 3 times, in how many ways can ve objects be selected. Consider

the generating function for the number of ways to select objects. The generating function

for the number of ways of selecting objects, selecting only As is 1+x, only Bs is 1+x+x2,

and only Cs is 1 + x + x2 + x3. The generating function when selecting As, Bs, and Cs

is the product.

(1 + x)(1 + x + x2)(1 + x + x2 + x3) = 1 + 3x + 5x2 + 6x3 + 5x4 + 3x5 + x6

The coecient of x5 is 3 and hence we can select ve objects in three ways: ABBCC,

ABCCC, or BBCCC.

The generating functions for the sum of random variables

Let f (x) =



(cid:80)

i=0

pixi be the generating function for an integer valued random variable

where pi is the probability that the random variable takes on value i. Let g(x) =

be the generating function of an independent integer valued random variable where qi

is the probability that the random variable takes on the value i. The sum of these two

random variables has the generating function f (x)g(x). This is because the coecient of

xi in the product f (x)g(x) is (cid:80)i

k=0 pkqki and this is also the probability that the sum of

the random variables is i. Repeating this, the generating function of a sum of independent

nonnegative integer valued random variables is the product of their generating functions.



(cid:80)

i=0

qixi

12.9.1 Generating Functions for Sequences Dened by Recurrence Relation-

ships

Consider the Fibonacci sequence

0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, . . .

dened by the recurrence relationship

f0 = 0

f1 = 1

fi = fi1 + fi2

i  2

452

Multiply each side of the recurrence by xi and sum i from two to innity.



(cid:88)

i=2

fixi =



(cid:88)

i=2

fi1xi +



(cid:88)

i=2

fi2xi

f2x2 + f3x3 +    = f1x2 + f2x3 +    + f0x2 + f1x3 +   

= x (cid:0)f1x + f2x2 +   (cid:1) + x2 (f0 + f1x +   )

(12.1)

Let

f (x) =



(cid:88)

i=0

fixi.

(12.2)

Substituting (12.2) into (12.1) yields

f (x)  f0  f1x = x (f (x)  f0) + x2f (x)

f (x)  x = xf (x) + x2f (x)

f (x)(1  x  x2) = x

Thus, f (x) = x

1xx2 is the generating function for the Fibonacci sequence.

Note that generating functions are formal manipulations and do not necessarily con-

verge outside some region of convergence. Consider the generating function f (x) =



(cid:80)

i=0

1xx2 for the Fibonacci sequence. Using

fixi = x



(cid:80)

i=0

fixi,

and using f (x) = x

1xx2

Asymptotic behavior

f (1) = f0 + f1 + f2 +    = 

f (1) =

1

1  1  1

= 1.

To determine the asymptotic behavior of the Fibonacci sequence write

f (x) =

x

1  x  x2 =



5

5

1  1x

+





5

5

1  2x



where 1 = 1+

2

1  x  x2 = 0.

5



and 1 = 1

2

5

are the reciprocals of the two roots of the quadratic

Then

Thus,

f (x) =



5

5

1 + 1x + (1x)2 +     (cid:0)1 + 2x + (2x)2 +   (cid:1)(cid:17)

(cid:16)

.



5

5

fn =

(n

1  n

2 ) .

453

5 n

Since 2 < 1 and 1 > 1, for large n, fn

integer and 2 < 1, it must be the case that fn =

all n.

Means and standard deviations of sequences

5



=



1 . In fact, since fn =



5

2 n

5 (n

. Hence fn =

(cid:106)

fn +

1  n

2 ) is an

(cid:107)

(cid:106) 

5

5 n

for

(cid:107)

2

1

5

Generating functions are useful for calculating the mean and standard deviation of a

sequence. Let z be an integral valued random variable where pi is the probability that

pixi be the

z equals i. The expected value of z is given by m =

ipi. Let p(x) =



(cid:80)

i=0



(cid:80)

i=0

generating function for the sequence p1, p2, . . .. The generating function for the sequence

p1, 2p2, 3p3, . . . is

x

d

dx

p(x) =



(cid:88)

i=0

ipixi.

Thus, the expected value of the random variable z is m = xp(cid:48)(x)|x=1 = p(cid:48)(1). If p was not

a probability function, its average value would be p(cid:48)(1)

p(1) since we would need to normalize

the area under p to one.

The variance of z is E(z2)  E2(z) and can be obtained as follows.

x2 d

dx

(cid:12)

(cid:12)

p(x)

(cid:12)

(cid:12)x=1

=

=



(cid:88)

i=0



(cid:88)

i=0

(cid:12)

(cid:12)

i(i  1)xip(x)

(cid:12)

(cid:12)

(cid:12)x=1



(cid:88)

i2xip(x)



(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)x=1

i=0

ixip(x)

(cid:12)

(cid:12)

(cid:12)

(cid:12)

(cid:12)x=1

= E(z2)  E(z).

Thus, 2 = E(z2)  E2(z) = E(z2)  E(z) + E(z)  E2(z) = p(cid:48)(cid:48)(1) + p(cid:48)(1)  (cid:0)p(cid:48)(1)(cid:1)2.

12.9.2 The Exponential Generating Function and the Moment Generating

Function

Besides the ordinary generating function there are a number of other types of gener-

ating functions. One of these is the exponential generating function. Given a sequence

a0, a1, . . . , the associated exponential generating function is g(x) =

Moment generating functions



(cid:80)

i=0

ai

xi

i! .

The kth moment of a random variable x around the point b is given by E((x  b)k).

Usually the word moment is used to denote the moment around the value 0 or around

the mean. In the following, we use moment to mean the moment about the origin.

454

The moment generating function of a random variable x is dened by

(t) = E(etx) =



(cid:90)



etxp(x)dx

Replacing etx by its power series expansion 1 + tx + (tx)2

2!

  gives

(t) =



(cid:90)

(cid:32)



1 + tx +

(tx)2

2!

(cid:33)

+   

p(x)dx

Thus, the kth moment of x about the origin is k! times the coecient of tk in the power

series expansion of the moment generating function. Hence, the moment generating func-

tion is the exponential generating function for the sequence of moments about the origin.

The moment generating function transforms the probability distribution p(x) into a

function  (t) of t. Note (0) = 1 and is the area or integral of p(x). The moment

generating function is closely related to the characteristic function which is obtained by

replacing etx by eitx in the above integral where i =

1 and is related to the Fourier

transform which is obtained by replacing etx by eitx.



(t) is closely related to the Fourier transform and its properties are essentially the

same. In particular, p(x) can be uniquely recovered by an inverse transform from (t).

mi

i! ti converges abso-

More specically, if all the moments mi are nite and the sum



(cid:80)

i=0

lutely in a region around the origin, then p(x) is uniquely determined.

The Gaussian probability distribution with zero mean and unit variance is given by

p (x) = 1

2

e x2

2 . Its moments are given by



(cid:90)

xne x2

2 dx

un =

=

1



2



(cid:40) n!

n

2 ( n

2

0

2 )!

n even

n odd

To derive the above, use integration by parts to get un = (n  1) un2 and combine

2 and v = xn1. Then

this with u0 = 1 and u1 = 0. The steps are as follows. Let u = e x2

u(cid:48) = xe x2

2 and v(cid:48) = (n  1) xn2. Now uv = (cid:82) u(cid:48)v+ (cid:82) uv(cid:48) or

e x2

2 xn1 =

(cid:90)

xne x2

2 dx +

(cid:90)

(n  1) xn2e x2

2 dx.

455

From which



Thus, un = (n  1) un2.

(cid:82) xne x2



xne x2

(cid:82)

2 dx = (n  1) (cid:82) xn2e x2

2 dx  e x2

2 xn1

2 dx = (n  1)

xn2e x2

2 dx



(cid:82)



The moment generating function is given by

g (s) =



(cid:88)

n=0

unsn

n!



(cid:88)

=

n=0

n even

n!

2 n

n

2 !

2

sn

n!

=



(cid:88)

i=0

s2i

2ii!

=



(cid:88)

i=0

(cid:18) s2

2

1

i!

(cid:19)i

= e

s2

2 .

For the general Gaussian, the moment generating function is

g (s) = esu+

(cid:16) 2

2

(cid:17)

s2

Thus, given two independent Gaussians with mean u1 and u2 and variances 2

the product of their moment generating functions is

1 and 2

2,

es(u1+u2)+(2

1+2

2)s2

,

the moment generating function for a Gaussian with mean u1 + u2 and variance 2

1 + 2

2.

Thus, the convolution of two Gaussians is a Gaussian and the sum of two random vari-

ables that are both Gaussian is a Gaussian random variable.

12.10 Miscellaneous

12.10.1 Lagrange multipliers

Lagrange multipliers are used to convert a constrained optimization problem into an un-

constrained optimization. Suppose we wished to maximize a function f (x) subject to a

constraint g(x) = c. The value of f (x) along the constraint g(x) = c might increase for

a while and then start to decrease. At the point where f (x) stops increasing and starts

to decrease, the contour line for f (x) is tangent to the curve of the constraint g(x) = c.

Stated another way the gradient of f (x) and the gradient of g(x) are parallel.

By introducing a new variable  we can express the condition by xf = xg and

g = c. These two conditions hold if and only if

x

(cid:0)f (x) +  (g (x)  c)(cid:1) = 0

The partial with respect to  establishes that g(x) = c. We have converted the constrained

optimization problem in x to an unconstrained problem with variables x and .

456

g(x, y) = c

f (x, y)

Figure 12.3: In nding the minimum of f (x, y) within the ellipse, the path head towards

the minimum of f (x, y) until it hits the boundary of the ellipse and then follows the

boundary of the ellipse until the tangent of the boundary is in the same direction as the

contour line of f (x, y).

12.10.2 Finite Fields

For a prime p and integer n there is a unique nite eld with pn elements. In Section

8.6 we used the eld GF(2n), which consists of polynomials of degree less than n with

coecients over the eld GF(2). In GF(28)

(x7 + x5 + x) + (x6 + x5 + x4) = x7 + x6 + x4 + x

Multiplication is modulo an irreducible polynomial. Thus

(x7 + x5 + x)(x6 + x5 + x4) = x13 + x12 + x11 + x11 + x10 + x9 + x7 + x6 + x5

= x13 + x12 + x10 + x9 + x7 + x6 + x5

= x6 + x4 + x3 + x2

mod x8 + x4 + x3 + x + 1

Division of x13 + x12 + x10 + x9 + x7 + x6 + x5 by x6 + x4 + x3 + x2 is illustrated below.

x5(x8 + x4 + x3 + x2 + 1) = x13

x13 +x12 +x10 +x9

+x9

x4(x8 + x4 + x3 + x2 + 1) =

x2(x8 + x4 + x3 + x2 + 1) =

x12 +x10

x12

x10

x10

12.10.3 Application of Mean Value Theorem

+x7 +x6 +x5

+x6 +x5

+x8

+x8 +x7

+x8 +x7

+x5 +x4

x4

+x5

x6 +x5

x6

x3

x2

+x4 +x3 +x2

The mean value theorem states that if f (x) is continuous and dierentiable on the

. That is, at some

interval [a, b], then there exists c, a  c  b such that f (cid:48)(c) = f (b)f (a)

ba

457

f (x)

a

c

b

Figure 12.4: Illustration of the mean value theorem.

point between a and b the derivative of f equals the slope of the line from f (a) to f (b).

See Figure 12.10.3.

One application of the mean value theorem is with the Taylor expansion of a function.

The Taylor expansion about the origin of f (x) is

f (x) = f (0) + f (cid:48)(0)x +

1

2!

f (cid:48)(cid:48)(0)x2 +

1

3!

f (cid:48)(cid:48)(cid:48)(0)x3 +   

(12.3)

By the mean value theorem there exists c, 0  c  x, such that f (cid:48)(c) = f (x)f (0)

f (x)  f (0) = xf (cid:48)(c). Thus

x

or

xf (cid:48)(c) = f (cid:48)(0)x +

1

2!

f (cid:48)(cid:48)(0)x2 +

1

3!

f (cid:48)(cid:48)(cid:48)(0)x3 +   

and

f (x) = f (0) + xf (cid:48)(c).

One could apply the mean value theorem to f (cid:48)(x) in

f (cid:48)(x) = f (cid:48)(0) + f (cid:48)(cid:48)(0)x +

1

2!

f (cid:48)(cid:48)(cid:48)(0)x2 +   

Then there exists d, 0  d  x such that

Integrating

xf (cid:48)(cid:48)(d) = f (cid:48)(cid:48)(0)x +

1

2!

f (cid:48)(cid:48)(cid:48)(0)x2 +   

1

2

x2f (cid:48)(cid:48)(d) =

1

2!

f (cid:48)(cid:48)(0)x +

1

3!

f (cid:48)(cid:48)(cid:48)(0)x3 +   

458

Substituting into Eq(12.3)

f (x) = f (0) + f (cid:48)(0)x +

1

2

x2f (cid:48)(cid:48)(d).

12.10.4 Sperners Lemma

Consider a triangulation of a 2-dimensional simplex. Let the vertices of the simplex

be colored R, B, and G. If the vertices on each edge of the simplex are colored only with

the two colors at the endpoints then the triangulation must have a triangle whose ver-

tices are three dierent colors. In fact, it must have an odd number of such vertices. A

generalization of the lemma to higher dimensions also holds.

Create a graph whose vertices correspond to the triangles of the triangulation plus an

additional vertex corresponding to the outside region. Connect two vertices of the graph

by an edge if the triangles corresponding to the two vertices share a common edge that

is color R and B. The edge of the original simplex must have an odd number of such

triangular edges. Thus, the outside vertex of the graph must be of odd degree. The graph

must have an even number of odd degree vertices. Each odd vertex is of degree 0, 1, or 2.

The vertices of odd degree, i.e. degree one, correspond to triangles which have all three

colors.

12.10.5 Prufer

Here we prove that the number of labeled trees with n vertices is nn2. By a labeled

tree we mean a tree with n vertices and n distinct labels, each label assigned to one vertex.

Theorem 12.28 The number of labeled trees with n vertices is nn2.

Proof: (Prufer sequence) There is a one-to-one correspondence between labeled trees

and sequences of length n  2 of integers between 1 and n. An integer may repeat in the

sequence. The number of such sequences is clearly nn2. Although each vertex of the tree

has a unique integer label the corresponding sequence has repeating labels. The reason for

this is that the labels in the sequence refer to interior vertices of the tree and the number

of times the integer corresponding to an interior vertex occurs in the sequence is related

to the degree of the vertex. Integers corresponding to leaves do not appear in the sequence.

To see the one-to-one correspondence, rst convert a tree to a sequence by deleting

the lowest numbered leaf. If the lowest numbered leaf is i and its parent is j, append j to

the tail of the sequence. Repeating the process until only two vertices remain yields the

sequence. Clearly a labeled tree gives rise to only one sequence.

It remains to show how to construct a unique tree from a sequence. The proof is

by induction on n. For n = 1 or 2 the induction hypothesis is trivially true. Assume

the induction hypothesis true for n  1. Certain numbers from 1 to n do not appear

459

in the sequence and these numbers correspond to vertices that are leaves. Let i be

the lowest number not appearing in the sequence and let j be the rst integer in the

sequence. Then i corresponds to a leaf connected to vertex j. Delete the integer j from

the sequence. By the induction hypothesis there is a unique labeled tree with integer

labels 1, . . . , i  1, i + 1, . . . , n. Add the leaf i by connecting the leaf to vertex j. We

need to argue that no other sequence can give rise to the same tree. Suppose some other

sequence did. Then the ith integer in the sequence must be j. By the induction hypothesis

the sequence with j removed is unique.

Algorithm

Create leaf list - the list of labels not appearing in the Prufer sequence. n is the

length of the Prufer list plus two.

while Prufer sequence is non empty do

begin

p =rst integer in Prufer sequence

e =smallest label in leaf list

Add edge (p, e)

Delete e from leaf list

Delete p from Prufer sequence

If p no longer appears in Prufer sequence add p to leaf list

end

There are two vertices e and f on leaf list, add edge (e, f )

12.11 Exercises

Exercise 12.1 What is the dierence between saying f (n) is O (n3) and f (n) is o (n3)?

Exercise 12.2 If f (n)  g (n) what can we say about f (n) + g(n) and f (n)  g(n)?

Exercise 12.3 What is the dierence between  and ?

Exercise 12.4 If f (n) is O (g (n)) does this imply that g (n) is  (f (n))?

Exercise 12.5 What is lim

k

(cid:0) k1

k2

(cid:1)k2.

Exercise 12.6 Select a, b, and c uniformly at random from [0, 1]. The probability that

b < a is 1/2. The probability that c<a is 1/2. However, the probability that both b and c are

less than a is 1

3 not 1/4. Why is this? Note that the six possible permutations abc, acb,

bac, cab, bca, and cba, are all equally likely. Assume that a, b, and c are drawn from the

interval (0,1]. Given that b < a, what is the probability that c < a?

Exercise 12.7 Let A1, A2, . . . , An be events. Prove that Prob(A1A2   An) 

n

(cid:80)

i=1

Prob(Ai)

460

Exercise 12.8 Give an example of three random variables that are pairwise independent

but not fully independent.

Exercise 12.9 Give examples of nonnegative valued random variables with median >>

mean. Can we have median << mean?

Exercise 12.10 Consider n samples x1, x2, . . . , xn from a Gaussian distribution of mean

 and variance . For this distribution m = x1+x2++xn

is an unbiased estimator of

(xi  )2 is an unbiased estimator of 2. Prove that if we

n

. If  is known then 1

n

n

(cid:80)

i=1

approximate  by m, then 1

n1

n

(cid:80)

i=1

(xi  m)2 is an unbiased estimator of 2.

Exercise 12.11 Given the distribution

1

23

2 ( x

3 )2

e 1

what is the probability that x >1?

Exercise 12.12 e

we wished to approximate e

x2

2 has value 1 at x = 0 and drops o very fast as x increases. Suppose

x2

2 by a function f (x) where

f (x) =

(cid:26) 1

0

|x|  a

|x| > a

.

What value of a should we use? What is the integral of the error between f (x) and e

x2

2 ?

Exercise 12.13 Given two sets of red and black balls with the number of red and black

balls in each set shown in the table below.

Set 1

Set 2

red black

40

50

60

50

Randomly draw a ball from one of the sets. Suppose that it turns out to be red. What is

the probability that it was drawn from Set 1?

Exercise 12.14 Why cannot one prove an analogous type of theorem that states p (x  a) 

E(x)

a ?

Exercise 12.15 Compare the Markov and Chebyshev bounds for the following probability

distributions

1. p(x) =

(cid:26) 1 x = 1

0 otherwise

2. p(x) =

(cid:26) 1/2

0

0  x  2

otherwise

461

Exercise 12.16 Let s be the sum of n independent random variables x1, x2, . . . , xn where

for each i

xi =

(cid:26) 0 Prob

1 Prob

p

1  p

1. How large must  be if we wish to have P rob (cid:0)s < (1  ) m(cid:1) < ?

2. If we wish to have P rob (cid:0)s > (1 + ) m(cid:1) < ?

Exercise 12.17 What is the expected number of ips of a coin until a head is reached?

Assume p is probability of a head on an individual ip. What is value if p=1/2?

Exercise 12.18 Given the joint probability

P(A,B)

B=0

B=1

A=0

1/16

1/4

A=1

1/8

9/16

1. What is the marginal probability of A? of B?

2. What is the conditional probability of B given A?

Exercise 12.19 Consider independent random variables x1, x2, and x3, each equal to

zero with probability 1

2. Let S = x1 + x2 + x3 and let F be event that S  {1, 2}. Condi-

tioning on F , the variables x1, x2, and x3 are still each zero with probability 1

2. Are they

still independent?

Exercise 12.20 Consider rolling two dice A and B. What is the probability that the sum

S will add to nine? What is the probability that the sum will be 9 if the roll of A is 3?

Exercise 12.21 Write the generating function for the number of ways of producing chains

using only pennies, nickels, and dines. In how many ways can you produce 23 cents?

Exercise 12.22 A dice has six faces, each face of the dice having one of the numbers 1

though 6. The result of a role of the dice is the integer on the top face. Consider two roles

of the dice. In how many ways can an integer be the sum of two roles of the dice.

Exercise 12.23 If a(x) is the generating function for the sequence a0, a1, a2, . . ., for what

sequence is a(x)(1-x) the generating function.

Exercise 12.24 How many ways can one draw n a(cid:48)s and b(cid:48)s with an even number of a(cid:48)s.

Exercise 12.25 Find the generating function for the recurrence ai = 2ai1 + i where

a0 = 1.

462

Exercise 12.26 Find a closed form for the generating function for the innite sequence

of prefect squares 1, 4, 9, 16, 25, . . .

Exercise 12.27 Given that

what sequence is

1

12x the generating function?

1

1x is the generating function for the sequence 1, 1, . . ., for

Exercise 12.28 Find a closed form for the exponential generating function for the innite

sequence of prefect squares 1, 4, 9, 16, 25, . . .

Exercise 12.29 Prove that the L2 norm of (a1, a2, . . . , an) is less than or equal to the L1

norm of (a1, a2, . . . , an).

Exercise 12.30 Prove that there exists a y, 0  y  x, such that f (x) = f (0) + f (cid:48)(y)x.

Exercise 12.31 Show that the eigenvectors of a matrix A are not a continuous function

of changes to the matrix.

Exercise 12.32 What are the eigenvalues of the two graphs shown below? What does

this say about using eigenvalues to determine if two graphs are isomorphic.

Exercise 12.33 Let A be the adjacency matrix of an undirected graph G. Prove that

eigenvalue 1 of A is at least the average degree of G.

Exercise 12.34 Show that if A is a symmetric matrix and 1 and 2 are distinct eigen-

values then their corresponding eigenvectors x1 and x2 are orthogonal.

Hint:

Exercise 12.35 Show that a matrix is rank k if and only if it has k nonzero eigenvalues

and eigenvalue 0 of rank n-k.

Exercise 12.36 Prove that maximizing xT Ax

to the condition that x be of unit length.

xT x is equivalent to maximizing xT Ax subject

Exercise 12.37 Let A be a symmetric matrix with smallest eigenvalue min. Give a

bound on the largest element of A1.

Exercise 12.38 Let A be the adjacency matrix of an n vertex clique with no self loops.

Thus, each row of A is all ones except for the diagonal entry which is zero. What is the

spectrum of A.

Exercise 12.39 Let A be the adjacency matrix of an undirect graph G. Prove that the

eigenvalue 1 of A is at least the average degree of G.

463

Exercise 12.40 We are given the probability distribution for two random vectors x and

y and we wish to stretch space to maximize the expected distance between them. Thus,

we will multiply each coordinate by some quantity ai. We restrict

a2

i = d. Thus, if we

increase some coordinate by ai > 1, some other coordinate must shrink. Given random

vectors x = (x1, x2, . . . , xd) and y = (y1, y2, . . . , yd) how should we select ai to maximize

E (cid:0)|x  y|2(cid:1)? The ai stretch dierent coordinates. Assume

d

(cid:80)

i=1

yi =

(cid:26) 0

1

1

2

1

2

and that xi has some arbitrary distribution.

E (cid:0)|x  y|2(cid:1) = E

d

(cid:80)

i=1

(cid:2)a2

i (xi  yi)2(cid:3) =

i E (x2

a2

i  2xiyi + y2

i )

d

(cid:80)

i=1

i  xi + 1

2

(cid:1)

=

d

(cid:80)

i=1

i E (cid:0)x2

a2

i ) = E (xi) we get . Thus, weighting the coordinates has no eect assuming

Since E (x2

d

(cid:80)

i=1

i = 1. Why is this? Since E (yi) = 1

a2

2.

E (cid:0)|x  y|2(cid:1) is independent of the value of xi hence its distribution.

What if yi =

(cid:26) 0

1

3

4

1

4

and E (yi) = 1

4. Then

E (cid:0)|x  y|2(cid:1) =

d

(cid:80)

i=1

i ) =

i  2xiyi + y2

i E (x2

a2

d

(cid:80)

i=1

(cid:0) 1

2E (xi) + 1

a2

i

=

4

d

(cid:80)

i=1

(cid:1)

i E (cid:0)xi  1

a2

2xi + 1

4

(cid:1)

.

To maximize put all weight on the coordinate of x with highest probability of one. What

if we used 1-norm instead of the two norm?

E (|x  y|) = E

d

(cid:88)

i=1

ai |xi  yi| =

d

(cid:88)

i=1

aiE |xi  yi| =

d

(cid:88)

i=1

aibi

where bi = E (xi  yi). If

i = 1, then to maximize let ai = bi

a2

b . Taking the dot product

of a and b is maximized when both are in the same direction.

d

(cid:80)

i=1

Exercise 12.41 Maximize x+y subject to the constraint that x2 + y2 = 1.

Exercise 12.42 Draw a tree with 10 vertices and label each vertex with a unique integer

from 1 to 10. Construct the Prfer sequence for the tree. Given the Prfer sequence recreate

the tree.

464

Exercise 12.43 Construct the tree corresponding to the following Prfer sequences

1. 113663

2. 552833226

465

Index

2-universal, 184

4-way independence, 191

Anity matrix, 229

Algorithm

greedy k-clustering, 215

k-means, 211

singular value decomposition, 51

Almost surely, 253

Anchor Term, 317

Aperiodic, 77

Arithmetic mean, 417

Bad pair, 257

Bayes rule, 428

Bayesian, 338

Bayesian network, 338

Belief Network, 338

belief propagation, 337

Bernoulli trials, 425

Best t, 40

Bigoh, 406

Binomial distribution, 248

approximated by Poisson, 426

boosting, 158

Branching process, 272

Cartesian coordinates, 17

Cauchy-Schwartz inequality, 414, 416

Central Limit Theorem, 423

Characteristic equation, 437

Characteristic function, 455

Chebyshevs inequality, 13

Cherno bounds, 430

Clustering, 208

k-center criterion, 215

k-means, 211

Sparse Cuts, 229

CNF

CNF-sat, 279

Cohesion, 232

Combining expert advice, 162

Commute time, 104

Conditional probability, 421

Conductance, 97

Coordinates

Cartesian, 17

polar, 17

Coupon collector problem, 107

Cumulative distribution function, 420

Current

probabilistic interpretation, 100

Cycles, 266

emergence, 265

number of, 265

Data streams

counting frequent elements, 187

frequency moments, 182

frequent element, 188

majority element, 187

number of distinct elements, 183

number of occurrences of an element,

186

second moment, 189

Degree distribution, 248

power law, 248

Depth rst search, 261

Diagonalizable, 438

Diameter of a graph, 256, 268

Diameter two, 266

dilation, 385

Disappearance of isolated vertices, 266

Discovery time, 102

Distance

total variation, 82

Distribution

vertex degree, 246

Document ranking, 62

Eective resistance, 105

Eigenvalue, 437

466

Eigenvector, 54, 437

Electrical network, 97

Erdos Renyi, 245

Error correcting codes, 190

Escape probability, 101

Eulers constant, 108

Event, 420

Expected degree

vertex, 245

Expected value, 421

Exponential generating function, 454

Extinct families

size, 276

Extinction probability, 272, 274

Finite elds, 457

First moment method, 254

Fourier transform, 370, 455

Frequency domain, 371

G(n,p), 245

Gamma function, 18

Gamma function , 415

Gaussian, 23, 424, 456

tting to data, 29

tail, 419

Gaussians

sparating, 27

General tail bounds, 433

Generating function, 272

component size, 288

for sum of two variables, 272

Generating functions, 451

Generating points in the unit ball, 22

Geometric mean, 417

Giant component, 246, 253, 259, 261, 266

Gibbs sampling, 84

Graph

connecntivity, 265

resistance, 108

Graphical model, 337

Greedy

k-clustering, 215

Growth models, 286

with preferential attachment, 293

without preferential attachment, 287

Holders inequality, 414, 416

Haar wavelet, 386

Harmonic function, 98

Hash function

universal, 184

Heavy tail, 248

Hidden Markov model, 332

Hitting time, 102, 114

Immortality probability, 274

Incoherent, 368, 371

Increasing property, 253, 270

unsatisability, 279

Independence

limited way, 190

Independent, 421

Indicator random variable, 257

of triangle, 251

Indicator variable, 422

Ising model, 352

Isolated vertices, 259, 266

number of, 259

Jensens inequality, 418

Johnson-Lindenstrauss lemma, 25, 26

k-clustering, 215

k-means clustering algorithm, 211

Kernel methods, 228

Kirchhos law, 99

Kleinberg, 295

Lagrange, 456

Laplacian, 70

Law of large numbers, 12, 14

Learning, 129

Linearity of expectation, 251, 421

Lloyds algorithm, 211

Local algorithm, 295

Long-term probabilities, 80

m-fold, 270

467

Markov chain, 77

state, 82

Markov Chain Monte Carlo, 78

Markov random eld, 340

Markovs inequality, 13

Matrix

multiplication

by sampling, 193

diagonalizable, 438

similar, 437

Maximum cut problem, 63

Maximum likelihood estimation, 429

Maximum likelihood estimator, 29

Maximum principle, 98

MCMC, 78

Mean value theorem, 457

Median, 423

Metropolis-Hastings algorithm, 83

Mixing time, 80

Model

random graph, 245

Molloy Reed, 285

Moment generating function, 455

Mutually independent, 421

Nearest neighbor problem, 27

Nonuniform Random Graphs, 284

Normalized conductance, 80, 89

Number of triangles in G(n, p), 251

Ohms law, 99

Orthonormal, 445

Page rank, 113

personalized , 116

Persistent, 77

Phase transition, 253

CNF-sat, 279

nonnite components, 291

Poisson distribution, 426

Polar coordinates, 17

Polynomial interpolation, 190

Positive semidenite, 451

Power iteration, 62

Power law distribution, 248

Power method, 51

Power-law distribution, 284

Prufer, 459

Principle component analysis, 56

Probability density function, 420

Probability distribution function, 420

Psuedo random, 191

Pure-literal heuristic, 280

Queue, 281

arrival rate, 281

Radon, 152

Random graph, 245

Random projection, 25

theorem, 25

Random variable, 420

Random walk

Eucleadean space, 109

in three dimensions, 110

in two dimensions, 110

on lattice, 109

undirected graph, 102

web, 112

Rapid Mixing, 82

Real spectral theorem, 439

Replication, 270

Resistance, 97, 108

efective, 101

Restart, 113

value, 113

Return time, 113

Sample space, 420

Sampling

length squared, 194

Satisfying assignments

expected number of, 280

Scale function, 386

Scale vector, 386

Second moment method, 251, 254

Sharp threshold, 253

Similar matrices, 437

468

Variance, 422

variational method, 413

VC-dimension, 148

convex polygons, 151

nite sets, 153

half spaces, 151

intervals, 151

pairs of intervals, 151

rectangles, 151

spheres, 152

Viterbi algorithm, 334

Voltage

probabilistic interpretation, 99

Wavelet, 385

World Wide Web, 112

Youngs inequality, 414, 416

Singular value decomposition, 40

Singular vector, 42

rst, 43

left, 45

right, 45

second, 43

Six-degrees separation, 295

Sketch

matrix, 197

Sketches

documents, 201

Small world, 294

Smallest-clause heuristic, 280

Spam, 115

Spectral clustering, 216

Sperners lemma, 459

Stanley Milgram, 294

State, 82

Stirling approximation, 414

Streaming model, 181

Symmetric matrices, 439

Tail bounds, 430, 433

Tail of Gaussian, 419

Taylor series, 409

Threshold, 252

CNF-sat, 277

diameter O(ln n), 269

disappearance of isolated vertices, 259

emergence of cycles, 265

emergence of diameter two, 256

giant component plus isolated vertices,

267

Time domain, 371

Total variation distance, 82

Trace, 448

Triangle inequality, 414

Triangles, 250

Union bound, 422

Unit-clause heuristic, 280

Unitary matrix, 445

Unsatisability, 279

469

References

[AB15]

Pranjal Awasthi and Maria-Florina Balcan. Center based clustering: A foun-

dational perspective. In Christian Hennig, Marina Meila, Fionn Murtagh,

and Roberto Rocci, editors, Handbook of cluster analysis. CRC Press, 2015.

[ACORT11] Dimitris Achlioptas, Amin Coja-Oghlan, and Federico Ricci-Tersenghi. On

the solution-space geometry of random constraint satisfaction problems.

Random Structures & Algorithms, 38(3):251268, 2011.

[AGKM16] Sanjeev Arora, Rong Ge, Ravi Kannan, and Ankur Moitra. Computing a

nonnegative matrix factorization - provably. SIAM J. Comput., 45(4):1582

1611, 2016.

[AK05]

Sanjeev Arora and Ravindran Kannan. Learning mixtures of separated non-

spherical gaussians. Annals of Applied Probability, 15(1A):6992, 2005. Pre-

liminary version in STOC 2001.

[Alo86]

Noga Alon. Eigenvalues and expanders. Combinatorica, 6:8396, 1986.

[AM05]

[AMS96]

[AN72]

[AP03]

[Arr50]

[AV07]

[BA]

[BB10]

Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures

of distributions. In COLT, pages 458469, 2005.

Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of

approximating the frequency moments. In Proceedings of the twenty-eighth

annual ACM symposium on Theory of computing, pages 2029. ACM, 1996.

Krishna Athreya and P. E. Ney. Branching Processes, volume 107. Springer,

Berlin, 1972.

Dimitris Achlioptas and Yuval Peres. The threshold for random k-sat is 2k

(ln 2 - o(k)). In STOC, pages 223231, 2003.

Kenneth J. Arrow. A diculty in the concept of social welfare. Journal of

Political Economy, 58(4):328346, 1950.

David Arthur and Sergei Vassilvitskii. k-means++: The advantages of care-

ful seeding. In Proceedings of the eighteenth annual ACM-SIAM symposium

on Discrete algorithms, pages 10271035. Society for Industrial and Applied

Mathematics, 2007.

Albert-Lszl Barabsi and Rka Albert. Emergence of scaling in random net-

works. Science, 286(5439).

M.-F. Balcan and A. Blum. A discriminative model for semi-supervised

learning. Journal of the ACM, 57(3):19:119:46, March 2010.

470

[BBG13]

[BBIS16]

[BBK14]

[BBL09]

[BBV08]

Maria-Florina Balcan, Avrim Blum, and Anupam Gupta. Clustering under

approximation stability. Journal of the ACM (JACM), 60(2):8, 2013.

Tom Balyo, Armin Biere, Markus Iser, and Carsten Sinz. {SAT} race 2015.

Articial Intelligence, 241:45  65, 2016.

Trapit Bansal, Chiranjib Bhattacharyya, and Ravindran Kannan. A provable

svd-based algorithm for learning topics in dominant admixture corpus. In

Advances in Neural Information Processing Systems 27 (NIPS), pages 1997

2005, 2014.

M.-F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning.

Journal of Computer and System Sciences, 75(1):78  89, 2009. Special Issue

on Learning Theory. An earlier version appeared in International Conference

on Machine Learning 2006.

Maria-Florina Balcan, Avrim Blum, and Santosh Vempala. A discriminative

framework for clustering via similarity functions. In Proceedings of the forti-

eth annual ACM symposium on Theory of computing, pages 671680. ACM,

2008.

[BEHW87] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Occams

razor. Information Processing Letters, 24:377380, April 1987.

[BEHW89] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Learnabil-

ity and the vapnik-chervonenkis dimension. Journal of the Association for

Computing Machinery, 36(4):929865, 1989.

[Ben09]

Yoshua Bengio. Learning deep architectures for ai. Foundations and Trends

in Machine Learning, 2(1):1127, 2009.

[BGMZ97] Andrei Z Broder, Steven C Glassman, Mark S Manasse, and Georey Zweig.

Syntactic clustering of the web. Computer Networks and ISDN Systems,

29(8-13):11571166, 1997.

[BGV92]

[Bis06]

[Ble12]

[BLG14]

B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for

optimal margin classiers. In Proceedings of the Fifth Annual Workshop on

Computational Learning Theory, 1992.

Christopher M Bishop. Pattern recognition and machine learning. springer,

2006.

David M. Blei. Probabilistic topic models. Commun. ACM, 55(4):7784,

2012.

Maria-Florina Balcan, Yingyu Liang, and Pramod Gupta. Robust hierar-

chical clustering. Journal of Machine Learning Research, 15(1):38313871,

2014.

471

[Blo62]

[BM98]

[BM02]

H.D. Block. The perceptron: A model for brain functioning. Reviews of

Modern Physics, 34:123135, 1962. Reprinted in Neurocomputing, Anderson

and Rosenfeld.

A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-

In Conference on Learning Theory (COLT). Morgan Kaufmann

training.

Publishers, 1998.

P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities:

Risk bounds and structural results. Journal of Machine Learning Research,

3:463482, 2002.

[BM07]

A. Blum and Y. Mansour. From external to internal regret. Journal of

Machine Learning Research, 8:13071324, 2007.

[BMPW98] Sergey Brin, Rajeev Motwani, Lawrence Page, and Terry Winograd. What

can you do with a web in your pocket? Data Engineering Bulletin, 21:3747,

1998.

[BMZ05]

Alfredo Braunstein, Marc Mezard, and Riccardo Zecchina. Survey propa-

gation: An algorithm for satisability. Random Structures & Algorithms,

27(2):201226, 2005.

[BNJ03]

David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet

allocation. Journal of Machine Learning Research, 3:9931022, 2003.

[Bol01]

Bela Bollobas. Random Graphs. Cambridge University Press, 2001.

[BSS08]

[BT87]

[BU14]

[BVZ98]

Mohsen Bayati, Devavrat Shah, and Mayank Sharma. Max-product for max-

imum weight matching: Convergence, correctness, and lp duality.

IEEE

Transactions on Information Theory, 54(3):12411251, 2008.

Bela Bollobas and Andrew Thomason. Threshold functions. Combinatorica,

7(1):3538, 1987.

Maria-Florina Balcan and Ruth Urner. Active Learning - Modern Learning

Theory, pages 16. Springer Berlin Heidelberg, Berlin, Heidelberg, 2014.

Yuri Boykov, Olga Veksler, and Ramin Zabih. Markov random elds with

ecient approximations. In Computer vision and pattern recognition, 1998.

Proceedings. 1998 IEEE computer society conference on, pages 648655.

IEEE, 1998.

[CBFH+97] N. Cesa-Bianchi, Y. Freund, D.P. Helmbold, D. Haussler, R.E. Schapire, and

M.K. Warmuth. How to use expert advice. Journal of the ACM, 44(3):427

485, 1997.

472

[CD10]

Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for the

cluster tree. In Advances in Neural Information Processing Systems, pages

343351, 2010.

[CF86]

Ming-Te Chao and John V. Franco. Probabilistic analysis of two heuristics

for the 3-satisability problem. SIAM J. Comput., 15(4):11061118, 1986.

[CHK+01] Duncan S. Callaway, John E. Hopcroft, Jon M. Kleinberg, M. E. J. Newman,

and Steven H. Strogatz. Are randomly grown graphs really random? Phys.

Rev. E, 64((Issue 4)), 2001.

[Chv92]

[CSZ06]

[CV95]

[Das11]

[DE03]

33rd Annual Symposium on Foundations of Computer Science, 24-27 Octo-

ber 1992, Pittsburgh, Pennsylvania, USA. IEEE, 1992.

O. Chapelle, B. Scholkopf, and A. Zien, editors. Semi-Supervised Learning.

MIT Press, Cambridge, MA, 2006.

C. Cortes and V. Vapnik. Support-vector networks. Machine Learning,

20(3):273  297, 1995.

Sanjoy Dasgupta. Two faces of active learning. Theor. Comput. Sci.,

412(19):17671781, April 2011.

David L. Donoho and Michael Elad. Optimally sparse representation in

general (nonorthogonal) dictionaries via (cid:96)1 minimization. Proceedings of the

National Academy of Sciences, 100(5):21972202, 2003.

[DFK91]

Martin Dyer, Alan Frieze, and Ravindran Kannan. A random polynomial

time algorithm for approximating the volume of convex bodies. Journal of

the Association for Computing Machinary, 38:117, 1991.

[DG99]

Sanjoy Dasgupta and Anupam Gupta. An elementary proof of the johnson-

lindenstrauss lemma. 99(006), 1999.

[DKM06a] Petros Drineas, Ravi Kannan, and Michael W Mahoney. Fast monte carlo

algorithms for matrices i: Approximating matrix multiplication. SIAM Jour-

nal on Computing, 36(1):132157, 2006.

[DKM06b] Petros Drineas, Ravi Kannan, and Michael W Mahoney. Fast monte carlo

algorithms for matrices ii: Computing a low-rank approximation to a matrix.

SIAM Journal on computing, 36(1):158183, 2006.

[Don06]

[DS84]

David L Donoho. Compressed sensing. IEEE Transactions on information

theory, 52(4):12891306, 2006.

Peter G. Doyle and J. Laurie Snell. Random walks and electric networks,

volume 22 of Carus Mathematical Monographs. Mathematical Association

of America, Washington, DC, 1984.

473

[DS03]

[DS07]

[ER60]

David L. Donoho and Victoria Stodden. When does non-negative matrix

factorization give a correct decomposition into parts? In Advances in Neural

Information Processing Systems 16 (NIPS), pages 11411148, 2003.

Sanjoy Dasgupta and Leonard J. Schulman. A probabilistic analysis of em

for mixtures of separated, spherical gaussians. Journal of Machine Learning

Research, 8:203226, 2007.

Paul Erdos and Alfred Renyi. On the evolution of random graphs. Publi-

cation of the Mathematical Institute of the Hungarian Academy of Sciences,

5:1761, 1960.

[FCMR08] Maurizio Filippone, Francesco Camastra, Francesco Masulli, and Stefano

Rovetta. A survey of kernel and spectral methods for clustering. Pattern

recognition, 41(1):176190, 2008.

[FD07]

[FK99]

[FK00]

[FK15]

[FKV04]

[F(cid:32)LP+51]

[FM85]

[Fri99]

[FS96]

Brendan J Frey and Delbert Dueck. Clustering by passing messages between

data points. Science, 315(5814):972976, 2007.

Alan M. Frieze and Ravindan Kannan. Quick approximation to matrices

and applications. Combinatorica, 19(2):175220, 1999.

Brendan J Frey and Ralf Koetter. Exact inference using the attenuated

max-product algorithm. Advanced mean eld methods: Theory and Practice,

2000.

A. Frieze and M. Karonski. Introduction to Random Graphs. Cambridge

University Press, 2015.

Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast monte-carlo algo-

rithms for nding low-rank approximations. Journal of the ACM (JACM),

51(6):10251041, 2004.

K Florek, J (cid:32)Lukaszewicz, J Perkal, Hugo Steinhaus, and S Zubrzycki. Sur

la liaison et la division des points dun ensemble ni. In Colloquium Mathe-

maticae, volume 2, pages 282285, 1951.

Philippe Flajolet and G Nigel Martin. Probabilistic counting algorithms for

data base applications. Journal of computer and system sciences, 31(2):182

209, 1985.

Friedgut. Sharp thresholds of graph properties and the k-sat problem. Jour-

nal of the American Math. Soc., 12, no 4:10171054, 1999.

Alan M. Frieze and Stephen Suen. Analysis of two simple heuristics on a

random instance of k-sat. J. Algorithms, 20(2):312355, 1996.

474

[FS97]

[GEB15]

[Gha01]

Y. Freund and R. Schapire. A decision-theoretic generalization of on-line

learning and an application to boosting. Journal of Computer and System

Sciences, 55(1):119139, 1997.

Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. A neural algo-

rithm of artistic style. CoRR, abs/1508.06576, 2015.

Zoubin Ghahramani. An introduction to hidden markov models and bayesian

networks. International journal of pattern recognition and articial intelli-

gence, 15(01):942, 2001.

[Gib73]

A. Gibbard. Manipulation of voting schemes: a general result. Econometrica,

41:587601, 1973.

[GKL+15]

[GKSS08]

[GLS12]

[GN03]

[Gon85]

[GvL96]

[GW95]

Jacob R. Gardner, Matt J. Kusner, Yixuan Li, Paul Upchurch, Kilian Q.

Weinberger, and John E. Hopcroft. Deep manifold traversal: Changing labels

with convolutional features. CoRR, abs/1511.06421, 2015.

Carla P. Gomes, Henry A. Kautz, Ashish Sabharwal, and Bart Selman. Sat-

isability solvers. Handbook of Knowledge Representation, pages 89134,

2008.

Martin Grotschel, Laszlo Lovasz, and Alexander Schrijver. Geometric algo-

rithms and combinatorial optimization, volume 2. Springer Science & Busi-

ness Media, 2012.

Remi Gribonval and Morten Nielsen. Sparse decompositions in incoherent

dictionaries. In Proceedings of the 2003 International Conference on Image

Processing, ICIP 2003, Barcelona, Catalonia, Spain, September 14-18, 2003,

pages 3336, 2003.

Teolo F Gonzalez. Clustering to minimize the maximum intercluster dis-

tance. Theoretical Computer Science, 38:293306, 1985.

Gene H. Golub and Charles F. van Loan. Matrix computations (3. ed.).

Johns Hopkins University Press, 1996.

Michel X Goemans and David P Williamson. Improved approximation al-

gorithms for maximum cut and satisability problems using semidenite

programming. Journal of the ACM (JACM), 42(6):11151145, 1995.

[HMMR15] Christian Hennig, Marina Meila, Fionn Murtagh, and Roberto Rocci. Hand-

book of cluster analysis. CRC Press, 2015.

[IN77]

DB Iudin and Arkadi S Nemirovskii. Informational complexity and ecient

methods for solving complex extremal problems. Matekon, 13(3):2545, 1977.

475

[Jai10]

[Jer98]

Anil K Jain. Data clustering: 50 years beyond k-means. Pattern recognition

letters, 31(8):651666, 2010.

Mark Jerrum. Mathematical foundations of the markov chain monte carlo

method. In Dorit Hochbaum, editor, Approximation Algorithms for NP-hard

Problems, 1998.

[JKLP93]

Svante Janson, Donald E. Knuth, Tomasz Luczak, and Boris Pittel. The

birth of the giant component. Random Struct. Algorithms, 4(3):233359,

1993.

[JLR00]

[Joa99]

[Kan09]

[Kar90]

[KFL01]

[Kha79]

[KK10]

[Kle99]

[Kle00]

[KS13]

[KV09]

Svante Janson, Tomasz Luczak, and Andrzej Rucinski. Random Graphs.

John Wiley and Sons, Inc, 2000.

T. Joachims. Transductive inference for text classication using support

vector machines. In International Conference on Machine Learning, pages

200209, 1999.

Ravindran Kannan. A new probability inequality using typical moments and

concentration results. In FOCS, pages 211220, 2009.

Richard M. Karp. The transitive closure of a random digraph. Random

Structures and Algorithms, 1(1):7394, 1990.

Frank R Kschischang, Brendan J Frey, and H-A Loeliger. Factor graphs

and the sum-product algorithm. IEEE Transactions on information theory,

47(2):498519, 2001.

Leonid G Khachiyan. A polynomial algorithm in linear programming.

Akademiia Nauk SSSR, Doklady, 244:10931096, 1979.

Amit Kumar and Ravindran Kannan. Clustering with spectral norm and

the k-means algorithm. In Foundations of Computer Science (FOCS), 2010

51st Annual IEEE Symposium on, pages 299308. IEEE, 2010.

Jon M. Kleinberg. Authoritative sources in a hyperlinked environment.

JOURNAL OF THE ACM, 46(5):604632, 1999.

Jon M. Kleinberg. The small-world phenomenon: an algorithm perspective.

In STOC, pages 163170, 2000.

Michael Krivelevich and Benny Sudakov. The phase transition in random

graphs: A simple proof. Random Struct. Algorithms, 43(2):131138, 2013.

Ravi Kannan and Santosh Vempala. Spectral algorithms. Foundations and

Trends in Theoretical Computer Science, 4(3-4):157288, 2009.

476

[KVV04]

Ravi Kannan, Santosh Vempala, and Adrian Vetta. On clusterings: Good,

bad and spectral. J. ACM, 51(3):497515, May 2004.

[LCB+04] Gert RG Lanckriet, Nello Cristianini, Peter Bartlett, Laurent El Ghaoui,

and Michael I Jordan. Learning the kernel matrix with semidenite pro-

gramming. Journal of Machine learning research, 5(Jan):2772, 2004.

[Lis13]

[Lit87]

Christian List. Social choice theory. In Edward N. Zalta, editor, The Stanford

Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University,

winter 2013 edition, 2013.

Nick Littlestone. Learning quickly when irrelevant attributes abound: A

new linear-threshold algorithm. In 28th Annual Symposium on Foundations

of Computer Science, pages 6877. IEEE, 1987.

[Liu01]

Jun Liu. Monte Carlo Strategies in Scientic Computing. Springer, 2001.

[Llo82]

[LW94]

[McS01]

[MG82]

[MM02]

[MP69]

[MPZ02]

[MR95a]

[MR95b]

[MU05]

Stuart Lloyd. Least squares quantization in pcm.

information theory, 28(2):129137, 1982.

IEEE transactions on

N. Littlestone and M. K. Warmuth. The weighted majority algorithm. In-

formation and Computation, 108(2):212261, 1994.

Frank McSherry. Spectral partitioning of random graphs. In FOCS, pages

529537, 2001.

Jayadev Misra and David Gries. Finding repeated elements. Science of

computer programming, 2(2):143152, 1982.

Gurmeet Singh Manku and Rajeev Motwani. Approximate frequency counts

over data streams. In Proceedings of the 28th international conference on

Very Large Data Bases, pages 346357. VLDB Endowment, 2002.

M. Minsky and S. Papert. Perceptrons: An Introduction to Computational

Geometry. The MIT Press, 1969.

Marc Mezard, Giorgio Parisi, and Riccardo Zecchina. Analytic and algorith-

mic solution of random satisability problems. Science, 297(5582):812815,

2002.

Michael Molloy and Bruce A. Reed. A critical point for random graphs with

a given degree sequence. Random Struct. Algorithms, 6(2/3):161180, 1995.

Rajeev Motwani and Prabhakar Raghavan. Randomized Algorithms. Cam-

bridge University Press, 1995.

Michael Mitzenmacher and Eli Upfal. Probability and computing - random-

ized algorithms and probabilistic analysis. Cambridge University Press, 2005.

477

[MV10]

[Nov62]

Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of

mixtures of gaussians. In FOCS, pages 93102, 2010.

A.B.J. Noviko. On convergence proofs on perceptrons. In Proceedings of

the Symposium on the Mathematical Theory of Automata, Vol. XII, pages

615622, 1962.

[per10]

Markov Chains and Mixing Times. American Mathematical Society, 2010.

[RV99]

Kannan Ravi and Vinay V. Analyzing the structure of large graphs. 1999.

[Sat75]

[Sch90]

[Sho70]

[SJ89]

[SS01]

M.A. Satterthwaite. Strategy-proofness and arrows conditions: existence and

correspondence theorems for voting procedures and social welfare functions.

Journal of Economic Theory, 10:187217, 1975.

Rob Schapire. Strength of weak learnability. Machine Learning, 5:197227,

1990.

Naum Z Shor. Convergence rate of the gradient descent method with dilata-

tion of the space. Cybernetics and Systems Analysis, 6(2):102108, 1970.

Alistair Sinclair and Mark Jerrum. Approximate counting, uniform gen-

eration and rapidly mixing markov chains. Information and Computation,

82:93133, 1989.

Bernhard Scholkopf and Alexander J. Smola. Learning with Kernels: Support

Vector Machines, Regularization, Optimization, and Beyond. MIT Press,

Cambridge, MA, USA, 2001.

[STBWA98] John Shawe-Taylor, Peter L Bartlett, Robert C Williamson, and Martin An-

thony. Structural risk minimization over data-dependent hierarchies. IEEE

transactions on Information Theory, 44(5):19261940, 1998.

[SWY75]

G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic

indexing. Commun. ACM, 18:613620, November 1975.

[Thr96]

[TM95]

S. Thrun. Explanation-Based Neural Network Learning: A Lifelong Learning

Approach. Kluwer Academic Publishers, Boston, MA, 1996.

Sebastian Thrun and Tom M. Mitchell. Lifelong robot learning. Robotics

and Autonomous Systems, 15(1-2):2546, 1995.

[Val84]

Leslie G. Valiant. A theory of the learnable. In STOC, pages 436445, 1984.

[Vap82]

[Vap98]

V. N. Vapnik. Estimation of Dependences Based on Empirical Data.

Springer-Verlag, New York, 1982.

V. N. Vapnik. Statistical Learning Theory. John Wiley and Sons Inc., New

York, 1998.

478

[VC71]

V. Vapnik and A. Chervonenkis. On the uniform convergence of relative

frequencies of events to their probabilities. Theory of Probability and its

Applications, 16(2):264280, 1971.

[Vem04]

Santosh Vempala. The Random Projection Method. DIMACS, 2004.

[VW02]

[War63]

[Wei97]

[WF01]

[Wis69]

[WS98]

[YFW01]

[YFW03]

[ZGL03]

Santosh Vempala and Grant Wang. A spectral algorithm for learning mix-

tures of distributions. Journal of Computer and System Sciences, pages

113123, 2002.

J.H. Ward. Hierarchical grouping to optimize an objective function. Journal

of the American statistical association, 58(301):236244, 1963.

Yair Weiss. Belief propagation and revision in networks with loops. Technical

Report A.I. Memo No. 1616, MIT, 1997.

Yair Weiss and William T Freeman. On the optimality of solutions of the

max-product belief-propagation algorithm in arbitrary graphs. IEEE Trans-

actions on Information Theory, 47(2):736744, 2001.

David Wishart. Mode analysis: A generalization of nearest neighbor which

reduces chaining eects. Numerical taxonomy, 76(282-311):17, 1969.

D. J. Watts and S. H. Strogatz. Collective dynamics of small-world net-

works. Nature, 393 (6684), 1998.

Jonathan S Yedidia, William T Freeman, and Yair Weiss. Bethe free energy,

kikuchi approximations, and belief propagation algorithms. Advances in

neural information processing systems, 13, 2001.

Jonathan S Yedidia, William T Freeman, and Yair Weiss. Understanding

belief propagation and its generalizations. Exploring articial intelligence in

the new millennium, 8:236239, 2003.

X. Zhu, Z. Ghahramani, and J. Laerty. Semi-supervised learning using

gaussian elds and harmonic functions. In Proc. 20th International Confer-

ence on Machine Learning, pages 912912, 2003.

[Zhu06]

X. Zhu. Semi-supervised learning literature survey. 2006. Computer Sciences

TR 1530 University of Wisconsin - Madison.

479